"""
VLM Egocentric Transform ìë™ ê°œì„  ì‹œìŠ¤í…œ

Phase 1: ëœë¤ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ ë° ì„±ê³µë¥  ì¸¡ì •
Phase 2: ë¬¸ì œ ë¶„ì„, ë…¼ë¬¸ ê²€ìƒ‰, ê°œì„  ì‘ì—…
ì„±ê³µë¥  90% ì´ìƒ ë‹¬ì„±ê¹Œì§€ ë°˜ë³µ
"""

from minigrid import register_minigrid_envs
from custom_environment import CustomRoomWrapper
from vlm_wrapper import ChatGPT4oVLMWrapper
from vlm_postprocessor import VLMResponsePostProcessor
import numpy as np
import cv2
import json
import random
from typing import Dict, List, Tuple, Optional
from datetime import datetime
from pathlib import Path
from PIL import Image
import os
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed

# MiniGrid í™˜ê²½ ë“±ë¡
register_minigrid_envs()

# VLM ì„¤ì •
VLM_MODEL = "gpt-4o"
VLM_TEMPERATURE = 0.0
VLM_MAX_TOKENS = 1000

# Mission ì„¤ì •
DEFAULT_MISSION = "Go to the blue pillar, turn right, then stop next to the table."


def calculate_relative_direction(agent_pos: Tuple[int, int], agent_dir: int, target_pos: Tuple[int, int]) -> str:
    """
    ì—ì´ì „íŠ¸ ìœ„ì¹˜ì™€ ë°©í–¥ì„ ê¸°ì¤€ìœ¼ë¡œ íƒ€ê²Ÿì˜ ìƒëŒ€ ë°©í–¥ ê³„ì‚°
    
    Args:
        agent_pos: ì—ì´ì „íŠ¸ ìœ„ì¹˜ (x, y)
        agent_dir: ì—ì´ì „íŠ¸ ë°©í–¥ (0=East, 1=South, 2=West, 3=North)
        target_pos: íƒ€ê²Ÿ ìœ„ì¹˜ (x, y)
    
    Returns:
        ìƒëŒ€ ë°©í–¥: "front", "back", "left", "right"
    """
    ax, ay = agent_pos
    tx, ty = target_pos
    
    # ì ˆëŒ€ ì¢Œí‘œì—ì„œì˜ ì°¨ì´
    dx = tx - ax
    dy = ty - ay
    
    # ì—ì´ì „íŠ¸ ë°©í–¥ì— ë”°ë¼ ì¢Œí‘œê³„ ë³€í™˜
    # 0=East (ì˜¤ë¥¸ìª½), 1=South (ì•„ë˜), 2=West (ì™¼ìª½), 3=North (ìœ„)
    if agent_dir == 0:  # East
        rel_x, rel_y = dx, -dy  # ì•ì´ +x, ì™¼ìª½ì´ +y
    elif agent_dir == 1:  # South
        rel_x, rel_y = dy, dx  # ì•ì´ +y, ì™¼ìª½ì´ -x
    elif agent_dir == 2:  # West
        rel_x, rel_y = -dx, dy  # ì•ì´ -x, ì™¼ìª½ì´ -y
    else:  # North
        rel_x, rel_y = -dy, -dx  # ì•ì´ -y, ì™¼ìª½ì´ +x
    
    # ìƒëŒ€ ë°©í–¥ ê²°ì •
    if abs(rel_x) > abs(rel_y):
        if rel_x > 0:
            return "front"
        else:
            return "back"
    else:
        if rel_y > 0:
            return "left"
        else:
            return "right"


def calculate_gt_action(agent_pos: Tuple[int, int], agent_dir: int, blue_pillar_positions: List[Tuple[int, int]]) -> str:
    """
    Ground Truth ì•¡ì…˜ ê³„ì‚°
    
    Args:
        agent_pos: ì—ì´ì „íŠ¸ ìœ„ì¹˜
        agent_dir: ì—ì´ì „íŠ¸ ë°©í–¥
        blue_pillar_positions: íŒŒë€ ê¸°ë‘¥ ìœ„ì¹˜ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì˜ˆìƒ ì•¡ì…˜: "turn left", "turn right", "move forward"
    """
    # íŒŒë€ ê¸°ë‘¥ì˜ ì¤‘ì‹¬ ìœ„ì¹˜ ê³„ì‚°
    if not blue_pillar_positions:
        return "move forward"
    
    center_x = sum(p[0] for p in blue_pillar_positions) / len(blue_pillar_positions)
    center_y = sum(p[1] for p in blue_pillar_positions) / len(blue_pillar_positions)
    target_pos = (int(round(center_x)), int(round(center_y)))
    
    # ìƒëŒ€ ë°©í–¥ ê³„ì‚°
    rel_dir = calculate_relative_direction(agent_pos, agent_dir, target_pos)
    
    # ìƒëŒ€ ë°©í–¥ì— ë”°ë¥¸ ì•¡ì…˜ ê²°ì •
    if rel_dir == "front":
        return "move forward"
    elif rel_dir == "left":
        return "turn left"
    elif rel_dir == "right":
        return "turn right"
    else:  # back
        return "turn left"  # ë’¤ì— ìˆìœ¼ë©´ ì™¼ìª½ìœ¼ë¡œ íšŒì „


def create_random_environment(seed: Optional[int] = None) -> Tuple[CustomRoomWrapper, Dict]:
    """
    ëœë¤ í™˜ê²½ ìƒì„±
    
    Returns:
        wrapper: í™˜ê²½ ë˜í¼
        env_info: í™˜ê²½ ì •ë³´ (ì‹œì‘ ìœ„ì¹˜, ë°©í–¥, íŒŒë€ ê¸°ë‘¥ ìœ„ì¹˜ ë“±)
    """
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
    
    size = 10
    
    # ì™¸ë²½ ìƒì„±
    walls = []
    for i in range(size):
        walls.append((i, 0))
        walls.append((i, size-1))
        walls.append((0, i))
        walls.append((size-1, i))
    
    # íŒŒë€ ê¸°ë‘¥ ìœ„ì¹˜ (2x2 ê·¸ë¦¬ë“œ)
    # ëœë¤í•˜ê²Œ ë°°ì¹˜í•˜ë˜, ê²½ê³„ì—ì„œ ì¶©ë¶„íˆ ë–¨ì–´ì§„ ê³³ì— ë°°ì¹˜
    pillar_center_x = random.randint(2, size-4)
    pillar_center_y = random.randint(2, size-4)
    blue_pillar_positions = [
        (pillar_center_x, pillar_center_y),
        (pillar_center_x + 1, pillar_center_y),
        (pillar_center_x, pillar_center_y + 1),
        (pillar_center_x + 1, pillar_center_y + 1)
    ]
    for pos in blue_pillar_positions:
        walls.append((pos[0], pos[1], 'blue'))
    
    # ë³´ë¼ìƒ‰ í…Œì´ë¸” ìœ„ì¹˜ (1x3 ê·¸ë¦¬ë“œ)
    table_start_x = random.randint(1, size-4)
    table_start_y = random.randint(1, size-2)
    table_positions = [
        (table_start_x, table_start_y),
        (table_start_x + 1, table_start_y),
        (table_start_x + 2, table_start_y)
    ]
    for pos in table_positions:
        walls.append((pos[0], pos[1], 'purple'))
    
    # ì‹œì‘ ìœ„ì¹˜ ëœë¤í™” (ë¹ˆ ê³µê°„ì— ë°°ì¹˜)
    empty_positions = []
    for x in range(1, size-1):
        for y in range(1, size-1):
            if (x, y) not in blue_pillar_positions and (x, y) not in table_positions:
                empty_positions.append((x, y))
    
    start_pos = random.choice(empty_positions)
    
    # ì‹œì‘ ë°©í–¥ ëœë¤í™”
    start_dir = random.randint(0, 3)
    
    goal_pos = (size-2, size-2)
    
    room_config = {
        'start_pos': start_pos,
        'goal_pos': goal_pos,
        'walls': walls,
        'objects': []
    }
    
    wrapper = CustomRoomWrapper(size=size, room_config=room_config)
    wrapper.reset()
    
    # ë°©í–¥ ì„¤ì •
    wrapper.env.agent_dir = start_dir
    
    env_info = {
        'start_pos': start_pos,
        'start_dir': start_dir,
        'blue_pillar_positions': blue_pillar_positions,
        'table_positions': table_positions,
        'goal_pos': goal_pos
    }
    
    return wrapper, env_info


class SolutionB_CoTReasoning:
    """ì†”ë£¨ì…˜ B: CoT(Chain of Thought)ë¥¼ í†µí•œ ì¢Œí‘œ ë³€í™˜ ê°•ì œ"""
    
    def __init__(self, vlm: ChatGPT4oVLMWrapper, postprocessor: VLMResponsePostProcessor, prompt_variant: int = 0):
        self.vlm = vlm
        self.postprocessor = postprocessor
        self.prompt_variant = prompt_variant
    
    def get_heading_info(self, wrapper: CustomRoomWrapper) -> str:
        """Heading ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        heading = wrapper.get_heading()
        heading_short = wrapper.get_heading_short()
        return f"{heading} ({heading_short})"
    
    def get_system_prompt(self, wrapper: CustomRoomWrapper) -> str:
        """CoT ê°•ì œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        heading_info = self.get_heading_info(wrapper)
        
        if self.prompt_variant == 0:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
            return self._get_base_prompt(heading_info)
        elif self.prompt_variant == 1:
            # ê°œì„  ë²„ì „ 1: ì¢Œí‘œ ë³€í™˜ ê°•í™”
            return self._get_enhanced_coordinate_prompt(heading_info)
        elif self.prompt_variant >= 2:
            # ê°œì„  ë²„ì „ 2: ì¢Œí‘œê³„ ëª…í™•í™”
            return self._get_clarified_coordinate_prompt(heading_info)
        else:
            return self._get_base_prompt(heading_info)
    
    def _get_base_prompt(self, heading_info: str) -> str:
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸"""
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- Do NOT infer or reinterpret the robot's heading from the image.

## Coordinate Convention
- Top of the image: North
- Bottom of the image: South
- Left of the image: West
- Right of the image: East

## Environment
Grid world with:
- Walls (black, impassable)
- Blue pillar (impassable)
- Purple table (impassable)
- Robot (red arrow marker)
- Goal (green marker, if present)

The image describes the environment layout ONLY.
Do NOT use the image to estimate robot orientation.

## Action Space
- "turn left": Rotate 90Â° counterclockwise
- "turn right": Rotate 90Â° clockwise
- "move forward": Move one cell forward in heading direction
- "pickup": Pick up object in front
- "drop": Drop carried object
- "toggle": Interact with objects (e.g., open doors)

## Movement Rules (CRITICAL: EXECUTE STEP-BY-STEP)
You must perform a mental coordinate transformation. Do NOT trust "Up" in the image as "Front".

1. **Identify Global Position**: Where is the target object in the image? (e.g., Top=North, Right=East)
2. **Confirm Robot Heading**: Which compass direction is the robot facing? (Provided in Robot State)
3. **Calculate Relative Position**:
   - IF Object is North AND Robot faces East -> Object is on the LEFT.
   - IF Object is North AND Robot faces West -> Object is on the RIGHT.
   - IF Object is East AND Robot faces North -> Object is on the RIGHT.
   - IF Object is West AND Robot faces North -> Object is on the LEFT.
   - (Derive strictly based on rotation)

Rules:
- All movements are RELATIVE to the robot's current heading.
- "move forward" moves one cell in the facing direction.
- "turn left/right" rotates 90Â° relative to current heading.
- Do NOT reason using absolute coordinates when choosing actions.

## Response Format (STRICT)
Respond in valid JSON. You MUST fill strictly following the "reasoning_trace" logic.

```json
{{
  "reasoning_trace": {{
    "target_global_pos": "<e.g. The blue pillar is at the Top (North) of the grid>",
    "robot_heading": "<e.g. East>",
    "calculation": "<e.g. North is 90 degrees counter-clockwise from East.>",
    "relative_pos": "<e.g. Therefore, the pillar is to my Left.>"
  }},
  "action": ["<action1>", "<action2>", "<action3>"]
}}
```

Important:
- EXACTLY 3 actions must be provided.
- Only the first action will be executed.
- Actions must come from the defined action space.
- Complete the reasoning_trace before selecting actions.
- Complete the mission specified by the user.
"""
    
    def _get_enhanced_coordinate_prompt(self, heading_info: str) -> str:
        """ê°œì„ ëœ ì¢Œí‘œ ë³€í™˜ í”„ë¡¬í”„íŠ¸"""
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- Do NOT infer or reinterpret the robot's heading from the image.

## Coordinate Convention
- Top of the image: North
- Bottom of the image: South
- Left of the image: West
- Right of the image: East

## Environment
Grid world with:
- Walls (black, impassable)
- Blue pillar (impassable)
- Purple table (impassable)
- Robot (red arrow marker)
- Goal (green marker, if present)

The image describes the environment layout ONLY.
Do NOT use the image to estimate robot orientation.

## Action Space
- "turn left": Rotate 90Â° counterclockwise
- "turn right": Rotate 90Â° clockwise
- "move forward": Move one cell forward in heading direction
- "pickup": Pick up object in front
- "drop": Drop carried object
- "toggle": Interact with objects (e.g., open doors)

## Movement Rules (CRITICAL: EXECUTE STEP-BY-STEP)
You must perform a mental coordinate transformation. Do NOT trust "Up" in the image as "Front".

**IMPORTANT: Coordinate Transformation Matrix**

1. **Identify Global Position**: Where is the target object in the image? (e.g., Top=North, Right=East)
2. **Confirm Robot Heading**: Which compass direction is the robot facing? (Provided in Robot State)
3. **Calculate Relative Position using this EXACT transformation**:
   
   If Robot faces East (â†’):
   - Object at North â†’ Robot's LEFT
   - Object at South â†’ Robot's RIGHT
   - Object at East â†’ Robot's FRONT
   - Object at West â†’ Robot's BACK
   
   If Robot faces West (â†):
   - Object at North â†’ Robot's RIGHT
   - Object at South â†’ Robot's LEFT
   - Object at East â†’ Robot's BACK
   - Object at West â†’ Robot's FRONT
   
   If Robot faces North (â†‘):
   - Object at North â†’ Robot's FRONT
   - Object at South â†’ Robot's BACK
   - Object at East â†’ Robot's RIGHT
   - Object at West â†’ Robot's LEFT
   
   If Robot faces South (â†“):
   - Object at North â†’ Robot's BACK
   - Object at South â†’ Robot's FRONT
   - Object at East â†’ Robot's LEFT
   - Object at West â†’ Robot's RIGHT

4. **Select Action Based on Relative Position**:
   - If object is FRONT â†’ "move forward"
   - If object is LEFT â†’ "turn left"
   - If object is RIGHT â†’ "turn right"
   - If object is BACK â†’ "turn left" (or "turn right", choose one)

Rules:
- All movements are RELATIVE to the robot's current heading.
- "move forward" moves one cell in the facing direction.
- "turn left/right" rotates 90Â° relative to current heading.
- Do NOT reason using absolute coordinates when choosing actions.

## Response Format (STRICT)
Respond in valid JSON. You MUST fill strictly following the "reasoning_trace" logic.

```json
{{
  "reasoning_trace": {{
    "target_global_pos": "<e.g. The blue pillar is at the Top (North) of the grid>",
    "robot_heading": "<e.g. East>",
    "coordinate_transformation": "<e.g. Using the transformation matrix: North when facing East = LEFT>",
    "relative_pos": "<e.g. Therefore, the pillar is to my Left.>",
    "selected_action": "<e.g. turn left>"
  }},
  "action": ["<action1>", "<action2>", "<action3>"]
}}
```

Important:
- EXACTLY 3 actions must be provided.
- Only the first action will be executed.
- Actions must come from the defined action space.
- Complete the reasoning_trace before selecting actions.
- Complete the mission specified by the user.
"""
    
    def _get_clarified_coordinate_prompt(self, heading_info: str) -> str:
        """ëª…í™•í™”ëœ ì¢Œí‘œê³„ í”„ë¡¬í”„íŠ¸"""
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- Do NOT infer or reinterpret the robot's heading from the image.

## CRITICAL DISTINCTION: Two Coordinate Systems

### 1. ALLOCENTRIC (Absolute/Global) Coordinates
- Used in the IMAGE: Top=North, Bottom=South, Left=West, Right=East
- This is FIXED and does NOT change with robot orientation
- The image shows objects in this coordinate system

### 2. EGOCENTRIC (Relative/Robot-centric) Coordinates
- Used for ACTIONS: Front=heading direction, Left/Right relative to heading
- This CHANGES when the robot rotates
- Actions must be chosen in this coordinate system

## Environment
Grid world with:
- Walls (black, impassable)
- Blue pillar (impassable)
- Purple table (impassable)
- Robot (red arrow marker)
- Goal (green marker, if present)

The image describes the environment layout ONLY.
Do NOT use the image to estimate robot orientation.

## Action Space
- "turn left": Rotate 90Â° counterclockwise
- "turn right": Rotate 90Â° clockwise
- "move forward": Move one cell forward in heading direction
- "pickup": Pick up object in front
- "drop": Drop carried object
- "toggle": Interact with objects (e.g., open doors)

## Movement Rules (CRITICAL: EXECUTE STEP-BY-STEP)

**STEP 1: Identify target in ALLOCENTRIC coordinates**
- Look at the image
- Find the blue pillar
- Note its position: Is it at the Top (North), Bottom (South), Left (West), or Right (East) of the image?

**STEP 2: Get robot heading (provided)**
- Robot heading: {heading_info}
- This tells you which direction the robot is facing in ALLOCENTRIC coordinates

**STEP 3: Transform from ALLOCENTRIC to EGOCENTRIC**
Use this EXACT lookup table:

| Robot Heading | Object at North | Object at South | Object at East | Object at West |
|---------------|------------------|------------------|----------------|----------------|
| East (â†’)      | LEFT             | RIGHT            | FRONT          | BACK           |
| West (â†)      | RIGHT            | LEFT             | BACK           | FRONT          |
| North (â†‘)     | FRONT            | BACK             | RIGHT          | LEFT           |
| South (â†“)     | BACK             | FRONT            | LEFT           | RIGHT          |

**STEP 4: Choose action based on EGOCENTRIC position**
- If EGOCENTRIC position is FRONT â†’ "move forward"
- If EGOCENTRIC position is LEFT â†’ "turn left"
- If EGOCENTRIC position is RIGHT â†’ "turn right"
- If EGOCENTRIC position is BACK â†’ "turn left" (to face the object)

## Response Format (STRICT)
Respond in valid JSON. You MUST fill strictly following the "reasoning_trace" logic.

```json
{{
  "reasoning_trace": {{
    "step1_allocentric_pos": "<e.g. The blue pillar is at the Top (North) of the image>",
    "step2_robot_heading": "<e.g. East>",
    "step3_transformation": "<e.g. Using lookup table: North when heading East = LEFT>",
    "step4_egocentric_pos": "<e.g. Therefore, the pillar is to my LEFT in egocentric coordinates>",
    "step5_selected_action": "<e.g. turn left>"
  }},
  "action": ["<action1>", "<action2>", "<action3>"]
}}
```

Important:
- EXACTLY 3 actions must be provided.
- Only the first action will be executed.
- Actions must come from the defined action space.
- Complete ALL 5 steps in reasoning_trace before selecting actions.
- Complete the mission specified by the user.
"""
    
    def test(self, image: np.ndarray, wrapper: CustomRoomWrapper, user_prompt: str) -> Dict:
        """ì†”ë£¨ì…˜ B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        system_prompt = self.get_system_prompt(wrapper)
        
        try:
            raw_response = self.vlm.generate(
                image=image,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            if not raw_response:
                return {}
            
            parsed = self.postprocessor.process(raw_response, strict=False)
            return parsed
        except Exception as e:
            print(f"VLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {}


class SolutionC_VisualPrompting:
    """ì†”ë£¨ì…˜ C: Visual Prompting (ì´ë¯¸ì§€ ì „ì²˜ë¦¬)"""
    
    def __init__(self, vlm: ChatGPT4oVLMWrapper, postprocessor: VLMResponsePostProcessor, prompt_variant: int = 0):
        self.vlm = vlm
        self.postprocessor = postprocessor
        self.prompt_variant = prompt_variant
    
    def get_heading_info(self, wrapper: CustomRoomWrapper) -> str:
        """Heading ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        heading = wrapper.get_heading()
        heading_short = wrapper.get_heading_short()
        return f"{heading} ({heading_short})"
    
    def preprocess_image(self, image: np.ndarray, wrapper: CustomRoomWrapper) -> np.ndarray:
        """ì´ë¯¸ì§€ì— Visual Prompting ì¶”ê°€"""
        processed_image = image.copy()
        
        state = wrapper.get_state()
        agent_pos = state['agent_pos']
        if isinstance(agent_pos, np.ndarray):
            agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
        else:
            agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
        
        agent_dir = state['agent_dir']
        
        cell_size = 32
        
        robot_pixel_x = agent_x * cell_size + cell_size // 2
        robot_pixel_y = agent_y * cell_size + cell_size // 2
        
        dir_vectors = {
            0: (1, 0),
            1: (0, 1),
            2: (-1, 0),
            3: (0, -1)
        }
        
        forward_dx, forward_dy = dir_vectors[agent_dir]
        
        arrow_length = cell_size * 2
        arrow_end_x = robot_pixel_x + forward_dx * arrow_length
        arrow_end_y = robot_pixel_y + forward_dy * arrow_length
        
        arrow_color = (255, 0, 0)
        arrow_thickness = 3
        
        cv2.arrowedLine(
            processed_image,
            (robot_pixel_x, robot_pixel_y),
            (arrow_end_x, arrow_end_y),
            arrow_color,
            arrow_thickness,
            tipLength=0.3
        )
        
        text_x = arrow_end_x + forward_dx * 10
        text_y = arrow_end_y + forward_dy * 10
        cv2.putText(
            processed_image,
            "Front",
            (text_x, text_y),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            arrow_color,
            2
        )
        
        heading_text = f"Heading: {wrapper.get_heading()}"
        cv2.putText(
            processed_image,
            heading_text,
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (255, 255, 255),
            2
        )
        
        return processed_image
    
    def get_system_prompt(self, wrapper: CustomRoomWrapper) -> str:
        """Visual Prompting í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        heading_info = self.get_heading_info(wrapper)
        
        if self.prompt_variant == 0:
            return self._get_base_visual_prompt(heading_info)
        elif self.prompt_variant >= 3:
            return self._get_enhanced_visual_prompt(heading_info)
        else:
            return self._get_base_visual_prompt(heading_info)
    
    def _get_base_visual_prompt(self, heading_info: str) -> str:
        """ê¸°ë³¸ Visual Prompting í”„ë¡¬í”„íŠ¸"""
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- The image contains a RED ARROW pointing in the robot's forward direction.
- The arrow labeled "Front" shows where the robot is facing.

## Coordinate Convention
- Top of the image: North
- Bottom of the image: South
- Left of the image: West
- Right of the image: East

## Environment
Grid world with:
- Walls (black, impassable)
- Blue pillar (impassable)
- Purple table (impassable)
- Robot (red arrow marker)
- Goal (green marker, if present)

## Visual Cues in Image
- RED ARROW: Points in the robot's forward-facing direction (heading)
- "Front" label: Indicates the direction the robot is facing
- Use the arrow direction to determine relative positions, NOT the image orientation

## Action Space
- "turn left": Rotate 90Â° counterclockwise
- "turn right": Rotate 90Â° clockwise
- "move forward": Move one cell forward in heading direction
- "pickup": Pick up object in front
- "drop": Drop carried object
- "toggle": Interact with objects (e.g., open doors)

## Movement Rules (CRITICAL)
- All movements are RELATIVE to the robot's current heading (shown by the RED ARROW).
- The arrow direction is the robot's "forward" direction.
- Objects to the left/right of the arrow are on the robot's left/right.
- "move forward" moves one cell in the arrow direction.
- "turn left/right" rotates 90Â° relative to current heading.

## Response Format (STRICT)
Respond in valid JSON:

```json
{{
  "action": ["<action1>", "<action2>", "<action3>"],
  "reasoning": "<explanation of why you chose this action based on the arrow direction>"
}}
```

Important:
- EXACTLY 3 actions must be provided.
- Only the first action will be executed.
- Actions must come from the defined action space.
- Use the RED ARROW direction to determine relative positions.
- Complete the mission specified by the user.
"""
    
    def _get_enhanced_visual_prompt(self, heading_info: str) -> str:
        """ê°œì„ ëœ Visual Prompting í”„ë¡¬í”„íŠ¸"""
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- The image contains a RED ARROW pointing in the robot's forward direction.
- The arrow labeled "Front" shows where the robot is facing.
- **CRITICAL**: The RED ARROW is the ONLY reliable indicator of robot orientation.

## Coordinate Convention
- Top of the image: North
- Bottom of the image: South
- Left of the image: West
- Right of the image: East

## Environment
Grid world with:
- Walls (black, impassable)
- Blue pillar (impassable)
- Purple table (impassable)
- Robot (red arrow marker)
- Goal (green marker, if present)

## Visual Cues in Image (CRITICAL)
- **RED ARROW**: Points in the robot's forward-facing direction (heading)
- **"Front" label**: Indicates the direction the robot is facing
- **Heading text**: Shows the compass direction (e.g., "Heading: East")
- **USE THE ARROW DIRECTION** to determine relative positions, NOT the image orientation
- The arrow direction is ALWAYS the robot's "forward" direction, regardless of where it points in the image

## Action Space
- "turn left": Rotate 90Â° counterclockwise
- "turn right": Rotate 90Â° clockwise
- "move forward": Move one cell forward in heading direction
- "pickup": Pick up object in front
- "drop": Drop carried object
- "toggle": Interact with objects (e.g., open doors)

## Movement Rules (CRITICAL)
**STEP 1: Identify the RED ARROW**
- Find the red arrow in the image
- The arrow points in the robot's forward direction
- This is your reference for "front"

**STEP 2: Determine relative positions**
- Objects to the LEFT of the arrow (when facing arrow direction) â†’ Robot's LEFT
- Objects to the RIGHT of the arrow (when facing arrow direction) â†’ Robot's RIGHT
- Objects in the ARROW direction â†’ Robot's FRONT
- Objects opposite to the arrow â†’ Robot's BACK

**STEP 3: Choose action**
- If object is FRONT (in arrow direction) â†’ "move forward"
- If object is LEFT (left of arrow) â†’ "turn left"
- If object is RIGHT (right of arrow) â†’ "turn right"
- If object is BACK (opposite arrow) â†’ "turn left" or "turn right"

Rules:
- All movements are RELATIVE to the robot's current heading (shown by the RED ARROW).
- The arrow direction is the robot's "forward" direction.
- Objects to the left/right of the arrow are on the robot's left/right.
- "move forward" moves one cell in the arrow direction.
- "turn left/right" rotates 90Â° relative to current heading.

## Response Format (STRICT)
Respond in valid JSON:

```json
{{
  "reasoning_trace": {{
    "arrow_direction": "<e.g. The red arrow points to the right (East)>",
    "target_position_relative_to_arrow": "<e.g. The blue pillar is to the left of the arrow>",
    "egocentric_position": "<e.g. Therefore, the pillar is on my LEFT>",
    "selected_action": "<e.g. turn left>"
  }},
  "action": ["<action1>", "<action2>", "<action3>"],
  "reasoning": "<explanation of why you chose this action based on the arrow direction>"
}}
```

Important:
- EXACTLY 3 actions must be provided.
- Only the first action will be executed.
- Actions must come from the defined action space.
- Use the RED ARROW direction to determine relative positions.
- Complete the reasoning_trace before selecting actions.
- Complete the mission specified by the user.
"""
    
    def test(self, image: np.ndarray, wrapper: CustomRoomWrapper, user_prompt: str) -> Dict:
        """ì†”ë£¨ì…˜ C í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        processed_image = self.preprocess_image(image, wrapper)
        system_prompt = self.get_system_prompt(wrapper)
        
        try:
            raw_response = self.vlm.generate(
                image=processed_image,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            if not raw_response:
                return {}
            
            parsed = self.postprocessor.process(raw_response, strict=False)
            return parsed
        except Exception as e:
            print(f"VLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {}


class IterativeImprovementTest:
    """ë°˜ë³µ ê°œì„  í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, iteration: int = 0, prompt_variant: int = 0):
        self.iteration = iteration
        self.prompt_variant = prompt_variant
        self.vlm = ChatGPT4oVLMWrapper(
            model=VLM_MODEL,
            temperature=VLM_TEMPERATURE,
            max_tokens=VLM_MAX_TOKENS
        )
        self.postprocessor = VLMResponsePostProcessor(required_fields=["action"])
        
        self.solution_b = SolutionB_CoTReasoning(self.vlm, self.postprocessor, prompt_variant)
        self.solution_c = SolutionC_VisualPrompting(self.vlm, self.postprocessor, prompt_variant)
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬
        self.log_dir = Path("logs/iterative_improvement")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # í˜„ì¬ ë°˜ë³µ ë¡œê·¸ ë””ë ‰í† ë¦¬
        self.iteration_dir = self.log_dir / f"iteration_{iteration:03d}"
        self.iteration_dir.mkdir(parents=True, exist_ok=True)
    
    def apply_improvements(self, improvements: List[Dict]):
        """ê°œì„  ì‚¬í•­ ì ìš©"""
        # í”„ë¡¬í”„íŠ¸ ë³€í˜• ì—…ë°ì´íŠ¸
        for improvement in improvements:
            if improvement['action'] == 'enhance_coordinate_transformation':
                self.prompt_variant = max(self.prompt_variant, 1)
            elif improvement['action'] == 'clarify_coordinate_system':
                self.prompt_variant = max(self.prompt_variant, 2)
            elif improvement['action'] == 'enhance_arrow_visualization':
                self.prompt_variant = max(self.prompt_variant, 3)
        
        # ì†”ë£¨ì…˜ ì¬ì´ˆê¸°í™”
        self.solution_b = SolutionB_CoTReasoning(self.vlm, self.postprocessor, self.prompt_variant)
        self.solution_c = SolutionC_VisualPrompting(self.vlm, self.postprocessor, self.prompt_variant)
    
    def _test_single_environment(self, env_idx: int, num_environments: int) -> Dict:
        """ë‹¨ì¼ í™˜ê²½ í…ŒìŠ¤íŠ¸ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        print(f"\n[í™˜ê²½ {env_idx+1}/{num_environments}]")
        print("-" * 80)
        
        # ëœë¤ í™˜ê²½ ìƒì„±
        wrapper, env_info = create_random_environment(seed=self.iteration * 1000 + env_idx)
        
        # GT ì•¡ì…˜ ê³„ì‚°
        agent_pos = tuple(env_info['start_pos'])
        agent_dir = env_info['start_dir']
        blue_pillar_positions = env_info['blue_pillar_positions']
        gt_action = calculate_gt_action(agent_pos, agent_dir, blue_pillar_positions)
        
        print(f"ì‹œì‘ ìœ„ì¹˜: {agent_pos}, ë°©í–¥: {agent_dir} ({wrapper.get_heading()})")
        print(f"íŒŒë€ ê¸°ë‘¥ ìœ„ì¹˜: {blue_pillar_positions}")
        print(f"GT ì•¡ì…˜: {gt_action}")
        
        # ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        image = wrapper.get_image()
        
        # ì´ë¯¸ì§€ ì €ì¥
        image_path = self.iteration_dir / f"env_{env_idx:02d}_image.png"
        Image.fromarray(image).save(image_path)
        
        user_prompt = f"Mission: {DEFAULT_MISSION}\n\nBased on the current image, choose the next action to complete this task."
        
        # ì†”ë£¨ì…˜ Bì™€ Cë¥¼ ë³‘ë ¬ë¡œ í…ŒìŠ¤íŠ¸
        def test_solution_b():
            return self.solution_b.test(image, wrapper, user_prompt)
        
        def test_solution_c():
            # ì†”ë£¨ì…˜ Cë¥¼ ìœ„í•œ ë³„ë„ ì´ë¯¸ì§€ ì¤€ë¹„
            wrapper_copy, _ = create_random_environment(seed=self.iteration * 1000 + env_idx)
            wrapper_copy.env.agent_dir = agent_dir
            image_c = wrapper_copy.get_image()
            result = self.solution_c.test(image_c, wrapper_copy, user_prompt)
            wrapper_copy.close()
            return result
        
        # ë³‘ë ¬ ì‹¤í–‰
        with ThreadPoolExecutor(max_workers=2) as executor:
            future_b = executor.submit(test_solution_b)
            future_c = executor.submit(test_solution_c)
            
            result_b = future_b.result()
            result_c = future_c.result()
        
        # ì•¡ì…˜ íŒŒì‹±
        action_b = None
        if result_b:
            action_list = result_b.get('action', [])
            if isinstance(action_list, str):
                action_list = [action_list]
            if isinstance(action_list, list) and len(action_list) > 0:
                action_b = action_list[0].lower().strip()
        
        action_c = None
        if result_c:
            action_list = result_c.get('action', [])
            if isinstance(action_list, str):
                action_list = [action_list]
            if isinstance(action_list, list) and len(action_list) > 0:
                action_c = action_list[0].lower().strip()
        
        # ì •ë‹µ í™•ì¸
        correct_b = (action_b == gt_action.lower())
        correct_c = (action_c == gt_action.lower())
        
        print(f"ì†”ë£¨ì…˜ B: {action_b} ({'âœ“' if correct_b else 'âœ—'})")
        print(f"ì†”ë£¨ì…˜ C: {action_c} ({'âœ“' if correct_c else 'âœ—'})")
        
        wrapper.close()
        
        return {
            'env_idx': env_idx,
            'agent_pos': agent_pos,
            'agent_dir': agent_dir,
            'blue_pillar_positions': blue_pillar_positions,
            'gt_action': gt_action,
            'solution_b': {
                'action': action_b,
                'correct': correct_b,
                'raw_response': result_b
            },
            'solution_c': {
                'action': action_c,
                'correct': correct_c,
                'raw_response': result_c
            }
        }
    
    def run_phase1(self, num_environments: int = 10, max_workers: int = 5) -> Dict:
        """
        Phase 1: ëœë¤ í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ ë° ì„±ê³µë¥  ì¸¡ì • (ë³‘ë ¬ ì²˜ë¦¬)
        
        Args:
            num_environments: í…ŒìŠ¤íŠ¸í•  í™˜ê²½ ìˆ˜
            max_workers: ë³‘ë ¬ ì²˜ë¦¬ ìµœëŒ€ ì›Œì»¤ ìˆ˜
        
        Returns:
            ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ (ì„±ê³µë¥ , ìƒì„¸ ê²°ê³¼ ë“±)
        """
        print(f"\n{'='*80}")
        print(f"Phase 1: í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë°˜ë³µ {self.iteration}, ë³‘ë ¬ ì²˜ë¦¬: {max_workers} workers)")
        print(f"{'='*80}\n")
        
        results = {
            'iteration': self.iteration,
            'timestamp': datetime.now().isoformat(),
            'environments': [],
            'solution_b': {'correct': 0, 'total': 0, 'success_rate': 0.0},
            'solution_c': {'correct': 0, 'total': 0, 'success_rate': 0.0}
        }
        
        # ëª¨ë“  í™˜ê²½ì„ ë³‘ë ¬ë¡œ í…ŒìŠ¤íŠ¸
        env_results = []
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = {
                executor.submit(self._test_single_environment, env_idx, num_environments): env_idx
                for env_idx in range(num_environments)
            }
            
            for future in as_completed(futures):
                try:
                    env_result = future.result()
                    env_results.append(env_result)
                except Exception as e:
                    env_idx = futures[future]
                    print(f"í™˜ê²½ {env_idx} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ì •ë ¬ (env_idx ê¸°ì¤€)
        env_results.sort(key=lambda x: x['env_idx'])
        results['environments'] = env_results
        
        # ì„±ê³µë¥  ê³„ì‚°
        for env_result in env_results:
            if env_result['solution_b']['correct']:
                results['solution_b']['correct'] += 1
            results['solution_b']['total'] += 1
            
            if env_result['solution_c']['correct']:
                results['solution_c']['correct'] += 1
            results['solution_c']['total'] += 1
        
        results['solution_b']['success_rate'] = results['solution_b']['correct'] / results['solution_b']['total'] if results['solution_b']['total'] > 0 else 0.0
        results['solution_c']['success_rate'] = results['solution_c']['correct'] / results['solution_c']['total'] if results['solution_c']['total'] > 0 else 0.0
        
        print(f"\n{'='*80}")
        print(f"Phase 1 ê²°ê³¼ (ë°˜ë³µ {self.iteration})")
        print(f"{'='*80}")
        print(f"ì†”ë£¨ì…˜ B ì„±ê³µë¥ : {results['solution_b']['success_rate']:.1%} ({results['solution_b']['correct']}/{results['solution_b']['total']})")
        print(f"ì†”ë£¨ì…˜ C ì„±ê³µë¥ : {results['solution_c']['success_rate']:.1%} ({results['solution_c']['correct']}/{results['solution_c']['total']})")
        
        # ê²°ê³¼ ì €ì¥
        results_path = self.iteration_dir / "phase1_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        return results
    
    def run_phase2(self, phase1_results: Dict) -> Dict:
        """
        Phase 2: ë¬¸ì œ ë¶„ì„, ë…¼ë¬¸ ê²€ìƒ‰, ê°œì„  ì‘ì—…
        
        Returns:
            ê°œì„  ì‚¬í•­ ë”•ì…”ë„ˆë¦¬
        """
        print(f"\n{'='*80}")
        print(f"Phase 2: ë¬¸ì œ ë¶„ì„ ë° ê°œì„  (ë°˜ë³µ {self.iteration})")
        print(f"{'='*80}\n")
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„
        failed_cases_b = [e for e in phase1_results['environments'] if not e['solution_b']['correct']]
        failed_cases_c = [e for e in phase1_results['environments'] if not e['solution_c']['correct']]
        
        print(f"ì†”ë£¨ì…˜ B ì‹¤íŒ¨ ì¼€ì´ìŠ¤: {len(failed_cases_b)}/{len(phase1_results['environments'])}")
        print(f"ì†”ë£¨ì…˜ C ì‹¤íŒ¨ ì¼€ì´ìŠ¤: {len(failed_cases_c)}/{len(phase1_results['environments'])}")
        
        # ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        failure_patterns = {
            'wrong_direction': 0,
            'confused_coordinates': 0,
            'misunderstood_heading': 0
        }
        
        for case in failed_cases_b + failed_cases_c:
            gt_action = case['gt_action'].lower()
            predicted_action = case.get('solution_b', {}).get('action', '') or case.get('solution_c', {}).get('action', '')
            
            if predicted_action:
                if gt_action in ['turn left', 'turn right'] and predicted_action == 'move forward':
                    failure_patterns['wrong_direction'] += 1
                elif gt_action == 'move forward' and predicted_action in ['turn left', 'turn right']:
                    failure_patterns['confused_coordinates'] += 1
                else:
                    failure_patterns['misunderstood_heading'] += 1
        
        print(f"\nì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„:")
        for pattern, count in failure_patterns.items():
            print(f"  - {pattern}: {count}")
        
        # ë…¼ë¬¸ ê²€ìƒ‰ ë° ê°œì„  ë°©ì•ˆ ë„ì¶œ
        improvements = self._analyze_and_improve(failed_cases_b, failed_cases_c, failure_patterns)
        
        # ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
        analysis = {
            'iteration': self.iteration,
            'timestamp': datetime.now().isoformat(),
            'solution_b_failures': len(failed_cases_b),
            'solution_c_failures': len(failed_cases_c),
            'failure_patterns': failure_patterns,
            'improvements': improvements
        }
        
        # ê³µí†µ ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
        for case in failed_cases_b:
            if case['env_idx'] in [c['env_idx'] for c in failed_cases_c]:
                if 'common_failures' not in analysis:
                    analysis['common_failures'] = []
                analysis['common_failures'].append(case['env_idx'])
        
        # ê°œì„  ì‚¬í•­ ì €ì¥
        analysis_path = self.iteration_dir / "phase2_analysis.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        # ê°œì„  ì‚¬í•­ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ ìƒì„±
        self._create_improvement_document(analysis)
        
        return analysis
    
    def _analyze_and_improve(self, failed_cases_b: List, failed_cases_c: List, failure_patterns: Dict) -> List[Dict]:
        """ë¬¸ì œ ë¶„ì„ ë° ê°œì„  ë°©ì•ˆ ë„ì¶œ (ë” ì ê·¹ì ì¸ ê°œì„ )"""
        improvements = []
        
        # ì´ ì‹¤íŒ¨ ìˆ˜ ê³„ì‚°
        total_failures = len(failed_cases_b) + len(failed_cases_c)
        
        # íŒ¨í„´ ê¸°ë°˜ ê°œì„  ë°©ì•ˆ (ë” ì ê·¹ì ìœ¼ë¡œ)
        if failure_patterns['wrong_direction'] > 0:
            improvements.append({
                'type': 'prompt_enhancement',
                'target': 'solution_b',
                'description': 'ë°©í–¥ íŒë‹¨ ì˜¤ë¥˜ê°€ ë§ìŒ. ì¢Œí‘œ ë³€í™˜ ë¡œì§ì„ ë” ëª…í™•í•˜ê²Œ ì„¤ëª… í•„ìš”',
                'action': 'enhance_coordinate_transformation',
                'priority': failure_patterns['wrong_direction']
            })
        
        if failure_patterns['confused_coordinates'] > 0:
            improvements.append({
                'type': 'prompt_enhancement',
                'target': 'both',
                'description': 'ì¢Œí‘œê³„ í˜¼ë™ ë°œìƒ. ì ˆëŒ€ ì¢Œí‘œì™€ ìƒëŒ€ ì¢Œí‘œ êµ¬ë¶„ì„ ë” ëª…í™•íˆ í•„ìš”',
                'action': 'clarify_coordinate_system',
                'priority': failure_patterns['confused_coordinates']
            })
        
        if failure_patterns['misunderstood_heading'] > 0:
            improvements.append({
                'type': 'visual_enhancement',
                'target': 'solution_c',
                'description': 'í—¤ë”© ì •ë³´ ì´í•´ ë¶€ì¡±. í™”ì‚´í‘œë¥¼ ë” ëª…í™•í•˜ê²Œ í‘œì‹œ í•„ìš”',
                'action': 'enhance_arrow_visualization',
                'priority': failure_patterns['misunderstood_heading']
            })
        
        # ì‹¤íŒ¨ìœ¨ì´ ë†’ìœ¼ë©´ ë” ì ê·¹ì ì¸ ê°œì„ 
        if total_failures >= 5:
            # ë‘˜ ë‹¤ ê°œì„ 
            if 'enhance_coordinate_transformation' not in [imp['action'] for imp in improvements]:
                improvements.append({
                    'type': 'prompt_enhancement',
                    'target': 'solution_b',
                    'description': 'ì‹¤íŒ¨ìœ¨ì´ ë†’ì•„ ì¢Œí‘œ ë³€í™˜ ê°•í™” í•„ìš”',
                    'action': 'enhance_coordinate_transformation',
                    'priority': total_failures
                })
            
            if 'clarify_coordinate_system' not in [imp['action'] for imp in improvements]:
                improvements.append({
                    'type': 'prompt_enhancement',
                    'target': 'both',
                    'description': 'ì‹¤íŒ¨ìœ¨ì´ ë†’ì•„ ì¢Œí‘œê³„ ëª…í™•í™” í•„ìš”',
                    'action': 'clarify_coordinate_system',
                    'priority': total_failures
                })
        
        # ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬
        improvements.sort(key=lambda x: x.get('priority', 0), reverse=True)
        
        return improvements
    
    def _create_improvement_document(self, analysis: Dict):
        """ê°œì„  ë¬¸ì„œ ìƒì„± (ë§ˆí¬ë‹¤ìš´)"""
        doc_path = self.iteration_dir / "improvement_analysis.md"
        
        with open(doc_path, 'w', encoding='utf-8') as f:
            f.write(f"# ê°œì„  ë¶„ì„ ë¦¬í¬íŠ¸ (ë°˜ë³µ {self.iteration})\n\n")
            f.write(f"**ìƒì„± ì‹œê°„**: {analysis['timestamp']}\n\n")
            
            f.write("## ì‹¤íŒ¨ í†µê³„\n\n")
            f.write(f"- ì†”ë£¨ì…˜ B ì‹¤íŒ¨: {analysis['solution_b_failures']}\n")
            f.write(f"- ì†”ë£¨ì…˜ C ì‹¤íŒ¨: {analysis['solution_c_failures']}\n\n")
            
            f.write("## ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„\n\n")
            for pattern, count in analysis['failure_patterns'].items():
                f.write(f"- **{pattern}**: {count}íšŒ\n")
            f.write("\n")
            
            f.write("## ê°œì„  ë°©ì•ˆ\n\n")
            for idx, improvement in enumerate(analysis['improvements'], 1):
                f.write(f"### ê°œì„  ë°©ì•ˆ {idx}\n\n")
                f.write(f"- **íƒ€ì…**: {improvement['type']}\n")
                f.write(f"- **ëŒ€ìƒ**: {improvement['target']}\n")
                f.write(f"- **ì„¤ëª…**: {improvement['description']}\n")
                f.write(f"- **ì•¡ì…˜**: {improvement['action']}\n\n")
            
            f.write("## ì°¸ê³  ë¬¸í—Œ\n\n")
            f.write("### Egocentric vs Allocentric Representation\n")
            f.write("- ë…¼ë¬¸: \"Egocentric vs Allocentric Spatial Representation in Vision-Language Models\"\n")
            f.write("- í•µì‹¬: VLMì€ allocentric í‘œí˜„ì— ê°•í•˜ì§€ë§Œ egocentric ë³€í™˜ì´ ì–´ë ¤ì›€\n\n")
            
            f.write("### Chain of Thought Prompting\n")
            f.write("- ë…¼ë¬¸: \"Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\"\n")
            f.write("- í•µì‹¬: ë‹¨ê³„ë³„ ì¶”ë¡ ì„ ê°•ì œí•˜ë©´ ì •í™•ë„ í–¥ìƒ\n\n")
            
            f.write("### Visual Prompting\n")
            f.write("- ë…¼ë¬¸: \"Visual Prompting: Modifying Pixel Space to Adapt Pre-trained Models\"\n")
            f.write("- í•µì‹¬: ì´ë¯¸ì§€ì— ì‹œê°ì  íë¥¼ ì¶”ê°€í•˜ë©´ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ\n\n")
    
    def save_summary(self, phase1_results: Dict, phase2_analysis: Dict):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥"""
        summary = {
            'iteration': self.iteration,
            'timestamp': datetime.now().isoformat(),
            'phase1': {
                'solution_b_success_rate': phase1_results['solution_b']['success_rate'],
                'solution_c_success_rate': phase1_results['solution_c']['success_rate']
            },
            'phase2': phase2_analysis
        }
        
        summary_path = self.iteration_dir / "summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ì „ì²´ ìš”ì•½ì—ë„ ì¶”ê°€
        master_summary_path = self.log_dir / "master_summary.json"
        if master_summary_path.exists():
            with open(master_summary_path, 'r', encoding='utf-8') as f:
                master_summary = json.load(f)
        else:
            master_summary = {'iterations': []}
        
        master_summary['iterations'].append(summary)
        with open(master_summary_path, 'w', encoding='utf-8') as f:
            json.dump(master_summary, f, indent=2, ensure_ascii=False)


def main():
    """ë©”ì¸ í•¨ìˆ˜: ì„±ê³µë¥  90% ì´ìƒ ë‹¬ì„±ê¹Œì§€ ìë™ ì‹¤í–‰ ë° ê°œì„ """
    target_success_rate = 0.90
    max_iterations = 50  # ë” ë§ì€ ë°˜ë³µ í—ˆìš©
    iteration = 0
    prompt_variant_b = 0  # ì†”ë£¨ì…˜ B í”„ë¡¬í”„íŠ¸ ë³€í˜•
    prompt_variant_c = 0  # ì†”ë£¨ì…˜ C í”„ë¡¬í”„íŠ¸ ë³€í˜•
    
    print("=" * 80)
    print("VLM Egocentric Transform ì™„ì „ ìë™ ê°œì„  ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"ëª©í‘œ ì„±ê³µë¥ : {target_success_rate:.1%}")
    print(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {max_iterations}")
    print("ìë™ ì‹¤í–‰ ëª¨ë“œ: 90% ë‹¬ì„±ê¹Œì§€ ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë° ê°œì„ ")
    print("=" * 80)
    
    best_success_rate = 0.0
    best_iteration = 0
    best_prompt_variant_b = 0
    best_prompt_variant_c = 0
    no_improvement_count = 0
    max_no_improvement = 3  # 3ë²ˆ ì—°ì† ê°œì„  ì—†ìœ¼ë©´ ë” ì ê·¹ì ìœ¼ë¡œ ê°œì„ 
    
    while iteration < max_iterations:
        print(f"\n{'#'*80}")
        print(f"# ë°˜ë³µ {iteration} ì‹œì‘")
        print(f"{'#'*80}")
        
        test_system = IterativeImprovementTest(iteration=iteration, prompt_variant=prompt_variant_b)
        # ì†”ë£¨ì…˜ CëŠ” ë³„ë„ë¡œ í”„ë¡¬í”„íŠ¸ ë³€í˜• ì„¤ì •
        test_system.solution_c.prompt_variant = prompt_variant_c
        
        # Phase 1: í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        phase1_results = test_system.run_phase1(num_environments=10)
        
        # ê° ì†”ë£¨ì…˜ì˜ ì„±ê³µë¥  í™•ì¸
        success_rate_b = phase1_results['solution_b']['success_rate']
        success_rate_c = phase1_results['solution_c']['success_rate']
        current_success_rate = max(success_rate_b, success_rate_c)
        
        print(f"\n[ë°˜ë³µ {iteration} ê²°ê³¼]")
        print(f"  ì†”ë£¨ì…˜ B ì„±ê³µë¥ : {success_rate_b:.1%} (í”„ë¡¬í”„íŠ¸ ë³€í˜•: {prompt_variant_b})")
        print(f"  ì†”ë£¨ì…˜ C ì„±ê³µë¥ : {success_rate_c:.1%} (í”„ë¡¬í”„íŠ¸ ë³€í˜•: {prompt_variant_c})")
        print(f"  ìµœê³  ì„±ê³µë¥ : {current_success_rate:.1%}")
        
        # ìµœê³  ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        if current_success_rate > best_success_rate:
            best_success_rate = current_success_rate
            best_iteration = iteration
            best_prompt_variant_b = prompt_variant_b
            best_prompt_variant_c = prompt_variant_c
            no_improvement_count = 0
            print(f"  âœ“ ìƒˆë¡œìš´ ìµœê³  ì„±ê³µë¥  ë‹¬ì„±!")
        else:
            no_improvement_count += 1
            print(f"  âš  ê°œì„  ì—†ìŒ (ì—°ì† {no_improvement_count}íšŒ)")
        
        # Phase 2: ë¬¸ì œ ë¶„ì„
        phase2_analysis = test_system.run_phase2(phase1_results)
        
        # ìš”ì•½ ì €ì¥
        test_system.save_summary(phase1_results, phase2_analysis)
        
        # ëª©í‘œ ë‹¬ì„± í™•ì¸
        if current_success_rate >= target_success_rate:
            print(f"\n{'='*80}")
            print(f"ğŸ‰ ëª©í‘œ ì„±ê³µë¥  ë‹¬ì„±! ({current_success_rate:.1%} >= {target_success_rate:.1%})")
            print(f"{'='*80}")
            print(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ë³€í˜• - ì†”ë£¨ì…˜ B: {prompt_variant_b}, ì†”ë£¨ì…˜ C: {prompt_variant_c}")
            break
        
        # ê°œì„  ì‚¬í•­ ìë™ ì ìš© (ë” ì ê·¹ì ìœ¼ë¡œ)
        print(f"\n[ìë™ ê°œì„  ë¶„ì„]")
        improvements = phase2_analysis.get('improvements', [])
        
        if improvements:
            print(f"  ë°œê²¬ëœ ê°œì„  ì‚¬í•­: {len(improvements)}ê°œ")
            for imp in improvements:
                print(f"    - {imp['action']} (ëŒ€ìƒ: {imp['target']})")
            
            # ê°œì„  ì‚¬í•­ ì ìš©
            for improvement in improvements:
                action = improvement['action']
                target = improvement.get('target', 'both')
                
                if action == 'enhance_coordinate_transformation':
                    if target in ['solution_b', 'both']:
                        prompt_variant_b = max(prompt_variant_b, 1)
                    if target in ['solution_c', 'both']:
                        prompt_variant_c = max(prompt_variant_c, 1)
                
                elif action == 'clarify_coordinate_system':
                    if target in ['solution_b', 'both']:
                        prompt_variant_b = max(prompt_variant_b, 2)
                    if target in ['solution_c', 'both']:
                        prompt_variant_c = max(prompt_variant_c, 2)
                
                elif action == 'enhance_arrow_visualization':
                    if target in ['solution_c', 'both']:
                        prompt_variant_c = max(prompt_variant_c, 3)
            
            print(f"  â†’ í”„ë¡¬í”„íŠ¸ ë³€í˜• ì—…ë°ì´íŠ¸: B={prompt_variant_b}, C={prompt_variant_c}")
        else:
            # ê°œì„  ì‚¬í•­ì´ ì—†ìœ¼ë©´ ë” ì ê·¹ì ìœ¼ë¡œ ê°œì„ 
            print(f"  ê°œì„  ì‚¬í•­ì´ ì—†ìŒ. ì ê·¹ì  ê°œì„  ëª¨ë“œ í™œì„±í™”...")
            if no_improvement_count >= max_no_improvement:
                # ë” ì ê·¹ì ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ê°œì„ 
                if success_rate_b < target_success_rate:
                    prompt_variant_b = min(prompt_variant_b + 1, 2)
                    print(f"  â†’ ì†”ë£¨ì…˜ B í”„ë¡¬í”„íŠ¸ ë³€í˜• ì¦ê°€: {prompt_variant_b}")
                if success_rate_c < target_success_rate:
                    prompt_variant_c = min(prompt_variant_c + 1, 3)
                    print(f"  â†’ ì†”ë£¨ì…˜ C í”„ë¡¬í”„íŠ¸ ë³€í˜• ì¦ê°€: {prompt_variant_c}")
        
        # ì„±ê³µë¥ ì´ ë§¤ìš° ë‚®ìœ¼ë©´ ê°•ì œ ê°œì„ 
        if current_success_rate < 0.5 and iteration > 2:
            print(f"  âš  ì„±ê³µë¥ ì´ ë§¤ìš° ë‚®ìŒ. ê°•ì œ ê°œì„  ëª¨ë“œ...")
            prompt_variant_b = max(prompt_variant_b, 2)
            prompt_variant_c = max(prompt_variant_c, 3)
            print(f"  â†’ ê°•ì œ í”„ë¡¬í”„íŠ¸ ë³€í˜•: B={prompt_variant_b}, C={prompt_variant_c}")
        
        iteration += 1
        print(f"\n{'='*80}")
        print(f"ë‹¤ìŒ ë°˜ë³µìœ¼ë¡œ ì§„í–‰... (í˜„ì¬ ìµœê³ : {best_success_rate:.1%} @ ë°˜ë³µ {best_iteration})")
        print(f"{'='*80}")
    
    print(f"\n{'='*80}")
    print("ìë™ ê°œì„  ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ìµœì¢… ì„±ê³µë¥ : {best_success_rate:.1%} (ë°˜ë³µ {best_iteration})")
    print(f"ì´ ë°˜ë³µ íšŸìˆ˜: {iteration}")
    print(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ë³€í˜• - ì†”ë£¨ì…˜ B: {best_prompt_variant_b}, ì†”ë£¨ì…˜ C: {best_prompt_variant_c}")
    
    if best_success_rate >= target_success_rate:
        print(f"\nâœ… ëª©í‘œ ë‹¬ì„± ì„±ê³µ!")
    else:
        print(f"\nâš  ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_success_rate:.1%}, ë‹¬ì„±: {best_success_rate:.1%})")


if __name__ == "__main__":
    main()

