######################################################
#                                                    #
#                         VLM                        #
#                      PROCESSOR                     #
#                                                    #
######################################################


""""""




######################################################
#                                                    #
#                      LIBRARIES                     #
#                                                    #
######################################################


import numpy as np

from utils.prompt_manager.prompt_interp import *
import utils.prompt_manager.terminal_formatting_utils as tfu
from utils.vlm_manager.vlm_wrapper import ChatGPT4oVLMWrapper
from utils.vlm_manager.vlm_postprocessor import VLMResponsePostProcessor

from utils.global_variables import VLM_MAX_TOKENS, VLM_MODEL, VLM_TEMPERATURE




######################################################
#                                                    #
#                       CLASS                        #
#                                                    #
######################################################


class VLMProcessor:
    """VLM Request and Parsing Processing Class"""
    
    def __init__(self,
                 model: str = VLM_MODEL,
                 temperature: float = VLM_TEMPERATURE,
                 max_tokens: int = VLM_MAX_TOKENS
                ):
        self.vlm = ChatGPT4oVLMWrapper(model=model,
                                       temperature=temperature,
                                       max_tokens=max_tokens
                                      )
        self.postprocessor_action = VLMResponsePostProcessor(required_fields=["action", "reasoning", "grounding", "memory"])
        self.postprocessor_feedback = VLMResponsePostProcessor(required_fields=["knowledge"])
    
    def requester(self,
                  image: np.ndarray,
                  system_prompt: str,
                  user_prompt: str
                 ) -> str:
        """Send Request to VLM (Default Method)"""
        
        try:
            response = self.vlm.generate(image=image,
                                         system_prompt=system_prompt,
                                         user_prompt=user_prompt
                                        )
            return response
        except Exception as e:
            tfu.cprint(f"VLM API call failed: {e}", tfu.RED, True)
            return ""
    
    def parser_action(self, raw_response: str) -> dict:
        """Parsing the Response Generated by the Action"""
        
        try:
            parsed = self.postprocessor_action.process(raw_response, strict=True)
            return parsed
        except ValueError as e:
            tfu.cprint(f"Response parsing failed: {e}", tfu.RED, True)
            return {
                "action": ["0"],  # Default value: move up
                "reasoning": "Parsing failed",
                "grounding": "",
                "memory": {
                    "spatial_description": "",
                    "task_process": {"goal": "", "status": ""},
                    "previous_action": ""
                }
            }
    
    def parser_feedback(self, raw_response: str) -> dict:
        """Parse the response generated for feedback"""
        
        try:
            parsed = self.postprocessor_feedback.process(raw_response, strict=True)
            return parsed
        except ValueError as e:
            tfu.cprint(f"Feedback response parsing failed: {e}", tfu.RED, True)
            return {"knowledge": ""}



