"""
MiniGrid VLM ìƒí˜¸ì‘ìš© ìŠ¤í¬ë¦½íŠ¸ (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë²„ì „)

ì„±ê³µí•œ ì†”ë£¨ì…˜ B í”„ë¡¬í”„íŠ¸(90% ì„±ê³µë¥ )ë¥¼ ì ìš©í•œ ë²„ì „ì…ë‹ˆë‹¤.
- Allocentric vs Egocentric ì¢Œí‘œê³„ ëª…í™• êµ¬ë¶„
- 5ë‹¨ê³„ ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ ê°•ì œ
- Lookup Table ê¸°ë°˜ ë³€í™˜ ë¡œì§

VLMì„ ì‚¬ìš©í•˜ì—¬ MiniGrid í™˜ê²½ì„ ì œì–´í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
ë¡œê¹…, ë©”ëª¨ë¦¬, ê·¸ë¼ìš´ë”© ë“± ë³µì¡í•œ ê¸°ëŠ¥ì€ ì œê±°í•˜ê³  í•µì‹¬ ê¸°ëŠ¥ë§Œ í¬í•¨í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    python minigrid_vlm_interact_improved.py
"""

from minigrid import register_minigrid_envs
# Actual path: legacy.relative_movement.custom_environment
from legacy import CustomRoomWrapper
# Actual paths: utils.vlm.vlm_wrapper, utils.vlm.vlm_postprocessor
from utils import ChatGPT4oVLMWrapper, VLMResponsePostProcessor
import numpy as np
import cv2

# MiniGrid í™˜ê²½ ë“±ë¡
register_minigrid_envs()

# VLM ì„¤ì •
VLM_MODEL = "gpt-4o"
VLM_TEMPERATURE = 0.0
VLM_MAX_TOKENS = 1000


def get_system_prompt(wrapper: CustomRoomWrapper) -> str:
    """System Prompt ìƒì„± (ì„±ê³µí•œ ì†”ë£¨ì…˜ B í”„ë¡¬í”„íŠ¸ - 90% ì„±ê³µë¥  ë‹¬ì„±)"""
    heading = wrapper.get_heading()
    heading_short = wrapper.get_heading_short()
    heading_info = f"{heading} ({heading_short})"
    
    return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- Do NOT infer or reinterpret the robot's heading from the image.

## CRITICAL DISTINCTION: Two Coordinate Systems

### 1. ALLOCENTRIC (Absolute/Global) Coordinates
- Used in the IMAGE: Top=North, Bottom=South, Left=West, Right=East
- This is FIXED and does NOT change with robot orientation
- The image shows objects in this coordinate system

### 2. EGOCENTRIC (Relative/Robot-centric) Coordinates
- Used for ACTIONS: Front=heading direction, Left/Right relative to heading
- This CHANGES when the robot rotates
- Actions must be chosen in this coordinate system

## Environment
Grid world with:
- Walls (black, impassable)
- Blue pillar (impassable)
- Purple table (impassable)
- Robot (red arrow marker)
- Goal (green marker, if present)

The image describes the environment layout ONLY.
Do NOT use the image to estimate robot orientation.

## Action Space
- "turn left": Rotate 90Â° counterclockwise
- "turn right": Rotate 90Â° clockwise
- "move forward": Move one cell forward in heading direction
- "pickup": Pick up object in front
- "drop": Drop carried object
- "toggle": Interact with objects (e.g., open doors)

## Movement Rules (CRITICAL: EXECUTE STEP-BY-STEP)

**STEP 1: Identify target in ALLOCENTRIC coordinates**
- Look at the image
- Find the blue pillar
- Note its position: Is it at the Top (North), Bottom (South), Left (West), or Right (East) of the image?

**STEP 2: Get robot heading (provided)**
- Robot heading: {heading_info}
- This tells you which direction the robot is facing in ALLOCENTRIC coordinates

**STEP 3: Transform from ALLOCENTRIC to EGOCENTRIC**
Use this EXACT lookup table:

| Robot Heading | Object at North | Object at South | Object at East | Object at West |
|---------------|------------------|------------------|----------------|----------------|
| East (â†’)      | LEFT             | RIGHT            | FRONT          | BACK           |
| West (â†)      | RIGHT            | LEFT             | BACK           | FRONT          |
| North (â†‘)     | FRONT            | BACK             | RIGHT          | LEFT           |
| South (â†“)     | BACK             | FRONT            | LEFT           | RIGHT          |

**STEP 4: Choose action based on EGOCENTRIC position**
- If EGOCENTRIC position is FRONT â†’ "move forward"
- If EGOCENTRIC position is LEFT â†’ "turn left"
- If EGOCENTRIC position is RIGHT â†’ "turn right"
- If EGOCENTRIC position is BACK â†’ "turn left" (to face the object)

## Response Format (STRICT)
Respond in valid JSON. You MUST fill strictly following the "reasoning_trace" logic.

```json
{{
  "reasoning_trace": {{
    "step1_allocentric_pos": "<e.g. The blue pillar is at the Top (North) of the image>",
    "step2_robot_heading": "<e.g. East>",
    "step3_transformation": "<e.g. Using lookup table: North when heading East = LEFT>",
    "step4_egocentric_pos": "<e.g. Therefore, the pillar is to my LEFT in egocentric coordinates>",
    "step5_selected_action": "<e.g. turn left>"
  }},
  "action": "<action_name>",
  "environment_info": "<description of current state with spatial relationships relative to robot heading orientation>",
  "reasoning": "<explanation of why you chose this action>"
}}
```

Important:
- Valid JSON format required
- Actions must be from the list above
- Complete ALL 5 steps in reasoning_trace before selecting actions
- Complete the mission specified by the user
"""


def create_scenario2_environment():
    """ì‹œë‚˜ë¦¬ì˜¤ 2 í™˜ê²½ ìƒì„±"""
    size = 10
    
    # ì™¸ë²½ ìƒì„±
    walls = []
    for i in range(size):
        walls.append((i, 0))
        walls.append((i, size-1))
        walls.append((0, i))
        walls.append((size-1, i))
    
    # íŒŒë€ ê¸°ë‘¥: 2x2 Grid (ìƒ‰ìƒì´ ìˆëŠ” ë²½ìœ¼ë¡œ ë³€ê²½)
    blue_pillar_positions = [(3, 4), (4, 4), (3, 5), (4, 5)]
    for pos in blue_pillar_positions:
        walls.append((pos[0], pos[1], 'blue'))
    
    # í…Œì´ë¸”: ë³´ë¼ìƒ‰ 1x3 Grid (ìƒ‰ìƒì´ ìˆëŠ” ë²½ìœ¼ë¡œ ë³€ê²½)
    table_positions = [(5, 1), (6, 1), (7, 1)]
    for pos in table_positions:
        walls.append((pos[0], pos[1], 'purple'))
    
    # ì‹œì‘ì ê³¼ ì¢…ë£Œì 
    start_pos = (1, 8)
    goal_pos = (8, 1)
    
    room_config = {
        'start_pos': start_pos,
        'goal_pos': goal_pos,
        'walls': walls,
        'objects': []  # box ê°ì²´ ì œê±°
    }
    
    return CustomRoomWrapper(size=size, room_config=room_config)


def visualize_grid_cli(wrapper: CustomRoomWrapper, state: dict):
    """CLIì—ì„œ ê·¸ë¦¬ë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ì‹œê°í™”"""
    env = wrapper.env
    size = wrapper.size
    
    # ì—ì´ì „íŠ¸ ìœ„ì¹˜ ë° ë°©í–¥
    agent_pos = state['agent_pos']
    if isinstance(agent_pos, np.ndarray):
        agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
    else:
        agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
    
    agent_dir = state['agent_dir']
    direction_symbols = {0: '>', 1: 'v', 2: '<', 3: '^'}
    agent_symbol = direction_symbols.get(agent_dir, 'A')
    
    # ê·¸ë¦¬ë“œ ìƒì„±
    grid_chars = []
    for y in range(size):
        row = []
        for x in range(size):
            cell = env.grid.get(x, y)
            
            if x == agent_x and y == agent_y:
                row.append(agent_symbol)
            elif cell is not None and cell.type == 'wall':
                # ìƒ‰ìƒì´ ìˆëŠ” ë²½ í‘œì‹œ
                if hasattr(cell, 'color'):
                    if cell.color == 'blue':
                        row.append('ğŸŸ¦')
                    elif cell.color == 'purple':
                        row.append('ğŸŸª')
                    elif cell.color == 'red':
                        row.append('ğŸŸ¥')
                    elif cell.color == 'green':
                        row.append('ğŸŸ©')
                    elif cell.color == 'yellow':
                        row.append('ğŸŸ¨')
                    else:
                        row.append('â¬›')  # ê¸°ë³¸ ìƒ‰ìƒ (grey)
                else:
                    row.append('â¬›')  # ìƒ‰ìƒ ì—†ìŒ
            elif cell is not None and cell.type == 'goal':
                row.append('ğŸŸ©')
            elif cell is not None:
                if hasattr(cell, 'color'):
                    if cell.color == 'blue':
                        row.append('ğŸŸ¦')
                    elif cell.color == 'purple':
                        row.append('ğŸŸª')
                    else:
                        row.append('ğŸŸ¨')
                else:
                    row.append('ğŸŸ¨')
            else:
                row.append('â¬œï¸')
        grid_chars.append(row)
    
    # ê·¸ë¦¬ë“œ ì¶œë ¥
    print("\n" + "=" * 60)
    print("Current Grid State:")
    print("=" * 60)
    for y in range(size):
        print(''.join(grid_chars[y]))
    print("=" * 60)
    print(f"Agent Position: ({agent_x}, {agent_y}), Direction: {agent_dir} ({agent_symbol})")
    print("=" * 60 + "\n")


def display_image(img, window_name="MiniGrid VLM Control (Improved)", cell_size=32):
    """OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ í‘œì‹œ"""
    if img is not None:
        try:
            img_bgr = cv2.cvtColor(img.copy(), cv2.COLOR_RGB2BGR)
            
            # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •
            height, width = img_bgr.shape[:2]
            max_size = 800
            if height < max_size and width < max_size:
                scale = min(max_size // height, max_size // width, 4)
                if scale > 1:
                    new_width = width * scale
                    new_height = height * scale
                    img_bgr = cv2.resize(img_bgr, (new_width, new_height), interpolation=cv2.INTER_NEAREST)
            
            cv2.imshow(window_name, img_bgr)
            cv2.waitKey(1)
        except Exception as e:
            print(f"Image display error: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 80)
    print("MiniGrid VLM ìƒí˜¸ì‘ìš© (ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ë²„ì „)")
    print("=" * 80)
    print("\nEnvironment Configuration:")
    print("  - Blue Pillar: 2x2 Grid")
    print("  - Table: ë³´ë¼ìƒ‰ 1x3 Grid")
    print("  - Start Point: (1, 8)")
    print("  - End Point: (8, 1)")
    print("\nMission: Blue Pillarìœ¼ë¡œ ê°€ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ëŒê³ , Table ì˜†ì— ë©ˆì¶”ì‹œì˜¤")
    print("\n[ê°œì„  ì‚¬í•­]")
    print("  - ì„±ê³µí•œ ì†”ë£¨ì…˜ B í”„ë¡¬í”„íŠ¸ ì ìš© (90% ì„±ê³µë¥ )")
    print("  - Allocentric vs Egocentric ì¢Œí‘œê³„ ëª…í™• êµ¬ë¶„")
    print("  - 5ë‹¨ê³„ ì¶”ë¡  í”„ë¡œì„¸ìŠ¤ ê°•ì œ")
    print("  - Lookup Table ê¸°ë°˜ ë³€í™˜ ë¡œì§")
    
    # í™˜ê²½ ìƒì„±
    print("\n[1] Creating environment...")
    wrapper = create_scenario2_environment()
    wrapper.reset()
    
    state = wrapper.get_state()
    print(f"Agent start position: {state['agent_pos']}")
    print(f"Agent direction: {state['agent_dir']}")
    print(f"ì—ì´ì „íŠ¸ Heading: {wrapper.get_heading()}")
    
    # VLM ì´ˆê¸°í™”
    print("\n[2] VLM ì´ˆê¸°í™” ì¤‘...")
    try:
        vlm = ChatGPT4oVLMWrapper(
            model=VLM_MODEL,
            temperature=VLM_TEMPERATURE,
            max_tokens=VLM_MAX_TOKENS
        )
        print(f"VLM initialization completed: {VLM_MODEL}")
    except Exception as e:
        print(f"VLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # PostProcessor ì´ˆê¸°í™”
    postprocessor = VLMResponsePostProcessor(required_fields=["action", "environment_info"])
    
    # ë©”ì¸ ë£¨í”„
    step = 0
    done = False
    WINDOW_NAME = "MiniGrid VLM Control (Improved)"
    
    print("\n" + "=" * 80)
    print("Experiment Started")
    print("=" * 80)
    
    while not done:
        step += 1
        print("\n" + "=" * 80)
        print(f"STEP {step}")
        print("=" * 80)
        
        # í˜„ì¬ ìƒíƒœ
        image = wrapper.get_image()
        state = wrapper.get_state()
        print(f"Position: {state['agent_pos']}, Direction: {state['agent_dir']} ({wrapper.get_heading()})")
        
        # CLI ì‹œê°í™”
        visualize_grid_cli(wrapper, state)
        
        # GUI ì‹œê°í™”
        display_image(image, WINDOW_NAME)
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ ì…ë ¥
        print("Enter command (Enter: ê¸°ë³¸ í”„ë¡¬í”„íŠ¸):")
        user_prompt = input("> ").strip()
        if not user_prompt:
            user_prompt = "Based on the current image, choose the next action to complete the mission: Go to the blue pillar, turn right, then stop next to the table."
        
        # System Prompt ìƒì„± (í˜„ì¬ ìƒíƒœ ê¸°ë°˜)
        system_prompt = get_system_prompt(wrapper)
        
        # VLM í˜¸ì¶œ
        print("\n[3] VLMì— ìš”ì²­ ì „ì†¡ ì¤‘...")
        try:
            vlm_response_raw = vlm.generate(
                image=image,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            print(f"VLM response received")
        except Exception as e:
            print(f"VLM API call failed: {e}")
            break
        
        # ì‘ë‹µ íŒŒì‹±
        print("[4] Parsing response...")
        try:
            vlm_response = postprocessor.process(vlm_response_raw, strict=False)
            action_str = vlm_response.get('action', '2')
            print(f"íŒŒì‹±ëœ ì•¡ì…˜: {action_str}")
            
            # Reasoning trace ì¶œë ¥
            if 'reasoning_trace' in vlm_response:
                trace = vlm_response['reasoning_trace']
                print("\n[ì¶”ë¡  ê³¼ì • (Reasoning Trace)]")
                if isinstance(trace, dict):
                    for step_name, content in trace.items():
                        print(f"  {step_name}: {content}")
                else:
                    print(f"  {trace}")
            
            print(f"\nEnvironment Info: {vlm_response.get('environment_info', 'N/A')}")
            print(f"Reasoning: {vlm_response.get('reasoning', 'N/A')}")
        except ValueError as e:
            print(f"Response parsing failed: {e}")
            print(f"ì›ë³¸ ì‘ë‹µ: {vlm_response_raw[:200]}...")
            action_str = '2'  # ê¸°ë³¸ê°’: move forward
        
        # ì•¡ì…˜ ì‹¤í–‰
        print(f"\n[5] ì•¡ì…˜ ì‹¤í–‰ ì¤‘...")
        try:
            action_index = wrapper.parse_action(action_str)
            action_name = wrapper.ACTION_NAMES.get(action_index, f"action_{action_index}")
            print(f"ì‹¤í–‰í•  ì•¡ì…˜: {action_name} (ì¸ë±ìŠ¤: {action_index})")
            
            _, reward, terminated, truncated, _ = wrapper.step(action_index)
            done = terminated or truncated
            
            print(f"ë³´ìƒ: {reward}, ì¢…ë£Œ: {done}")
        except Exception as e:
            print(f"ì•¡ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            # ê¸°ë³¸ ì•¡ì…˜ ì‚¬ìš©
            _, reward, terminated, truncated, _ = wrapper.step(2)
            done = terminated or truncated
        
        # ì—…ë°ì´íŠ¸ëœ ìƒíƒœ í‘œì‹œ
        new_state = wrapper.get_state()
        visualize_grid_cli(wrapper, new_state)
        updated_image = wrapper.get_image()
        display_image(updated_image, WINDOW_NAME)
        
        # ì¢…ë£Œ í™•ì¸
        if done:
            print("\n" + "=" * 80)
            print("Goal ë„ì°©! ì¢…ë£Œ")
            print("=" * 80)
            break
        
        # ìµœëŒ€ ìŠ¤í… ì œí•œ
        if step >= 100:
            print("\nìµœëŒ€ ìŠ¤í… ìˆ˜(100)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
            break
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cv2.destroyAllWindows()
    wrapper.close()
    print("\nì‹¤í—˜ ì™„ë£Œ.")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

