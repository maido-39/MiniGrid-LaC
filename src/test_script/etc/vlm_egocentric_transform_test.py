"""
VLA Egocentric Transform Test

VLA ëª¨ë¸ì˜ "Allocentric(ì ˆëŒ€ ì¢Œí‘œ) to Egocentric(ìƒëŒ€ ì¢Œí‘œ) ë³€í™˜ ì‹¤íŒ¨" ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸.

2ê°€ì§€ ì†”ë£¨ì…˜ì„ ë¹„êµ í…ŒìŠ¤íŠ¸:
- ì†”ë£¨ì…˜ B: CoT(Chain of Thought)ë¥¼ í†µí•œ ì¢Œí‘œ ë³€í™˜ ê°•ì œ
- ì†”ë£¨ì…˜ C: Visual Prompting (ì´ë¯¸ì§€ ì „ì²˜ë¦¬)

ì‚¬ìš©ë²•:
    python vlm_egocentric_transform_test.py
"""

from minigrid import register_minigrid_envs
# Actual path: legacy.relative_movement.custom_environment
from legacy import CustomRoomWrapper
# Actual paths: utils.vlm.vlm_wrapper, utils.vlm.vlm_postprocessor
from utils import ChatGPT4oVLMWrapper, VLMResponsePostProcessor
import numpy as np
import cv2
import json
import random
from typing import Dict, List, Tuple, Optional
from datetime import datetime
from pathlib import Path
from PIL import Image
import os

# MiniGrid í™˜ê²½ ë“±ë¡
register_minigrid_envs()

# VLM ì„¤ì •
VLM_MODEL = "gpt-4o"
VLM_TEMPERATURE = 0.0
VLM_MAX_TOKENS = 1000

# Mission ì„¤ì •
DEFAULT_MISSION = "Go to the blue pillar, turn right, then stop next to the table."


def create_scenario2_environment() -> CustomRoomWrapper:
    """ì‹œë‚˜ë¦¬ì˜¤ 2 í™˜ê²½ ìƒì„±"""
    size = 10
    
    walls = []
    for i in range(size):
        walls.append((i, 0))
        walls.append((i, size-1))
        walls.append((0, i))
        walls.append((size-1, i))
    
    blue_pillar_positions = [(3, 4), (4, 4), (3, 5), (4, 5)]
    for pos in blue_pillar_positions:
        walls.append((pos[0], pos[1], 'blue'))
    
    table_positions = [(5, 1), (6, 1), (7, 1)]
    for pos in table_positions:
        walls.append((pos[0], pos[1], 'purple'))
    
    start_pos = (1, 8)
    goal_pos = (8, 1)
    
    room_config = {
        'start_pos': start_pos,
        'goal_pos': goal_pos,
        'walls': walls,
        'objects': []
    }
    
    return CustomRoomWrapper(size=size, room_config=room_config)


def visualize_grid_cli(wrapper: CustomRoomWrapper, state: dict):
    """CLIì—ì„œ ê·¸ë¦¬ë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ì‹œê°í™”"""
    env = wrapper.env
    size = wrapper.size
    
    agent_pos = state['agent_pos']
    if isinstance(agent_pos, np.ndarray):
        agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
    else:
        agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
    
    agent_dir = state['agent_dir']
    direction_symbols = {0: '>', 1: 'v', 2: '<', 3: '^'}
    agent_symbol = direction_symbols.get(agent_dir, 'A')
    
    grid_chars = []
    for y in range(size):
        row = []
        for x in range(size):
            cell = env.grid.get(x, y)
            
            if x == agent_x and y == agent_y:
                row.append(agent_symbol)
            elif cell is not None and cell.type == 'wall':
                if hasattr(cell, 'color'):
                    if cell.color == 'blue':
                        row.append('ğŸŸ¦')
                    elif cell.color == 'purple':
                        row.append('ğŸŸª')
                    elif cell.color == 'red':
                        row.append('ğŸŸ¥')
                    elif cell.color == 'green':
                        row.append('ğŸŸ©')
                    elif cell.color == 'yellow':
                        row.append('ğŸŸ¨')
                    else:
                        row.append('â¬›')
                else:
                    row.append('â¬›')
            elif cell is not None and cell.type == 'goal':
                row.append('ğŸŸ©')
            elif cell is not None:
                if hasattr(cell, 'color'):
                    if cell.color == 'blue':
                        row.append('ğŸŸ¦')
                    elif cell.color == 'purple':
                        row.append('ğŸŸª')
                    else:
                        row.append('ğŸŸ¨')
                else:
                    row.append('ğŸŸ¨')
            else:
                row.append('â¬œï¸')
        grid_chars.append(row)
    
    print("\n" + "=" * 60)
    print("Current Grid State:")
    print("=" * 60)
    for y in range(size):
        print(''.join(grid_chars[y]))
    print("=" * 60)
    print(f"Agent Position: ({agent_x}, {agent_y}), Direction: {agent_dir} ({agent_symbol})")
    print("=" * 60 + "\n")


def display_image(img: np.ndarray, window_name: str = "VLM Egocentric Transform Test"):
    """ì´ë¯¸ì§€ í‘œì‹œ (GUI ë¹„í™œì„±í™” - ì„œë²„ í™˜ê²½ì—ì„œ ì‹¤í–‰ ì‹œ í•„ìš”)"""
    # GUI í‘œì‹œ ë¹„í™œì„±í™” - ì´ë¯¸ì§€ëŠ” íŒŒì¼ë¡œë§Œ ì €ì¥
    pass


class SolutionB_CoTReasoning:
    """ì†”ë£¨ì…˜ B: CoT(Chain of Thought)ë¥¼ í†µí•œ ì¢Œí‘œ ë³€í™˜ ê°•ì œ"""
    
    def __init__(self, vlm: ChatGPT4oVLMWrapper, postprocessor: VLMResponsePostProcessor):
        self.vlm = vlm
        self.postprocessor = postprocessor
    
    def get_heading_info(self, wrapper: CustomRoomWrapper) -> str:
        """Heading ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        heading = wrapper.get_heading()
        heading_short = wrapper.get_heading_short()
        return f"{heading} ({heading_short})"
    
    def get_system_prompt(self, wrapper: CustomRoomWrapper) -> str:
        """CoT ê°•ì œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        heading_info = self.get_heading_info(wrapper)
        
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- Do NOT infer or reinterpret the robot's heading from the image.

## Coordinate Convention
- Top of the image: North
- Bottom of the image: South
- Left of the image: West
- Right of the image: East

## Environment
Grid world with:
- Walls (black, impassable)
- Blue pillar (impassable)
- Purple table (impassable)
- Robot (red arrow marker)
- Goal (green marker, if present)

The image describes the environment layout ONLY.
Do NOT use the image to estimate robot orientation.

## Action Space
- "turn left": Rotate 90Â° counterclockwise
- "turn right": Rotate 90Â° clockwise
- "move forward": Move one cell forward in heading direction
- "pickup": Pick up object in front
- "drop": Drop carried object
- "toggle": Interact with objects (e.g., open doors)

## Movement Rules (CRITICAL: EXECUTE STEP-BY-STEP)
You must perform a mental coordinate transformation. Do NOT trust "Up" in the image as "Front".

1. **Identify Global Position**: Where is the target object in the image? (e.g., Top=North, Right=East)
2. **Confirm Robot Heading**: Which compass direction is the robot facing? (Provided in Robot State)
3. **Calculate Relative Position**:
   - IF Object is North AND Robot faces East -> Object is on the LEFT.
   - IF Object is North AND Robot faces West -> Object is on the RIGHT.
   - IF Object is East AND Robot faces North -> Object is on the RIGHT.
   - IF Object is West AND Robot faces North -> Object is on the LEFT.
   - (Derive strictly based on rotation)

Rules:
- All movements are RELATIVE to the robot's current heading.
- "move forward" moves one cell in the facing direction.
- "turn left/right" rotates 90Â° relative to current heading.
- Do NOT reason using absolute coordinates when choosing actions.

## Response Format (STRICT)
Respond in valid JSON. You MUST fill strictly following the "reasoning_trace" logic.

```json
{{
  "reasoning_trace": {{
    "target_global_pos": "<e.g. The blue pillar is at the Top (North) of the grid>",
    "robot_heading": "<e.g. East>",
    "calculation": "<e.g. North is 90 degrees counter-clockwise from East.>",
    "relative_pos": "<e.g. Therefore, the pillar is to my Left.>"
  }},
  "action": ["<action1>", "<action2>", "<action3>"]
}}
```

Important:
- EXACTLY 3 actions must be provided.
- Only the first action will be executed.
- Actions must come from the defined action space.
- Complete the reasoning_trace before selecting actions.
- Complete the mission specified by the user.
"""
    
    def test(self, image: np.ndarray, wrapper: CustomRoomWrapper, user_prompt: str) -> Dict:
        """ì†”ë£¨ì…˜ B í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        system_prompt = self.get_system_prompt(wrapper)
        
        print("\n[ì†”ë£¨ì…˜ B] CoT ê°•ì œ í”„ë¡¬í”„íŠ¸ë¡œ VLM í˜¸ì¶œ ì¤‘...")
        try:
            raw_response = self.vlm.generate(
                image=image,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            if not raw_response:
                print("VLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return {}
            
            print("VLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            parsed = self.postprocessor.process(raw_response, strict=False)
            return parsed
        except Exception as e:
            print(f"VLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {}


class SolutionC_VisualPrompting:
    """ì†”ë£¨ì…˜ C: Visual Prompting (ì´ë¯¸ì§€ ì „ì²˜ë¦¬)"""
    
    def __init__(self, vlm: ChatGPT4oVLMWrapper, postprocessor: VLMResponsePostProcessor):
        self.vlm = vlm
        self.postprocessor = postprocessor
    
    def get_heading_info(self, wrapper: CustomRoomWrapper) -> str:
        """Heading ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
        heading = wrapper.get_heading()
        heading_short = wrapper.get_heading_short()
        return f"{heading} ({heading_short})"
    
    def preprocess_image(self, image: np.ndarray, wrapper: CustomRoomWrapper) -> np.ndarray:
        """ì´ë¯¸ì§€ì— Visual Prompting ì¶”ê°€ (ë¡œë´‡ì˜ ì‹œì•¼ ë°©í–¥ í‘œì‹œ)"""
        # ì´ë¯¸ì§€ ë³µì‚¬
        processed_image = image.copy()
        
        # ë¡œë´‡ ìœ„ì¹˜ ë° ë°©í–¥ ê°€ì ¸ì˜¤ê¸°
        state = wrapper.get_state()
        agent_pos = state['agent_pos']
        if isinstance(agent_pos, np.ndarray):
            agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
        else:
            agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
        
        agent_dir = state['agent_dir']
        heading = wrapper.get_heading()
        
        # ì…€ í¬ê¸° (MiniGridëŠ” ì¼ë°˜ì ìœ¼ë¡œ 32x32 í”½ì…€)
        cell_size = 32
        
        # ë¡œë´‡ ìœ„ì¹˜ë¥¼ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
        robot_pixel_x = agent_x * cell_size + cell_size // 2
        robot_pixel_y = agent_y * cell_size + cell_size // 2
        
        # ë°©í–¥ ë²¡í„° (0=ì˜¤ë¥¸ìª½/East, 1=ì•„ë˜/South, 2=ì™¼ìª½/West, 3=ìœ„/North)
        dir_vectors = {
            0: (1, 0),   # East (ì˜¤ë¥¸ìª½)
            1: (0, 1),   # South (ì•„ë˜)
            2: (-1, 0),  # West (ì™¼ìª½)
            3: (0, -1)   # North (ìœ„)
        }
        
        forward_dx, forward_dy = dir_vectors[agent_dir]
        
        # í™”ì‚´í‘œ ê·¸ë¦¬ê¸° (ë¡œë´‡ì˜ ì• ë°©í–¥)
        arrow_length = cell_size * 2
        arrow_end_x = robot_pixel_x + forward_dx * arrow_length
        arrow_end_y = robot_pixel_y + forward_dy * arrow_length
        
        # í™”ì‚´í‘œ ìƒ‰ìƒ (ë¹¨ê°„ìƒ‰)
        arrow_color = (255, 0, 0)
        arrow_thickness = 3
        
        # í™”ì‚´í‘œ ì„  ê·¸ë¦¬ê¸°
        cv2.arrowedLine(
            processed_image,
            (robot_pixel_x, robot_pixel_y),
            (arrow_end_x, arrow_end_y),
            arrow_color,
            arrow_thickness,
            tipLength=0.3
        )
        
        # "Front" í…ìŠ¤íŠ¸ ì¶”ê°€
        text_x = arrow_end_x + forward_dx * 10
        text_y = arrow_end_y + forward_dy * 10
        cv2.putText(
            processed_image,
            "Front",
            (text_x, text_y),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.6,
            arrow_color,
            2
        )
        
        # Heading ì •ë³´ í…ìŠ¤íŠ¸ ì¶”ê°€ (ì¢Œìƒë‹¨)
        heading_text = f"Heading: {heading}"
        cv2.putText(
            processed_image,
            heading_text,
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.7,
            (255, 255, 255),
            2
        )
        
        return processed_image
    
    def get_system_prompt(self, wrapper: CustomRoomWrapper) -> str:
        """Visual Prompting í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        heading_info = self.get_heading_info(wrapper)
        
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- The image contains a RED ARROW pointing in the robot's forward direction.
- The arrow labeled "Front" shows where the robot is facing.

## Coordinate Convention
- Top of the image: North
- Bottom of the image: South
- Left of the image: West
- Right of the image: East

## Environment
Grid world with:
- Walls (black, impassable)
- Blue pillar (impassable)
- Purple table (impassable)
- Robot (red arrow marker)
- Goal (green marker, if present)

## Visual Cues in Image
- RED ARROW: Points in the robot's forward-facing direction (heading)
- "Front" label: Indicates the direction the robot is facing
- Use the arrow direction to determine relative positions, NOT the image orientation

## Action Space
- "turn left": Rotate 90Â° counterclockwise
- "turn right": Rotate 90Â° clockwise
- "move forward": Move one cell forward in heading direction
- "pickup": Pick up object in front
- "drop": Drop carried object
- "toggle": Interact with objects (e.g., open doors)

## Movement Rules (CRITICAL)
- All movements are RELATIVE to the robot's current heading (shown by the RED ARROW).
- The arrow direction is the robot's "forward" direction.
- Objects to the left/right of the arrow are on the robot's left/right.
- "move forward" moves one cell in the arrow direction.
- "turn left/right" rotates 90Â° relative to current heading.

## Response Format (STRICT)
Respond in valid JSON:

```json
{{
  "action": ["<action1>", "<action2>", "<action3>"],
  "reasoning": "<explanation of why you chose this action based on the arrow direction>"
}}
```

Important:
- EXACTLY 3 actions must be provided.
- Only the first action will be executed.
- Actions must come from the defined action space.
- Use the RED ARROW direction to determine relative positions.
- Complete the mission specified by the user.
"""
    
    def test(self, image: np.ndarray, wrapper: CustomRoomWrapper, user_prompt: str) -> Dict:
        """ì†”ë£¨ì…˜ C í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
        processed_image = self.preprocess_image(image, wrapper)
        
        system_prompt = self.get_system_prompt(wrapper)
        
        print("\n[ì†”ë£¨ì…˜ C] Visual Promptingìœ¼ë¡œ VLM í˜¸ì¶œ ì¤‘...")
        try:
            raw_response = self.vlm.generate(
                image=processed_image,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            if not raw_response:
                print("VLM ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return {}
            
            print("VLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            parsed = self.postprocessor.process(raw_response, strict=False)
            return parsed
        except Exception as e:
            print(f"VLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {}


class EgocentricTransformTest:
    """VLA Egocentric Transform í…ŒìŠ¤íŠ¸ ë©”ì¸ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.wrapper = None
        self.vlm = ChatGPT4oVLMWrapper(
            model=VLM_MODEL,
            temperature=VLM_TEMPERATURE,
            max_tokens=VLM_MAX_TOKENS
        )
        self.postprocessor = VLMResponsePostProcessor(required_fields=["action"])
        
        self.solution_b = SolutionB_CoTReasoning(self.vlm, self.postprocessor)
        self.solution_c = SolutionC_VisualPrompting(self.vlm, self.postprocessor)
    
    def initialize(self):
        """í…ŒìŠ¤íŠ¸ ì´ˆê¸°í™”"""
        print("=" * 60)
        print("VLA Egocentric Transform Test")
        print("=" * 60)
        print("\ní™˜ê²½ êµ¬ì„±:")
        print("  - íŒŒë€ ê¸°ë‘¥: 2x2 Grid (ìƒ‰ìƒì´ ìˆëŠ” ë²½)")
        print("  - í…Œì´ë¸”: ë³´ë¼ìƒ‰ 1x3 Grid (ìƒ‰ìƒì´ ìˆëŠ” ë²½)")
        print("  - ì‹œì‘ì : (1, 8)")
        print("  - ì¢…ë£Œì : (8, 1)")
        print(f"\nMission: {DEFAULT_MISSION}")
        
        print("\n[1] í™˜ê²½ ìƒì„± ì¤‘...")
        self.wrapper = create_scenario2_environment()
        self.wrapper.reset()
        
        state = self.wrapper.get_state()
        print(f"ì—ì´ì „íŠ¸ ì‹œì‘ ìœ„ì¹˜: {state['agent_pos']}")
        print(f"ì—ì´ì „íŠ¸ ë°©í–¥: {state['agent_dir']}")
        heading = self.wrapper.get_heading()
        print(f"ì—ì´ì „íŠ¸ Heading: {heading}")
        
        print("\n[2] VLM ì´ˆê¸°í™” ì™„ë£Œ")
        print("\n" + "=" * 60)
        print("í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 60)
    
    def run_comparison_test(self):
        """2ê°€ì§€ ì†”ë£¨ì…˜ ë¹„êµ í…ŒìŠ¤íŠ¸"""
        # í™˜ê²½ ë¦¬ì…‹í•˜ì—¬ ë™ì¼í•œ ì´ˆê¸° ìƒíƒœ ë³´ì¥
        self.wrapper.reset()
        
        # í˜„ì¬ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        image = self.wrapper.get_image()
        state = self.wrapper.get_state()
        
        # Heading ì •ë³´ ì¶œë ¥
        heading = self.wrapper.get_heading()
        heading_desc = self.wrapper.get_heading_description()
        print(f"\nìœ„ì¹˜: {state['agent_pos']}, ë°©í–¥: {state['agent_dir']} ({heading})")
        print(f"í˜„ì¬ Heading: {heading_desc}")
        
        # ê·¸ë¦¬ë“œ ì‹œê°í™”
        visualize_grid_cli(self.wrapper, state)
        
        # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        user_prompt = f"Mission: {DEFAULT_MISSION}\n\nBased on the current image, choose the next action to complete this task."
        
        # ê²°ê³¼ ì €ì¥
        results = {}
        
        # ì†”ë£¨ì…˜ B í…ŒìŠ¤íŠ¸
        print("\n" + "=" * 80)
        print("ì†”ë£¨ì…˜ B: CoT ê°•ì œ (Chain of Thought)")
        print("=" * 80)
        display_image(image, "Solution B: CoT Reasoning")
        
        result_b = self.solution_b.test(image, self.wrapper, user_prompt)
        results['solution_b'] = result_b
        
        if result_b:
            print("\n[ì†”ë£¨ì…˜ B ê²°ê³¼]")
            print("-" * 80)
            action = result_b.get('action', [])
            if isinstance(action, str):
                action = [action]
            if not isinstance(action, list):
                action = [str(action)]
            
            print(f"Action: {action[0] if action else 'N/A'}")
            
            reasoning_trace = result_b.get('reasoning_trace', {})
            if isinstance(reasoning_trace, dict):
                print(f"Target Global Pos: {reasoning_trace.get('target_global_pos', 'N/A')}")
                print(f"Robot Heading: {reasoning_trace.get('robot_heading', 'N/A')}")
                print(f"Calculation: {reasoning_trace.get('calculation', 'N/A')}")
                print(f"Relative Pos: {reasoning_trace.get('relative_pos', 'N/A')}")
            else:
                print(f"Reasoning Trace: {reasoning_trace}")
        
        # ì†”ë£¨ì…˜ C í…ŒìŠ¤íŠ¸ (í™˜ê²½ ë¦¬ì…‹í•˜ì—¬ ë™ì¼í•œ ì´ˆê¸° ìƒíƒœ ë³´ì¥)
        self.wrapper.reset()
        image_c = self.wrapper.get_image()
        
        print("\n" + "=" * 80)
        print("ì†”ë£¨ì…˜ C: Visual Prompting (ì´ë¯¸ì§€ ì „ì²˜ë¦¬)")
        print("=" * 80)
        
        result_c = self.solution_c.test(image_c, self.wrapper, user_prompt)
        results['solution_c'] = result_c
        
        # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ í‘œì‹œ
        processed_image = self.solution_c.preprocess_image(image_c, self.wrapper)
        display_image(processed_image, "Solution C: Visual Prompting")
        
        if result_c:
            print("\n[ì†”ë£¨ì…˜ C ê²°ê³¼]")
            print("-" * 80)
            action = result_c.get('action', [])
            if isinstance(action, str):
                action = [action]
            if not isinstance(action, list):
                action = [str(action)]
            
            print(f"Action: {action[0] if action else 'N/A'}")
            print(f"Reasoning: {result_c.get('reasoning', 'N/A')}")
        
        # ê²°ê³¼ ë¹„êµ
        print("\n" + "=" * 80)
        print("ê²°ê³¼ ë¹„êµ")
        print("=" * 80)
        
        action_b = results.get('solution_b', {}).get('action', [])
        if isinstance(action_b, str):
            action_b = [action_b]
        if not isinstance(action_b, list):
            action_b = [str(action_b)]
        action_b = action_b[0] if action_b else None
        
        action_c = results.get('solution_c', {}).get('action', [])
        if isinstance(action_c, str):
            action_c = [action_c]
        if not isinstance(action_c, list):
            action_c = [str(action_c)]
        action_c = action_c[0] if action_c else None
        
        print(f"ì†”ë£¨ì…˜ B (CoT) ì„ íƒí•œ ì•¡ì…˜: {action_b}")
        print(f"ì†”ë£¨ì…˜ C (Visual) ì„ íƒí•œ ì•¡ì…˜: {action_c}")
        
        # ì˜ˆìƒ ì •ë‹µ: ë¡œë´‡ì´ Eastë¥¼ í–¥í•˜ê³ , íŒŒë€ ê¸°ë‘¥ì´ Northì— ìˆìœ¼ë¯€ë¡œ "turn left"ê°€ ì •ë‹µ
        expected_action = "turn left"
        print(f"\nì˜ˆìƒ ì •ë‹µ: {expected_action} (ë¡œë´‡ì´ Eastë¥¼ í–¥í•˜ê³ , íŒŒë€ ê¸°ë‘¥ì´ Northì— ìˆìœ¼ë¯€ë¡œ ì™¼ìª½ìœ¼ë¡œ íšŒì „)")
        
        if action_b == expected_action:
            print("âœ“ ì†”ë£¨ì…˜ B: ì •ë‹µ!")
        else:
            print(f"âœ— ì†”ë£¨ì…˜ B: ì˜¤ë‹µ (ì˜ˆìƒ: {expected_action})")
        
        if action_c == expected_action:
            print("âœ“ ì†”ë£¨ì…˜ C: ì •ë‹µ!")
        else:
            print(f"âœ— ì†”ë£¨ì…˜ C: ì˜¤ë‹µ (ì˜ˆìƒ: {expected_action})")
        
        return results
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        # GUI ë¹„í™œì„±í™”ë¡œ cv2.destroyAllWindows() ì œê±°
        if self.wrapper:
            self.wrapper.close()
        print("\ní…ŒìŠ¤íŠ¸ ì™„ë£Œ.")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    try:
        test = EgocentricTransformTest()
        test.initialize()
        test.run_comparison_test()
        test.cleanup()
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

