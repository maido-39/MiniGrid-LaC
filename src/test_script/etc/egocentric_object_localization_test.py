"""
Egocentric Object Localization Test
ë¡œë´‡ heading ê¸°ì¤€ egocentric ì¢Œí‘œê³„ì—ì„œ ë¬¼ì²´ ìœ„ì¹˜ í‘œí˜„ ì„±ëŠ¥ ê°œì„ 

ëª©í‘œ: ë¡œë´‡ headingê³¼ egocentric ì¢Œí‘œê³„ ê¸°ì¤€ ë¬¼ì²´ ê´€ê³„ ì¶”ì¸¡ ì„±ê³µë¥  90% ì´ìƒ ë‹¬ì„±
"""

from minigrid import register_minigrid_envs
# Actual path: legacy.relative_movement.custom_environment
from legacy import CustomRoomWrapper
# Actual paths: utils.vlm.vlm_wrapper, utils.vlm.vlm_postprocessor
from utils import ChatGPT4oVLMWrapper, VLMResponsePostProcessor
import numpy as np
import json
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

# MiniGrid í™˜ê²½ ë“±ë¡
register_minigrid_envs()

# VLM ì„¤ì •
VLM_MODEL = "gpt-4o"
VLM_TEMPERATURE = 0.0
VLM_MAX_TOKENS = 2000


def create_random_environment_with_objects(seed: Optional[int] = None) -> Tuple[CustomRoomWrapper, Dict]:
    """ëœë¤ í™˜ê²½ ìƒì„± (ë¬¼ì²´ ìƒ‰, ìœ„ì¹˜ ëœë¤í™”)"""
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
    
    size = 10
    
    # ì™¸ë²½ ìƒì„±
    walls = []
    for i in range(size):
        walls.append((i, 0))
        walls.append((i, size-1))
        walls.append((0, i))
        walls.append((size-1, i))
    
    # ëœë¤ ì‹œì‘ ìœ„ì¹˜ (ë²½ ì œì™¸)
    start_x = random.randint(1, size-2)
    start_y = random.randint(1, size-2)
    start_pos = (start_x, start_y)
    
    # ëœë¤ ì‹œì‘ ë°©í–¥
    start_dir = random.randint(0, 3)  # 0: East, 1: South, 2: West, 3: North
    
    # ëœë¤ ëª©í‘œ ìœ„ì¹˜
    goal_x = random.randint(1, size-2)
    goal_y = random.randint(1, size-2)
    # ì‹œì‘ ìœ„ì¹˜ì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡
    while (goal_x, goal_y) == start_pos:
        goal_x = random.randint(1, size-2)
        goal_y = random.randint(1, size-2)
    goal_pos = (goal_x, goal_y)
    
    # ëœë¤ ìƒ‰ìƒ ë¬¼ì²´ ìƒì„± (1-3ê°œ)
    num_objects = random.randint(1, 3)
    object_colors = ['blue', 'purple', 'red', 'green', 'yellow']
    objects = []
    
    for i in range(num_objects):
        # ëœë¤ ìƒ‰ìƒ ì„ íƒ
        color = random.choice(object_colors)
        
        # ëœë¤ í¬ê¸° (1x1 ë˜ëŠ” 2x2)
        obj_size = random.choice([1, 2])
        
        # ëœë¤ ìœ„ì¹˜ (ì‹œì‘ ìœ„ì¹˜, ëª©í‘œ ìœ„ì¹˜ì™€ ê²¹ì¹˜ì§€ ì•Šë„ë¡)
        max_attempts = 50
        placed = False
        for _ in range(max_attempts):
            obj_x = random.randint(1, size-2)
            obj_y = random.randint(1, size-2)
            
            # ì‹œì‘ ìœ„ì¹˜, ëª©í‘œ ìœ„ì¹˜ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            if (obj_x, obj_y) == start_pos or (obj_x, obj_y) == goal_pos:
                continue
            
            # ë‹¤ë¥¸ ë¬¼ì²´ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
            overlap = False
            for existing_obj in objects:
                ex_x, ex_y, ex_size, _ = existing_obj
                if abs(obj_x - ex_x) < ex_size and abs(obj_y - ex_y) < ex_size:
                    overlap = True
                    break
            
            if not overlap:
                objects.append((obj_x, obj_y, obj_size, color))
                placed = True
                break
        
        if not placed:
            continue
    
    # ë¬¼ì²´ë¥¼ ë²½ìœ¼ë¡œ ì¶”ê°€
    for obj_x, obj_y, obj_size, color in objects:
        for dx in range(obj_size):
            for dy in range(obj_size):
                walls.append((obj_x + dx, obj_y + dy, color))
    
    room_config = {
        'start_pos': start_pos,
        'goal_pos': goal_pos,
        'walls': walls,
        'objects': []
    }
    
    wrapper = CustomRoomWrapper(size=size, room_config=room_config)
    wrapper.reset()
    
    # ì‹œì‘ ë°©í–¥ ì„¤ì • (ì§ì ‘ ì„¤ì •)
    wrapper.env.agent_dir = start_dir
    
    # í˜„ì¬ ìƒíƒœ í™•ì¸
    state = wrapper.get_state()
    actual_pos = state['agent_pos']
    if isinstance(actual_pos, np.ndarray):
        actual_pos = (int(actual_pos[0]), int(actual_pos[1]))
    else:
        actual_pos = (int(actual_pos[0]), int(actual_pos[1]))
    actual_dir = int(state['agent_dir'])
    
    # í™˜ê²½ ì •ë³´
    env_info = {
        'agent_pos': actual_pos,
        'agent_dir': actual_dir,
        'agent_heading': wrapper.get_heading(),
        'goal_pos': goal_pos,
        'objects': []
    }
    
    # ë¬¼ì²´ ì •ë³´ ì¶”ê°€
    for obj_x, obj_y, obj_size, color in objects:
        positions = []
        for dx in range(obj_size):
            for dy in range(obj_size):
                positions.append((int(obj_x + dx), int(obj_y + dy)))
        env_info['objects'].append({
            'color': color,
            'size': int(obj_size),
            'positions': positions,
            'center': (int(obj_x + obj_size // 2), int(obj_y + obj_size // 2))
        })
    
    return wrapper, env_info


def calculate_gt_egocentric_position(agent_pos: Tuple[int, int], agent_dir: int, 
                                     object_pos: Tuple[int, int]) -> str:
    """GT Egocentric ìœ„ì¹˜ ê³„ì‚° (ì •í™•í•œ ë³€í™˜)"""
    agent_x, agent_y = agent_pos
    obj_x, obj_y = object_pos
    
    # Allocentric ì°¨ì´ ê³„ì‚°
    dx = obj_x - agent_x
    dy = obj_y - agent_y
    
    # ë°©í–¥ì— ë”°ë¥¸ ë³€í™˜
    # 0: East (â†’), 1: South (â†“), 2: West (â†), 3: North (â†‘)
    # MiniGrid ì¢Œí‘œê³„: (0,0)ì´ ì™¼ìª½ ìœ„, xëŠ” ì˜¤ë¥¸ìª½, yëŠ” ì•„ë˜
    
    if agent_dir == 0:  # East (â†’)
        # ì•: +x, ì™¼ìª½: -y (North), ì˜¤ë¥¸ìª½: +y (South), ë’¤: -x (West)
        if dx > 0:
            return "front"
        elif dx < 0:
            return "back"
        elif dy < 0:  # North (ìœ„ìª½)
            return "left"
        else:  # dy > 0, South (ì•„ë˜ìª½)
            return "right"
    elif agent_dir == 1:  # South (â†“)
        # ì•: +y, ì™¼ìª½: +x (East), ì˜¤ë¥¸ìª½: -x (West), ë’¤: -y (North)
        if dy > 0:
            return "front"
        elif dy < 0:
            return "back"
        elif dx > 0:  # East (ì˜¤ë¥¸ìª½)
            return "left"
        else:  # dx < 0, West (ì™¼ìª½)
            return "right"
    elif agent_dir == 2:  # West (â†)
        # ì•: -x, ì™¼ìª½: +y (South), ì˜¤ë¥¸ìª½: -y (North), ë’¤: +x (East)
        if dx < 0:
            return "front"
        elif dx > 0:
            return "back"
        elif dy > 0:  # South (ì•„ë˜ìª½)
            return "left"
        else:  # dy < 0, North (ìœ„ìª½)
            return "right"
    else:  # North (agent_dir == 3) (â†‘)
        # ì•: -y, ì™¼ìª½: -x (West), ì˜¤ë¥¸ìª½: +x (East), ë’¤: +y (South)
        if dy < 0:
            return "front"
        elif dy > 0:
            return "back"
        elif dx < 0:  # West (ì™¼ìª½)
            return "left"
        else:  # dx > 0, East (ì˜¤ë¥¸ìª½)
            return "right"


class EgocentricLocalizationSolution:
    """Egocentric ì¢Œí‘œê³„ ë¬¼ì²´ ìœ„ì¹˜ ì¶”ë¡  ì†”ë£¨ì…˜"""
    
    def __init__(self, vlm: ChatGPT4oVLMWrapper, postprocessor: VLMResponsePostProcessor, prompt_variant: int = 0):
        self.vlm = vlm
        self.postprocessor = postprocessor
        self.prompt_variant = prompt_variant
    
    def get_system_prompt(self, wrapper: CustomRoomWrapper, objects_info: List[Dict]) -> str:
        """System Prompt ìƒì„±"""
        heading = wrapper.get_heading()
        heading_short = wrapper.get_heading_short()
        heading_info = f"{heading} ({heading_short})"
        
        # ë¬¼ì²´ ì •ë³´ ë¬¸ìì—´ ìƒì„±
        objects_str = ""
        for i, obj in enumerate(objects_info):
            objects_str += f"\n- Object {i+1}: {obj['color']} color, size {obj['size']}x{obj['size']}"
        
        if self.prompt_variant == 0:
            return self._get_base_prompt(heading_info, objects_str)
        elif self.prompt_variant == 1:
            return self._get_enhanced_prompt(heading_info, objects_str)
        elif self.prompt_variant >= 2:
            return self._get_detailed_prompt(heading_info, objects_str)
        else:
            return self._get_base_prompt(heading_info, objects_str)
    
    def _get_base_prompt(self, heading_info: str, objects_str: str) -> str:
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸"""
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.

## Objects in Environment
{objects_str}

## Task
Your task is to identify the egocentric (relative) position of each object relative to the robot's current heading.

## Egocentric Coordinate System
- **Front**: In the direction the robot is facing (heading direction)
- **Back**: Opposite to the heading direction
- **Left**: 90 degrees counterclockwise from heading
- **Right**: 90 degrees clockwise from heading

## Response Format
Respond in valid JSON:
```json
{{
  "objects": [
    {{
      "color": "<color>",
      "egocentric_position": "<front|back|left|right>",
      "reasoning": "<explanation>"
    }}
  ]
}}
```

Important:
- Identify ALL objects in the environment
- Use egocentric coordinates (front/back/left/right) relative to robot heading
- Complete the reasoning for each object
"""
    
    def _get_enhanced_prompt(self, heading_info: str, objects_str: str) -> str:
        """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸"""
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.

## Objects in Environment
{objects_str}

## Task
Your task is to identify the egocentric (relative) position of each object relative to the robot's current heading.

## CRITICAL: Two Coordinate Systems

### 1. ALLOCENTRIC (Absolute/Global) Coordinates
- Used in the IMAGE: Top=North, Bottom=South, Left=West, Right=East
- This is FIXED and does NOT change with robot orientation

### 2. EGOCENTRIC (Relative/Robot-centric) Coordinates
- Used for OBJECT POSITIONS: Front/Back/Left/Right relative to heading
- This CHANGES when the robot rotates

## Transformation Process (STEP-BY-STEP)

**STEP 1: Identify object position in ALLOCENTRIC coordinates**
- Look at the image
- Find each object
- Note its position: Is it at the Top (North), Bottom (South), Left (West), or Right (East) of the image?

**STEP 2: Get robot heading (provided)**
- Robot heading: {heading_info}
- This tells you which direction the robot is facing in ALLOCENTRIC coordinates

**STEP 3: Transform from ALLOCENTRIC to EGOCENTRIC**
Use this EXACT lookup table:

| Robot Heading | Object at North | Object at South | Object at East | Object at West |
|---------------|------------------|------------------|----------------|----------------|
| East (â†’)      | LEFT             | RIGHT            | FRONT          | BACK           |
| West (â†)      | RIGHT            | LEFT             | BACK           | FRONT          |
| North (â†‘)     | FRONT            | BACK             | RIGHT          | LEFT           |
| South (â†“)     | BACK             | FRONT            | LEFT           | RIGHT          |

**STEP 4: Determine egocentric position**
- Apply the transformation for each object
- Use the result as the egocentric position

## Response Format
Respond in valid JSON:
```json
{{
  "reasoning_trace": {{
    "step1_allocentric": "<object positions in allocentric coordinates>",
    "step2_robot_heading": "<robot heading>",
    "step3_transformation": "<transformation applied>",
    "step4_egocentric": "<final egocentric positions>"
  }},
  "objects": [
    {{
      "color": "<color>",
      "egocentric_position": "<front|back|left|right>",
      "reasoning": "<explanation>"
    }}
  ]
}}
```

Important:
- Identify ALL objects in the environment
- Complete ALL 4 steps in reasoning_trace
- Use egocentric coordinates (front/back/left/right) relative to robot heading
"""
    
    def _get_detailed_prompt(self, heading_info: str, objects_str: str) -> str:
        """ìƒì„¸ í”„ë¡¬í”„íŠ¸"""
        return f"""You are a robot operating in a grid-based environment.

## Robot State (Authoritative)
- The robot's current heading is {heading_info}.
- Heading indicates the robot's forward-facing direction.
- This heading is ground-truth and MUST be used as-is.
- Do NOT infer or reinterpret the robot's heading from the image.

## Objects in Environment
{objects_str}

## Task
Your task is to identify the egocentric (relative) position of EACH object relative to the robot's current heading.

## CRITICAL DISTINCTION: Two Coordinate Systems

### 1. ALLOCENTRIC (Absolute/Global) Coordinates
- Used in the IMAGE: Top=North, Bottom=South, Left=West, Right=East
- This is FIXED and does NOT change with robot orientation
- The image shows objects in this coordinate system

### 2. EGOCENTRIC (Relative/Robot-centric) Coordinates
- Used for OBJECT POSITIONS: Front/Back/Left/Right relative to heading
- This CHANGES when the robot rotates
- Objects must be described in this coordinate system

## Transformation Process (CRITICAL: EXECUTE STEP-BY-STEP FOR EACH OBJECT)

**STEP 1: Identify object position in ALLOCENTRIC coordinates**
- Look at the image
- Find the object
- Determine its position: Top (North), Bottom (South), Left (West), or Right (East) of the image
- If the object spans multiple directions, use the CENTER of the object

**STEP 2: Get robot heading (provided)**
- Robot heading: {heading_info}
- This tells you which direction the robot is facing in ALLOCENTRIC coordinates

**STEP 3: Transform from ALLOCENTRIC to EGOCENTRIC**
Use this EXACT lookup table:

| Robot Heading | Object at North | Object at South | Object at East | Object at West |
|---------------|------------------|------------------|----------------|----------------|
| East (â†’)      | LEFT             | RIGHT            | FRONT          | BACK           |
| West (â†)      | RIGHT            | LEFT             | BACK           | FRONT          |
| North (â†‘)     | FRONT            | BACK             | RIGHT          | LEFT           |
| South (â†“)     | BACK             | FRONT            | LEFT           | RIGHT          |

**STEP 4: Determine egocentric position**
- Apply the transformation result
- Use: "front", "back", "left", or "right" (lowercase)

## Response Format (STRICT)
Respond in valid JSON. You MUST fill strictly following the format:

```json
{{
  "reasoning_trace": {{
    "step1_allocentric": "<For each object, identify its allocentric position (North/South/East/West)>",
    "step2_robot_heading": "<{heading_info}>",
    "step3_transformation": "<For each object, apply transformation using lookup table>",
    "step4_egocentric": "<Final egocentric positions for all objects>"
  }},
  "objects": [
    {{
      "color": "<color>",
      "egocentric_position": "<front|back|left|right>",
      "reasoning": "<step-by-step explanation for this specific object>"
    }}
  ]
}}
```

Important:
- Identify ALL objects in the environment
- Complete ALL 4 steps in reasoning_trace
- Use lowercase: "front", "back", "left", "right"
- Provide reasoning for EACH object separately
- The egocentric_position must match the transformation result
"""
    
    def test(self, image: np.ndarray, wrapper: CustomRoomWrapper, objects_info: List[Dict]) -> Dict:
        """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        system_prompt = self.get_system_prompt(wrapper, objects_info)
        
        user_prompt = "Identify the egocentric position (front, back, left, or right) of each object relative to the robot's current heading."
        
        try:
            raw_response = self.vlm.generate(
                image=image,
                system_prompt=system_prompt,
                user_prompt=user_prompt
            )
            
            if not raw_response:
                return {}
            
            parsed = self.postprocessor.process(raw_response, strict=False)
            return parsed
        except Exception as e:
            print(f"VLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return {}


class EgocentricLocalizationTest:
    """Egocentric ì¢Œí‘œê³„ ë¬¼ì²´ ìœ„ì¹˜ ì¶”ë¡  í…ŒìŠ¤íŠ¸ ì‹œìŠ¤í…œ"""
    
    def __init__(self, iteration: int = 0, prompt_variant: int = 0):
        self.iteration = iteration
        self.prompt_variant = prompt_variant
        self.vlm = ChatGPT4oVLMWrapper(
            model=VLM_MODEL,
            temperature=VLM_TEMPERATURE,
            max_tokens=VLM_MAX_TOKENS
        )
        self.postprocessor = VLMResponsePostProcessor(required_fields=["objects"])
        
        self.solution = EgocentricLocalizationSolution(self.vlm, self.postprocessor, prompt_variant)
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬
        self.log_dir = Path("logs/egocentric_localization")
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # í˜„ì¬ ë°˜ë³µ ë¡œê·¸ ë””ë ‰í† ë¦¬
        self.iteration_dir = self.log_dir / f"iteration_{iteration:03d}"
        self.iteration_dir.mkdir(parents=True, exist_ok=True)
    
    def _test_single_environment(self, env_idx: int, num_environments: int, seed: Optional[int] = None) -> Dict:
        """ë‹¨ì¼ í™˜ê²½ í…ŒìŠ¤íŠ¸"""
        try:
            wrapper, env_info = create_random_environment_with_objects(seed=seed)
            
            # ì´ë¯¸ì§€ ì €ì¥
            image = wrapper.get_image()
            image_path = self.iteration_dir / f"env_{env_idx:02d}_image.png"
            import cv2
            cv2.imwrite(str(image_path), cv2.cvtColor(image, cv2.COLOR_RGB2BGR))
            
            # VLM í…ŒìŠ¤íŠ¸
            vlm_response = self.solution.test(image, wrapper, env_info['objects'])
            
            # GT ê³„ì‚°
            gt_results = []
            for obj in env_info['objects']:
                obj_center = obj['center']
                agent_pos = env_info['agent_pos']
                agent_dir = int(env_info['agent_dir'])
                gt_pos = calculate_gt_egocentric_position(
                    agent_pos,
                    agent_dir,
                    obj_center
                )
                gt_results.append({
                    'color': obj['color'],
                    'gt_position': gt_pos
                })
            
            # ê²°ê³¼ ë¹„êµ
            vlm_objects = vlm_response.get('objects', [])
            correct_count = 0
            total_count = len(gt_results)
            
            results = []
            for gt_obj in gt_results:
                # VLM ì‘ë‹µì—ì„œ í•´ë‹¹ ìƒ‰ìƒ ì°¾ê¸°
                vlm_obj = None
                for vo in vlm_objects:
                    if vo.get('color', '').lower() == gt_obj['color'].lower():
                        vlm_obj = vo
                        break
                
                if vlm_obj:
                    vlm_pos = vlm_obj.get('egocentric_position', '').lower()
                    gt_pos = gt_obj['gt_position'].lower()
                    is_correct = vlm_pos == gt_pos
                    if is_correct:
                        correct_count += 1
                    
                    results.append({
                        'color': gt_obj['color'],
                        'gt_position': gt_pos,
                        'vlm_position': vlm_pos,
                        'correct': is_correct
                    })
                else:
                    results.append({
                        'color': gt_obj['color'],
                        'gt_position': gt_obj['gt_position'],
                        'vlm_position': 'not_found',
                        'correct': False
                    })
            
            wrapper.close()
            
            return {
                'env_idx': env_idx,
                'env_info': env_info,
                'vlm_response': vlm_response,
                'gt_results': gt_results,
                'comparison': results,
                'correct': correct_count,
                'total': total_count,
                'success_rate': correct_count / total_count if total_count > 0 else 0.0
            }
        except Exception as e:
            print(f"í™˜ê²½ {env_idx} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {
                'env_idx': env_idx,
                'error': str(e),
                'correct': 0,
                'total': 0,
                'success_rate': 0.0
            }
    
    def run_phase1(self, num_environments: int = 10) -> Dict:
        """Phase 1: í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"\n{'='*80}")
        print(f"Phase 1: í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ë°˜ë³µ {self.iteration}, ë³‘ë ¬ ì²˜ë¦¬: {os.cpu_count() * 2} workers)")
        print(f"{'='*80}")
        
        results = []
        
        # ë³‘ë ¬ ì²˜ë¦¬
        tasks = []
        with ThreadPoolExecutor(max_workers=min(os.cpu_count() * 2, num_environments)) as executor:
            for env_idx in range(num_environments):
                seed = self.iteration * 1000 + env_idx
                tasks.append(executor.submit(self._test_single_environment, env_idx, num_environments, seed))
            
            for future in as_completed(tasks):
                env_result = future.result()
                results.append(env_result)
                
                if 'error' not in env_result:
                    print(f"[í™˜ê²½ {env_result['env_idx']+1}/{num_environments}] "
                          f"ì„±ê³µë¥ : {env_result['success_rate']:.1%} "
                          f"({env_result['correct']}/{env_result['total']})")
        
        # ì „ì²´ í†µê³„
        total_correct = sum(r.get('correct', 0) for r in results)
        total_objects = sum(r.get('total', 0) for r in results)
        overall_success_rate = total_correct / total_objects if total_objects > 0 else 0.0
        
        print(f"\n{'='*80}")
        print(f"Phase 1 ê²°ê³¼ (ë°˜ë³µ {self.iteration})")
        print(f"{'='*80}")
        print(f"ì „ì²´ ì„±ê³µë¥ : {overall_success_rate:.1%} ({total_correct}/{total_objects})")
        
        # ê²°ê³¼ ì €ì¥
        phase1_results = {
            'iteration': self.iteration,
            'timestamp': datetime.now().isoformat(),
            'overall_success_rate': overall_success_rate,
            'total_correct': total_correct,
            'total_objects': total_objects,
            'environments': results
        }
        
        results_path = self.iteration_dir / "phase1_results.json"
        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(phase1_results, f, indent=2, ensure_ascii=False)
        
        return phase1_results
    
    def run_phase2(self, phase1_results: Dict) -> Dict:
        """Phase 2: ë¬¸ì œ ë¶„ì„ ë° ê°œì„ """
        print(f"\n{'='*80}")
        print(f"Phase 2: ë¬¸ì œ ë¶„ì„ ë° ê°œì„  (ë°˜ë³µ {self.iteration})")
        print(f"{'='*80}")
        
        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„
        failed_cases = []
        failure_patterns = {
            'wrong_direction': 0,
            'object_not_found': 0,
            'coordinate_confusion': 0,
            'transformation_error': 0
        }
        
        for env_result in phase1_results['environments']:
            if 'comparison' in env_result:
                for comp in env_result['comparison']:
                    if not comp.get('correct', False):
                        failed_cases.append({
                            'env_idx': env_result['env_idx'],
                            'color': comp.get('color'),
                            'gt': comp.get('gt_position'),
                            'vlm': comp.get('vlm_position'),
                            'error_type': self._classify_error(comp)
                        })
                        
                        error_type = self._classify_error(comp)
                        if error_type in failure_patterns:
                            failure_patterns[error_type] += 1
        
        print(f"\nì‹¤íŒ¨ ì¼€ì´ìŠ¤: {len(failed_cases)}/{phase1_results['total_objects']}")
        print(f"ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„:")
        for pattern, count in failure_patterns.items():
            print(f"  - {pattern}: {count}")
        
        # ê°œì„  ë°©ì•ˆ ë„ì¶œ
        improvements = self._analyze_and_improve(failed_cases, failure_patterns)
        
        analysis = {
            'iteration': self.iteration,
            'timestamp': datetime.now().isoformat(),
            'total_failures': len(failed_cases),
            'failure_patterns': failure_patterns,
            'failed_cases': failed_cases[:10],  # ì²˜ìŒ 10ê°œë§Œ ì €ì¥
            'improvements': improvements
        }
        
        # ë¶„ì„ ì €ì¥
        analysis_path = self.iteration_dir / "phase2_analysis.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, indent=2, ensure_ascii=False)
        
        # ê°œì„  ë¬¸ì„œ ìƒì„±
        self._create_improvement_document(analysis)
        
        return analysis
    
    def _classify_error(self, comp: Dict) -> str:
        """ì—ëŸ¬ íƒ€ì… ë¶„ë¥˜"""
        vlm_pos = comp.get('vlm_position', '').lower()
        gt_pos = comp.get('gt_position', '').lower()
        
        if vlm_pos == 'not_found':
            return 'object_not_found'
        elif vlm_pos not in ['front', 'back', 'left', 'right']:
            return 'coordinate_confusion'
        elif vlm_pos in ['front', 'back', 'left', 'right']:
            return 'wrong_direction'
        else:
            return 'transformation_error'
    
    def _analyze_and_improve(self, failed_cases: List[Dict], failure_patterns: Dict) -> List[Dict]:
        """ë¬¸ì œ ë¶„ì„ ë° ê°œì„  ë°©ì•ˆ ë„ì¶œ"""
        improvements = []
        
        if failure_patterns['object_not_found'] > 0:
            improvements.append({
                'type': 'prompt_enhancement',
                'description': 'ë¬¼ì²´ë¥¼ ì°¾ì§€ ëª»í•˜ëŠ” ê²½ìš°ê°€ ë§ìŒ. ë¬¼ì²´ íƒì§€ ì§€ì‹œë¥¼ ë” ëª…í™•í•˜ê²Œ í•„ìš”',
                'action': 'enhance_object_detection',
                'priority': failure_patterns['object_not_found']
            })
        
        if failure_patterns['coordinate_confusion'] > 0:
            improvements.append({
                'type': 'prompt_enhancement',
                'description': 'ì¢Œí‘œê³„ í˜¼ë™ ë°œìƒ. Allocentric vs Egocentric êµ¬ë¶„ì„ ë” ëª…í™•íˆ í•„ìš”',
                'action': 'clarify_coordinate_systems',
                'priority': failure_patterns['coordinate_confusion']
            })
        
        if failure_patterns['wrong_direction'] > 0:
            improvements.append({
                'type': 'prompt_enhancement',
                'description': 'ë°©í–¥ ë³€í™˜ ì˜¤ë¥˜. Lookup Table ì‚¬ìš©ì„ ë” ê°•ì œ í•„ìš”',
                'action': 'enhance_transformation',
                'priority': failure_patterns['wrong_direction']
            })
        
        if failure_patterns['transformation_error'] > 0:
            improvements.append({
                'type': 'prompt_enhancement',
                'description': 'ë³€í™˜ í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜. ë‹¨ê³„ë³„ ì¶”ë¡ ì„ ë” ê°•ì œ í•„ìš”',
                'action': 'enhance_step_by_step',
                'priority': failure_patterns['transformation_error']
            })
        
        improvements.sort(key=lambda x: x.get('priority', 0), reverse=True)
        return improvements
    
    def _create_improvement_document(self, analysis: Dict):
        """ê°œì„  ë¬¸ì„œ ìƒì„±"""
        doc_path = self.iteration_dir / "improvement_analysis.md"
        
        with open(doc_path, 'w', encoding='utf-8') as f:
            f.write(f"# ê°œì„  ë¶„ì„ ë¦¬í¬íŠ¸ (ë°˜ë³µ {self.iteration})\n\n")
            f.write(f"**ìƒì„± ì‹œê°„**: {analysis['timestamp']}\n\n")
            
            f.write("## ì‹¤íŒ¨ í†µê³„\n\n")
            f.write(f"- ì´ ì‹¤íŒ¨: {analysis['total_failures']}\n\n")
            
            f.write("## ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„\n\n")
            for pattern, count in analysis['failure_patterns'].items():
                f.write(f"- **{pattern}**: {count}íšŒ\n")
            f.write("\n")
            
            f.write("## ê°œì„  ë°©ì•ˆ\n\n")
            for idx, improvement in enumerate(analysis['improvements'], 1):
                f.write(f"### ê°œì„  ë°©ì•ˆ {idx}\n\n")
                f.write(f"- **íƒ€ì…**: {improvement['type']}\n")
                f.write(f"- **ì„¤ëª…**: {improvement['description']}\n")
                f.write(f"- **ì•¡ì…˜**: {improvement['action']}\n\n")
    
    def save_summary(self, phase1_results: Dict, phase2_analysis: Dict):
        """ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥"""
        summary = {
            'iteration': self.iteration,
            'timestamp': datetime.now().isoformat(),
            'phase1': {
                'success_rate': phase1_results['overall_success_rate'],
                'total_correct': phase1_results['total_correct'],
                'total_objects': phase1_results['total_objects']
            },
            'phase2': phase2_analysis
        }
        
        summary_path = self.iteration_dir / "summary.json"
        with open(summary_path, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # ì „ì²´ ìš”ì•½ì—ë„ ì¶”ê°€
        master_summary_path = self.log_dir / "master_summary.json"
        if master_summary_path.exists():
            with open(master_summary_path, 'r', encoding='utf-8') as f:
                master_summary = json.load(f)
        else:
            master_summary = {'iterations': []}
        
        master_summary['iterations'].append(summary)
        with open(master_summary_path, 'w', encoding='utf-8') as f:
            json.dump(master_summary, f, indent=2, ensure_ascii=False)


def main():
    """ë©”ì¸ í•¨ìˆ˜: ì„±ê³µë¥  90% ì´ìƒ ë‹¬ì„±ê¹Œì§€ ìë™ ì‹¤í–‰ ë° ê°œì„ """
    target_success_rate = 0.90
    max_iterations = 50
    iteration = 0
    prompt_variant = 0
    
    print("=" * 80)
    print("Egocentric Object Localization ì™„ì „ ìë™ ê°œì„  ì‹œìŠ¤í…œ")
    print("=" * 80)
    print(f"ëª©í‘œ ì„±ê³µë¥ : {target_success_rate:.1%}")
    print(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {max_iterations}")
    print("ìë™ ì‹¤í–‰ ëª¨ë“œ: 90% ë‹¬ì„±ê¹Œì§€ ìë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë° ê°œì„ ")
    print("=" * 80)
    
    best_success_rate = 0.0
    best_iteration = 0
    best_prompt_variant = 0
    no_improvement_count = 0
    max_no_improvement = 3
    
    while iteration < max_iterations:
        print(f"\n{'#'*80}")
        print(f"# ë°˜ë³µ {iteration} ì‹œì‘")
        print(f"{'#'*80}")
        
        test_system = EgocentricLocalizationTest(iteration=iteration, prompt_variant=prompt_variant)
        
        # Phase 1: í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        phase1_results = test_system.run_phase1(num_environments=10)
        
        # ì„±ê³µë¥  í™•ì¸
        current_success_rate = phase1_results['overall_success_rate']
        
        print(f"\n[ë°˜ë³µ {iteration} ê²°ê³¼]")
        print(f"  ì„±ê³µë¥ : {current_success_rate:.1%} (í”„ë¡¬í”„íŠ¸ ë³€í˜•: {prompt_variant})")
        print(f"  ì •ë‹µ: {phase1_results['total_correct']}/{phase1_results['total_objects']}")
        
        # ìµœê³  ì„±ê³µë¥  ì—…ë°ì´íŠ¸
        if current_success_rate > best_success_rate:
            best_success_rate = current_success_rate
            best_iteration = iteration
            best_prompt_variant = prompt_variant
            no_improvement_count = 0
            print(f"  âœ“ ìƒˆë¡œìš´ ìµœê³  ì„±ê³µë¥  ë‹¬ì„±!")
        else:
            no_improvement_count += 1
            print(f"  âš  ê°œì„  ì—†ìŒ (ì—°ì† {no_improvement_count}íšŒ)")
        
        # Phase 2: ë¬¸ì œ ë¶„ì„
        phase2_analysis = test_system.run_phase2(phase1_results)
        
        # ìš”ì•½ ì €ì¥
        test_system.save_summary(phase1_results, phase2_analysis)
        
        # ëª©í‘œ ë‹¬ì„± í™•ì¸
        if current_success_rate >= target_success_rate:
            print(f"\n{'='*80}")
            print(f"ğŸ‰ ëª©í‘œ ì„±ê³µë¥  ë‹¬ì„±! ({current_success_rate:.1%} >= {target_success_rate:.1%})")
            print(f"{'='*80}")
            print(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ë³€í˜•: {prompt_variant}")
            break
        
        # ê°œì„  ì‚¬í•­ ìë™ ì ìš©
        print(f"\n[ìë™ ê°œì„  ë¶„ì„]")
        improvements = phase2_analysis.get('improvements', [])
        
        if improvements:
            print(f"  ë°œê²¬ëœ ê°œì„  ì‚¬í•­: {len(improvements)}ê°œ")
            for imp in improvements:
                print(f"    - {imp['action']} (ìš°ì„ ìˆœìœ„: {imp['priority']})")
            
            # ê°œì„  ì‚¬í•­ ì ìš©
            for improvement in improvements:
                action = improvement['action']
                
                if action == 'enhance_object_detection':
                    prompt_variant = max(prompt_variant, 1)
                elif action == 'clarify_coordinate_systems':
                    prompt_variant = max(prompt_variant, 1)
                elif action == 'enhance_transformation':
                    prompt_variant = max(prompt_variant, 2)
                elif action == 'enhance_step_by_step':
                    prompt_variant = max(prompt_variant, 2)
            
            print(f"  â†’ í”„ë¡¬í”„íŠ¸ ë³€í˜• ì—…ë°ì´íŠ¸: {prompt_variant}")
        else:
            print(f"  ê°œì„  ì‚¬í•­ì´ ì—†ìŒ. ì ê·¹ì  ê°œì„  ëª¨ë“œ í™œì„±í™”...")
            if no_improvement_count >= max_no_improvement:
                prompt_variant = min(prompt_variant + 1, 2)
                print(f"  â†’ í”„ë¡¬í”„íŠ¸ ë³€í˜• ì¦ê°€: {prompt_variant}")
        
        # ì„±ê³µë¥ ì´ ë§¤ìš° ë‚®ìœ¼ë©´ ê°•ì œ ê°œì„ 
        if current_success_rate < 0.5 and iteration > 2:
            print(f"  âš  ì„±ê³µë¥ ì´ ë§¤ìš° ë‚®ìŒ. ê°•ì œ ê°œì„  ëª¨ë“œ...")
            prompt_variant = max(prompt_variant, 2)
            print(f"  â†’ ê°•ì œ í”„ë¡¬í”„íŠ¸ ë³€í˜•: {prompt_variant}")
        
        iteration += 1
        print(f"\n{'='*80}")
        print(f"ë‹¤ìŒ ë°˜ë³µìœ¼ë¡œ ì§„í–‰... (í˜„ì¬ ìµœê³ : {best_success_rate:.1%} @ ë°˜ë³µ {best_iteration})")
        print(f"{'='*80}")
    
    print(f"\n{'='*80}")
    print("ìë™ ê°œì„  ì™„ë£Œ")
    print(f"{'='*80}")
    print(f"ìµœì¢… ì„±ê³µë¥ : {best_success_rate:.1%} (ë°˜ë³µ {best_iteration})")
    print(f"ì´ ë°˜ë³µ íšŸìˆ˜: {iteration}")
    print(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ë³€í˜•: {best_prompt_variant}")
    
    if best_success_rate >= target_success_rate:
        print(f"\nâœ… ëª©í‘œ ë‹¬ì„± ì„±ê³µ!")
    else:
        print(f"\nâš  ëª©í‘œ ë¯¸ë‹¬ì„± (ëª©í‘œ: {target_success_rate:.1%}, ë‹¬ì„±: {best_success_rate:.1%})")


if __name__ == "__main__":
    main()

