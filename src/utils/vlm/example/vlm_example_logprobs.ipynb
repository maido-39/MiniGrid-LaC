{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VLM Logprobs Usage Examples\n",
        "\n",
        "This notebook demonstrates usage of Vertex AI logprobs feature with Gemini models.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Set environment variables:\n",
        "- `GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json`\n",
        "- `GOOGLE_CLOUD_PROJECT=your-project-id`\n",
        "- `GOOGLE_CLOUD_LOCATION=us-central1` (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src directory to path for imports\n",
        "script_dir = Path().resolve()\n",
        "src_dir = script_dir.parent.parent.parent.parent / \"src\"\n",
        "if str(src_dir) not in sys.path:\n",
        "    sys.path.insert(0, str(src_dir))\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Import VLM components\n",
        "from utils.vlm.vlm_wrapper import VLMWrapper\n",
        "from utils.vlm.vlm_postprocessor import VLMResponsePostProcessor\n",
        "\n",
        "# Vertex AI credentials setup\n",
        "credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n",
        "project_id = os.getenv(\"GOOGLE_CLOUD_PROJECT\")\n",
        "location = os.getenv(\"GOOGLE_CLOUD_LOCATION\", \"us-central1\")\n",
        "\n",
        "print(f\"Credentials path: {credentials_path}\")\n",
        "print(f\"Project ID: {project_id}\")\n",
        "print(f\"Location: {location}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 1: Basic logprobs usage with Vertex AI\n",
        "\n",
        "This example shows how to:\n",
        "- Initialize VLMWrapper with Vertex AI credentials\n",
        "- Generate responses with logprobs\n",
        "- Display top-k logprobs for each token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Basic logprobs usage with Vertex AI\n",
        "print(\"=\"*80)\n",
        "print(\"Example 1: Basic logprobs usage with Vertex AI\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not credentials_path or not project_id:\n",
        "    print(\"[SKIP] Vertex AI credentials not configured.\")\n",
        "    print(\"Set environment variables:\")\n",
        "    print(\"  - GOOGLE_APPLICATION_CREDENTIALS=/path/to/key.json\")\n",
        "    print(\"  - GOOGLE_CLOUD_PROJECT=your-project-id\")\n",
        "    print(\"  - GOOGLE_CLOUD_LOCATION=us-central1 (optional)\")\n",
        "else:\n",
        "    try:\n",
        "        # Initialize wrapper with Vertex AI\n",
        "        wrapper = VLMWrapper(\n",
        "            model=\"gemini-2.5-flash-vertex\",  # or \"gemini-2.5-flash-logprobs\"\n",
        "            logprobs=5,  # Get top-5 logprobs for each token\n",
        "            credentials=credentials_path,\n",
        "            project_id=project_id,\n",
        "            location=location,\n",
        "            temperature=0.0,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "        \n",
        "        # Generate response with logprobs\n",
        "        print(\"\\n[1] Generating response with logprobs...\")\n",
        "        response, logprobs_metadata = wrapper.generate_with_logprobs(\n",
        "            system_prompt=\"You are a helpful assistant.\",\n",
        "            user_prompt=\"What is the capital of France? Answer in one word.\",\n",
        "            debug=True\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n[2] Response: {response}\")\n",
        "        print(f\"\\n[3] Number of tokens: {len(logprobs_metadata.get('tokens', []))}\")\n",
        "        \n",
        "        if 'tokens' in logprobs_metadata:\n",
        "            print(f\"\\n[4] Tokens: {logprobs_metadata['tokens'][:20]}...\")  # First 20 tokens\n",
        "            if 'entropies' in logprobs_metadata:\n",
        "                avg_entropy = np.mean(logprobs_metadata['entropies'])\n",
        "                print(f\"[5] Average entropy: {avg_entropy:.4f} bits\")\n",
        "            \n",
        "            # Display top-k logprobs for each token\n",
        "            if 'top_logprobs' in logprobs_metadata and logprobs_metadata['top_logprobs']:\n",
        "                # Get k value from first non-empty top_logprobs\n",
        "                k_value = 0\n",
        "                for top_k in logprobs_metadata['top_logprobs']:\n",
        "                    if top_k:\n",
        "                        k_value = len(top_k)\n",
        "                        break\n",
        "                \n",
        "                print(f\"\\n[6] Top-{k_value} Logprobs for each token:\")\n",
        "                for i, (token, top_k) in enumerate(zip(\n",
        "                    logprobs_metadata['tokens'],\n",
        "                    logprobs_metadata['top_logprobs']\n",
        "                )):\n",
        "                    if top_k:\n",
        "                        print(f\"\\n  Token {i} ('{token}'):\")\n",
        "                        # Calculate probabilities from logprobs\n",
        "                        for j, candidate in enumerate(top_k):\n",
        "                            logprob = candidate.get('log_probability', 0)\n",
        "                            prob = np.exp(logprob)\n",
        "                            cand_token = candidate.get('token', '')\n",
        "                            print(f\"    {j+1}. '{cand_token}': prob={prob:.6f} (logprob={logprob:.4f})\")\n",
        "                    else:\n",
        "                        print(f\"\\n  Token {i} ('{token}'): No top candidates available\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 2: Extract logprobs for action field in JSON response\n",
        "\n",
        "This example shows how to:\n",
        "- Extract logprobs for specific fields (e.g., \"action\") in JSON responses\n",
        "- Use postprocessor to wrap logprobs around action tokens\n",
        "- Compare clean JSON vs JSON with logprobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Extract logprobs for action field in JSON response\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Example 2: Extract logprobs for action field in JSON response\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not credentials_path or not project_id:\n",
        "    print(\"[SKIP] Vertex AI credentials not configured.\")\n",
        "else:\n",
        "    try:\n",
        "        # Initialize wrapper\n",
        "        wrapper = VLMWrapper(\n",
        "            model=\"gemini-2.5-flash-vertex\",\n",
        "            logprobs=5,\n",
        "            credentials=credentials_path,\n",
        "            project_id=project_id,\n",
        "            location=location,\n",
        "            temperature=0.0,\n",
        "            max_tokens=2000\n",
        "        )\n",
        "        \n",
        "        # System prompt for robot control\n",
        "        system_prompt = \"\"\"You are a robot controller. \n",
        "Respond with JSON format containing:\n",
        "- action: The action to take (e.g., \"move up\", \"pickup\", \"drop\")\n",
        "- reasoning: Brief explanation of why this action was chosen\n",
        "\"\"\"\n",
        "        \n",
        "        user_prompt = \"\"\"Based on the current situation, what action should the robot take?\n",
        "Respond in JSON format:\n",
        "{\n",
        "  \"action\": \"move up\",\n",
        "  \"reasoning\": \"The goal is to the north\"\n",
        "}\n",
        "\"\"\"\n",
        "        \n",
        "        print(\"\\n[1] Generating response with logprobs...\")\n",
        "        response, logprobs_metadata = wrapper.generate_with_logprobs(\n",
        "            system_prompt=system_prompt,\n",
        "            user_prompt=user_prompt,\n",
        "            debug=False\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n[2] Response:\\n{response}\")\n",
        "        \n",
        "        # Process with postprocessor\n",
        "        print(\"\\n[3] Processing with postprocessor...\")\n",
        "        processor = VLMResponsePostProcessor(\n",
        "            required_fields=[\"action\", \"reasoning\"]\n",
        "        )\n",
        "        \n",
        "        # Option A: Get clean JSON without logprobs\n",
        "        print(\"\\n[4] Option A: Clean JSON (without logprobs)\")\n",
        "        parsed_clean = processor.process_without_logprobs(\n",
        "            response,\n",
        "            logprobs_metadata\n",
        "        )\n",
        "        print(f\"  Action: {parsed_clean.get('action')}\")\n",
        "        print(f\"  Reasoning: {parsed_clean.get('reasoning')}\")\n",
        "        \n",
        "        # Option B: Get JSON with action logprobs wrapped\n",
        "        print(\"\\n[5] Option B: JSON with action logprobs wrapped\")\n",
        "        parsed_with_logprobs = processor.process_with_action_logprobs(\n",
        "            response,\n",
        "            logprobs_metadata,\n",
        "            action_field=\"action\"\n",
        "        )\n",
        "        print(f\"  Action: {parsed_with_logprobs.get('action')}\")\n",
        "        print(f\"  Reasoning: {parsed_with_logprobs.get('reasoning')}\")\n",
        "        \n",
        "        # Display action logprobs\n",
        "        if 'action_logprobs' in parsed_with_logprobs:\n",
        "            action_logprobs = parsed_with_logprobs['action_logprobs']\n",
        "            print(f\"\\n[6] Action Logprobs:\")\n",
        "            print(f\"  Action tokens: {action_logprobs.get('action_tokens', [])}\")\n",
        "            print(f\"  Number of action tokens: {len(action_logprobs.get('action_tokens', []))}\")\n",
        "            if action_logprobs.get('action_entropies'):\n",
        "                avg_entropy = np.mean(action_logprobs['action_entropies'])\n",
        "                print(f\"  Average entropy for action: {avg_entropy:.4f} bits\")\n",
        "        \n",
        "        # Display remaining logprobs\n",
        "        if 'remaining_logprobs' in parsed_with_logprobs:\n",
        "            remaining = parsed_with_logprobs['remaining_logprobs']\n",
        "            print(f\"\\n[7] Remaining Logprobs:\")\n",
        "            print(f\"  Number of remaining tokens: {len(remaining.get('tokens', []))}\")\n",
        "            if remaining.get('entropies'):\n",
        "                avg_entropy = np.mean(remaining['entropies'])\n",
        "                print(f\"  Average entropy for remaining: {avg_entropy:.4f} bits\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 3: Analyze token entropy for uncertainty estimation\n",
        "\n",
        "This example shows how to:\n",
        "- Analyze token-wise entropy to estimate model uncertainty\n",
        "- Identify high-uncertainty tokens\n",
        "- View top-k logprobs for specific tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Analyze token entropy for uncertainty estimation\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Example 3: Analyze token entropy for uncertainty estimation\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not credentials_path or not project_id:\n",
        "    print(\"[SKIP] Vertex AI credentials not configured.\")\n",
        "else:\n",
        "    try:\n",
        "        wrapper = VLMWrapper(\n",
        "            model=\"gemini-2.5-flash-vertex\",\n",
        "            logprobs=5,\n",
        "            credentials=credentials_path,\n",
        "            project_id=project_id,\n",
        "            location=location,\n",
        "            temperature=0.0,\n",
        "            max_tokens=1000\n",
        "        )\n",
        "        \n",
        "        # Generate response\n",
        "        response, logprobs_metadata = wrapper.generate_with_logprobs(\n",
        "            system_prompt=\"You are a helpful assistant.\",\n",
        "            user_prompt=\"What is 2+2? Answer with just the number.\",\n",
        "            debug=False\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n[1] Response: {response}\")\n",
        "        \n",
        "        # Analyze entropy\n",
        "        if 'tokens' in logprobs_metadata and 'entropies' in logprobs_metadata:\n",
        "            tokens = logprobs_metadata['tokens']\n",
        "            entropies = logprobs_metadata['entropies']\n",
        "            \n",
        "            print(f\"\\n[2] Token-wise Entropy Analysis:\")\n",
        "            print(f\"  Total tokens: {len(tokens)}\")\n",
        "            print(f\"  Average entropy: {np.mean(entropies):.4f} bits\")\n",
        "            print(f\"  Max entropy: {np.max(entropies):.4f} bits (token: '{tokens[np.argmax(entropies)]}')\")\n",
        "            print(f\"  Min entropy: {np.min(entropies):.4f} bits (token: '{tokens[np.argmin(entropies)]}')\")\n",
        "            \n",
        "            # High uncertainty tokens (entropy > threshold)\n",
        "            threshold = np.mean(entropies) + np.std(entropies)\n",
        "            high_uncertainty = [(t, e) for t, e in zip(tokens, entropies) if e > threshold]\n",
        "            if high_uncertainty:\n",
        "                print(f\"\\n[3] High Uncertainty Tokens (entropy > {threshold:.4f}):\")\n",
        "                for token, entropy in high_uncertainty[:10]:  # Show first 10\n",
        "                    print(f\"  '{token}': {entropy:.4f} bits\")\n",
        "        \n",
        "        # Show top logprobs for first few tokens\n",
        "        if 'top_logprobs' in logprobs_metadata and logprobs_metadata['top_logprobs']:\n",
        "            print(f\"\\n[4] Top-5 Logprobs for First 3 Tokens:\")\n",
        "            for i, top_k in enumerate(logprobs_metadata['top_logprobs'][:3]):\n",
        "                if top_k:\n",
        "                    print(f\"  Token position {i}:\")\n",
        "                    for j, candidate in enumerate(top_k[:5]):\n",
        "                        prob = np.exp(candidate.get('log_probability', 0))\n",
        "                        print(f\"    {j+1}. '{candidate.get('token', '')}': {prob:.4f} (logprob: {candidate.get('log_probability', 0):.4f})\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example 4: Using VLMProcessor with logprobs\n",
        "\n",
        "This example shows how to:\n",
        "- Use VLMProcessor with logprobs support\n",
        "- Request and parse responses with logprobs\n",
        "- Extract action-specific logprobs using VLMProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 4: Using VLMProcessor with logprobs\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Example 4: Using VLMProcessor with logprobs\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not credentials_path or not project_id:\n",
        "    print(\"[SKIP] Vertex AI credentials not configured.\")\n",
        "else:\n",
        "    try:\n",
        "        from utils.vlm.vlm_processor import VLMProcessor\n",
        "        \n",
        "        # Initialize processor with logprobs\n",
        "        processor = VLMProcessor(\n",
        "            model=\"gemini-2.5-flash-vertex\",\n",
        "            logprobs=5,\n",
        "            credentials=credentials_path,\n",
        "            project_id=project_id,\n",
        "            location=location,\n",
        "            temperature=0.0,\n",
        "            max_tokens=2000,\n",
        "            debug=False\n",
        "        )\n",
        "        \n",
        "        # Create dummy image (or use real image)\n",
        "        dummy_image = np.random.randint(0, 255, (100, 100, 3), dtype=np.uint8)\n",
        "        \n",
        "        system_prompt = \"\"\"You are a robot controller. \n",
        "Respond with JSON format containing:\n",
        "- action: The action to take (e.g., [\"0\"] for move up, [\"1\"] for move down)\n",
        "- reasoning: Brief explanation\n",
        "- grounding: Grounding information\n",
        "- memory: Memory structure\n",
        "\"\"\"\n",
        "        \n",
        "        user_prompt = \"\"\"What action should the robot take? Respond in JSON format.\"\"\"\n",
        "        \n",
        "        print(\"\\n[1] Requesting with logprobs...\")\n",
        "        response, logprobs_metadata = processor.requester_with_logprobs(\n",
        "            image=dummy_image,\n",
        "            system_prompt=system_prompt,\n",
        "            user_prompt=user_prompt,\n",
        "            debug=False\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n[2] Response:\\n{response}\")\n",
        "        \n",
        "        print(\"\\n[3] Parsing with action logprobs...\")\n",
        "        parsed = processor.parser_action_with_logprobs(\n",
        "            response,\n",
        "            logprobs_metadata,\n",
        "            action_field=\"action\",\n",
        "            remove_logprobs=False\n",
        "        )\n",
        "        \n",
        "        print(f\"\\n[4] Parsed result:\")\n",
        "        print(f\"  Action: {parsed.get('action')}\")\n",
        "        print(f\"  Reasoning: {parsed.get('reasoning')}\")\n",
        "        \n",
        "        if 'action_logprobs' in parsed:\n",
        "            action_logprobs = parsed['action_logprobs']\n",
        "            print(f\"\\n[5] Action Logprobs:\")\n",
        "            print(f\"  Action tokens: {action_logprobs.get('action_tokens', [])}\")\n",
        "            if action_logprobs.get('action_entropies'):\n",
        "                avg_entropy = np.mean(action_logprobs['action_entropies'])\n",
        "                print(f\"  Average entropy: {avg_entropy:.4f} bits\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
