######################################################
#                                                    #
#                         VLM                        #
#                      PROCESSOR                     #
#                                                    #
######################################################


""""""




######################################################
#                                                    #
#                      LIBRARIES                     #
#                                                    #
######################################################


import numpy as np
from typing import Optional, Union
from pathlib import Path

from utils.prompt_manager.prompt_interp import *
import utils.prompt_manager.terminal_formatting_utils as tfu
from utils.vlm.vlm_wrapper import VLMWrapper
from utils.vlm.vlm_postprocessor import VLMResponsePostProcessor

from utils.miscellaneous.global_variables import VLM_MAX_TOKENS, VLM_MODEL, VLM_TEMPERATURE, VLM_THINKING_BUDGET




######################################################
#                                                    #
#                       CLASS                        #
#                                                    #
######################################################


class VLMProcessor:
    """VLM Request and Parsing Processing Class"""
    
    def __init__(self,
                 model: str = VLM_MODEL,
                 temperature: float = VLM_TEMPERATURE,
                 max_tokens: int = VLM_MAX_TOKENS,
                 debug: bool = False,
                 vertexai: bool = False,
                 credentials: Optional[Union[str, object]] = None,
                 project_id: Optional[str] = None,
                 location: Optional[str] = None,
                 logprobs: Optional[int] = None,
                 thinking_budget: Optional[int] = None
                ):
        # thinking_budget이 명시적으로 제공되지 않으면 global_variables에서 가져오기
        if thinking_budget is None:
            thinking_budget = VLM_THINKING_BUDGET
        
        self.vlm = VLMWrapper(model=model,
                                       temperature=temperature,
                                       max_tokens=max_tokens,
                                       thinking_budget=thinking_budget,
                                       vertexai=vertexai,
                                       credentials=credentials,
                                       project_id=project_id,
                                       location=location,
                                       logprobs=logprobs
                                      )
        self.debug = debug
        self.logprobs = logprobs
        # grounding, memory are optional (may be missing in truncated responses)
        self.postprocessor_action = VLMResponsePostProcessor(required_fields=["action", "reasoning"])
        self.postprocessor_feedback = VLMResponsePostProcessor(required_fields=["knowledge"])
    
    def requester(self,
                  image: np.ndarray,
                  system_prompt: str,
                  user_prompt: str,
                  grounding_file: Optional[Union[str, Path]] = None,
                  debug: bool = None
                 ) -> str:
        """Send Request to VLM (Default Method)
        
        Args:
            image: Input image array
            system_prompt: System prompt
            user_prompt: User prompt
            debug: Enable debug output. If None, uses instance's debug setting.
        
        Returns:
            Raw response string from VLM
        """
        
        try:
            # Use method-level debug if provided, otherwise use instance-level debug
            debug_flag = debug if debug is not None else self.debug
            response = self.vlm.generate(image=image,
                                         system_prompt=system_prompt,
                                         user_prompt=user_prompt,
                                         grounding_file=grounding_file,
                                         debug=debug_flag
                                        )
            return response
        except Exception as e:
            tfu.cprint(f"VLM API call failed: {e}", tfu.RED, True)
            return ""
    
    def parser_action(self, raw_response: str) -> dict:
        """Parsing the Response Generated by the Action"""
        
        try:
            parsed = self.postprocessor_action.process(raw_response, strict=True)
            return parsed
        except ValueError as e:
            tfu.cprint(f"Response parsing failed: {e}", tfu.RED, True)
            return {
                "action": ["0"],  # Default value: move up
                "reasoning": "Parsing failed",
                "grounding": "",
                "memory": {
                    "spatial_description": "",
                    "task_process": {"goal": "", "status": ""},
                    "previous_action": ""
                }
            }
    
    def parser_feedback(self, raw_response: str) -> dict:
        """Parse the response generated for feedback"""
        
        try:
            parsed = self.postprocessor_feedback.process(raw_response, strict=True)
            return parsed
        except ValueError as e:
            tfu.cprint(f"Feedback response parsing failed: {e}", tfu.RED, True)
            return {"knowledge": ""}

    def requester_with_logprobs(self,
                                image: np.ndarray,
                                system_prompt: str,
                                user_prompt: str,
                                grounding_file: Optional[Union[str, Path]] = None,
                                debug: bool = None
                               ) -> tuple:
        """Send Request to VLM with logprobs (Vertex AI only)
        
        Args:
            image: Input image array
            system_prompt: System prompt
            user_prompt: User prompt
            debug: Enable debug output. If None, uses instance's debug setting.
        
        Returns:
            Tuple of (response_text: str, logprobs_metadata: dict)
        """
        
        try:
            if not self.logprobs:
                raise ValueError(
                    "logprobs not enabled. Set logprobs parameter in __init__ "
                    "and use Vertex AI (vertexai=True or model='gemini-2.5-flash-vertex')."
                )
            
            # Use method-level debug if provided, otherwise use instance-level debug
            debug_flag = debug if debug is not None else self.debug
            response, logprobs_metadata = self.vlm.generate_with_logprobs(
                image=image,
                system_prompt=system_prompt,
                user_prompt=user_prompt,
                grounding_file=grounding_file,
                debug=debug_flag
            )
            return response, logprobs_metadata
        except Exception as e:
            tfu.cprint(f"VLM API call with logprobs failed: {e}", tfu.RED, True)
            return "", {}
    
    def parser_action_with_logprobs(self,
                                     raw_response: str,
                                     logprobs_metadata: dict,
                                     action_field: str = "action",
                                     remove_logprobs: bool = False
                                    ) -> dict:
        """Parse the response with logprobs information for action field
        
        Args:
            raw_response: Raw response text from VLM
            logprobs_metadata: Logprobs metadata from requester_with_logprobs
            action_field: Name of action field in JSON (default: "action")
            remove_logprobs: If True, return clean JSON without logprobs (default: False)
        
        Returns:
            Parsed dictionary. If remove_logprobs=False, includes 'action_logprobs_info' with:
                - 'actions': List of action values
                - 'action_logprobs': List of logprobs info for each action
                - 'action_entropies': List of entropies for each action
        """
        
        try:
            # 기본 JSON 파싱 (strict=False로 설정하여 필수 필드가 없어도 파싱 가능)
            parsed = self.postprocessor_action.process(raw_response, strict=False)
            
            # logprobs 정보 추가 (remove_logprobs=False인 경우)
            if not remove_logprobs and logprobs_metadata:
                action_logprobs_info = self.postprocessor_action.get_action_logprobs(
                    logprobs_metadata,
                    action_field=action_field
                )
                parsed['action_logprobs_info'] = action_logprobs_info
            
            return parsed
        except ValueError as e:
            tfu.cprint(f"Response parsing with logprobs failed: {e}", tfu.RED, True)
            return {
                "action": ["0"],  # Default value: move up
                "reasoning": "Parsing failed",
                "grounding": "",
                "memory": {
                    "spatial_description": "",
                    "task_process": {"goal": "", "status": ""},
                    "previous_action": ""
                }
            }



