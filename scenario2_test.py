"""
ì‹œë‚˜ë¦¬ì˜¤ 2 ì‹¤í—˜ í™˜ê²½ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ (VLM ì œì–´ ë²„ì „)

ì‹œë‚˜ë¦¬ì˜¤ 2: íŒŒë€ ê¸°ë‘¥ìœ¼ë¡œ ê°€ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ëŒê³ , í…Œì´ë¸” ì˜†ì— ë©ˆì¶”ì‹œì˜¤

í™˜ê²½ êµ¬ì„±:
- ë²½: ê²€ì€ìƒ‰ (ì™¸ë²½)
- íŒŒë€ ê¸°ë‘¥: íŒŒë€ìƒ‰ 2x2 Grid (í†µê³¼ë¶ˆê°€)
- í…Œì´ë¸”: ë³´ë¼ìƒ‰ 1x3 Grid (í†µê³¼ë¶ˆê°€)
- ì‹œì‘ì : ë¹¨ê°• 1x1
- ì¢…ë£Œì : ì´ˆë¡ 1x1

ë ˆì´ì•„ì›ƒ (10x10):
â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›
â¬›â¬œï¸â¬œï¸â¬œï¸â¬œï¸ğŸŸªğŸŸªğŸŸªğŸŸ©â¬›
â¬›â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬›
â¬›â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬›
â¬›â¬œï¸â¬œï¸ğŸŸ¦ğŸŸ¦â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬› 
â¬›â¬œï¸â¬œï¸ğŸŸ¦ğŸŸ¦â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬›
â¬›â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬›
â¬›â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬›
â¬›ğŸŸ¥â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬œï¸â¬›
â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›
"""

from minigrid import register_minigrid_envs
from custom_environment import CustomRoomWrapper
from vlm_wrapper import ChatGPT4oVLMWrapper
from vlm_postprocessor import VLMResponsePostProcessor
import json
from datetime import datetime
from pathlib import Path
import csv
from PIL import Image
import numpy as np
import cv2
import hashlib
import time

# MiniGrid í™˜ê²½ ë“±ë¡
register_minigrid_envs()


# ============================================================================
# VLM ì„¤ì •
# ============================================================================
# VLM ì„¤ì •ì„ ì—¬ê¸°ì„œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
# 
# OpenAI GPT-4 ê³„ì—´ ë¹„ì „ ì§€ì› ëª¨ë¸:
# - "gpt-4o": ìµœì‹  ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ (ê°€ì¥ ë¹ ë¥´ê³  ì •í™•, ê¶Œì¥)
# - "gpt-4o-mini": ê²½ëŸ‰ ë²„ì „ (ì €ë ´í•˜ê³  ë¹ ë¦„)
# - "gpt-4-turbo": ì´ì „ ë²„ì „ (ë¹„ì „ ì§€ì›)
# - "gpt-4-vision-preview": êµ¬ë²„ì „ (deprecated, ì‚¬ìš© ë¹„ê¶Œì¥)
#
# OpenAI GPT-5 ê³„ì—´ ë¹„ì „ ì§€ì› ëª¨ë¸ (2025ë…„ ì¶œì‹œ):
# - "gpt-5": ìµœì‹  ëª¨ë¸ (ë¹„ì „ ì§€ì›, ë©€í‹°ëª¨ë‹¬ ì¶”ë¡  ê°•í™”)
# - "gpt-5-mini": ê²½ëŸ‰ ë²„ì „ (API ì‚¬ìš© ê°€ëŠ¥ ì‹œ)
#
# ì°¸ê³ : 
# - "gpt-4o-nano"ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.
# - GPT-5ëŠ” 2025ë…„ 8ì›” ì¶œì‹œë˜ì—ˆìœ¼ë‚˜, API ëª¨ë¸ëª…ì€ OpenAI ê³µì‹ ë¬¸ì„œë¥¼ í™•ì¸í•˜ì„¸ìš”.
#   ì‹¤ì œ APIì—ì„œëŠ” "gpt-5" ë˜ëŠ” ë‹¤ë¥¸ ì´ë¦„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
VLM_MODEL = "gpt-4o"  # ì‚¬ìš©í•  ëª¨ë¸ëª… (GPT-5 ì‚¬ìš© ì‹œ "gpt-5"ë¡œ ë³€ê²½)
VLM_TEMPERATURE = 0.0  # ìƒì„± ì˜¨ë„ (0.0 ~ 2.0)
VLM_MAX_TOKENS = 1000  # ìµœëŒ€ í† í° ìˆ˜

# ì•¡ì…˜ ì˜ˆì¸¡ ê°œìˆ˜ ì„¤ì •
ACTION_PREDICTION_COUNT = 5  # VLMì´ ì˜ˆì¸¡í•  ì•¡ì…˜ ê°œìˆ˜ (ì²« ë²ˆì§¸ë§Œ ì‹¤í–‰, ë‚˜ë¨¸ì§€ëŠ” ë¡œê¹…ìš©)


# ============================================================================
# System Prompt ì •ì˜
# ============================================================================
# ì´ ë¶€ë¶„ì€ í™˜ê²½ ì„¤ëª…ê³¼ VLM ì‘ë‹µ í¬ë§·íŒ… ê°€ì´ë“œë¥¼ í¬í•¨í•©ë‹ˆë‹¤.
# í•„ìš”ì— ë”°ë¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

def get_system_prompt(action_count: int, memory_summary: str = "", grounding_section: str = "") -> str:
    """
    System Prompt ìƒì„± í•¨ìˆ˜ (ë™ì  ê°’ í¬í•¨)
    
    Args:
        action_count: ì˜ˆì¸¡í•  ì•¡ì…˜ ê°œìˆ˜
        memory_summary: ì´ì „ í–‰ë™ ìš”ì•½ (ì˜êµ¬ ë©”ëª¨ë¦¬)
        grounding_section: Grounding ì§€ì‹ ì„¹ì…˜ (ì‹¤ìˆ˜ ë¶„ì„ ë° êµí›ˆ)
    
    Returns:
        System Prompt ë¬¸ìì—´
    """
    memory_section = ""
    if memory_summary:
        memory_section = f"""
## Permanent Memory (Current Progress Summary)
{memory_summary}

**Important**: This memory contains a concise summary of what was done in the previous step and current progress toward the mission goal. Use this to understand where the robot is in the mission and plan the next actions accordingly. The memory will be updated after each step with a new summary (not accumulated).
"""
    
    grounding_section_text = ""
    if grounding_section:
        grounding_section_text = f"""
## Grounding Knowledge (Lessons Learned from Mistakes)
{grounding_section}

**Important**: This section contains knowledge learned from previous mistakes. When you made an error, the analysis of why it was wrong and how to avoid it in the future is recorded here. Always refer to this section to avoid repeating the same mistakes.
"""
    
    return f"""You are a robot action planner for object goal navigation.
{memory_section}
{grounding_section_text}

## Environment
Grid world with walls (black), blue pillar (impassable), purple table (impassable), robot (red arrow shows heading), and goal (green marker if present).

## Robot Orientation (CRITICAL)
**IMPORTANT**: The robot is represented as a RED ARROW. The robot's heading (orientation) is determined by the direction the ARROW POINT is pointing.
- The arrow's point (tip) indicates the robot's facing direction
- The arrow is drawn from the center of the robot's cell, pointing in the direction of movement
- When the arrow points RIGHT (â†’) â†’ heading = 0 (East, facing right)
- When the arrow points DOWN (â†“) â†’ heading = 1 (South, facing down)
- When the arrow points LEFT (â†) â†’ heading = 2 (West, facing left)
- When the arrow points UP (â†‘) â†’ heading = 3 (North, facing up)
- Always check the arrow's direction to determine the robot's current heading before planning actions
- The arrow is RED and clearly visible in the image

## Action Space
- 0 or "turn left": Rotate 90Â° counterclockwise
- 1 or "turn right": Rotate 90Â° clockwise
- 2 or "move forward": Move one cell forward in heading direction
- 3 or "pickup": Pick up object in front
- 4 or "drop": Drop carried object
- 5 or "toggle": Interact with objects (e.g., open doors)
- 6 or "done": Complete the task (terminate episode)

## Movement Rules
**CRITICAL**: All movements are RELATIVE to robot's current heading direction.
- "forward" = move one cell in facing direction
- "turn left/right" = rotate 90Â° from current heading
- Think in relative movements, NOT absolute coordinates
- Note: There is NO backward movement action. To move backward, turn 180Â° (turn left twice or turn right twice) then move forward.

## Response Format
You MUST predict a sequential trajectory of {action_count} actions. This is a continuous sequence of actions that the robot will take step by step. Respond in JSON format:
```json
{{
    "trajectory": [
        "<action_name_or_number>",
        "<action_name_or_number>",
        "<action_name_or_number>",
        ...
    ],
    "trajectory_reasoning": "<brief summary of the overall trajectory strategy>",
    "environment_info": "<description of current state with spatial relationships relative to robot>",
    "memory_update": "<concise summary updating the permanent memory: what was done in the previous step and current progress toward the mission goal. This will REPLACE the entire previous memory, not append to it. Keep it brief (2-3 sentences max).>",
    "grounding_update": "<ONLY if user feedback indicates a mistake: analyze why the previous action was wrong and provide knowledge to avoid this mistake in the future. Keep it brief (2-3 sentences). If no mistake feedback, leave empty or omit this field.>"
}}
```

**memory_update** (REQUIRED): You MUST provide a concise summary that updates the permanent memory. This should describe:
- What action was just taken in this step
- Current progress toward completing the mission
- This summary will REPLACE the entire previous memory (not append), so include all relevant context in a brief format (2-3 sentences max).
- This field is REQUIRED and must be included in every response.

**grounding_update** (REQUIRED when feedback detected): You MUST carefully analyze the user's prompt to determine if it contains ANY feedback indicating that the previous action was wrong, incorrect, or needs correction. Be SENSITIVE to feedback - even subtle corrections should be detected. Examples of feedback include:
- Explicit corrections: "wrong", "incorrect", "that's wrong", "no", "not that", "don't", "shouldn't", "error", "mistake"
- Questions about mistakes: "why did you...", "why didn't you...", "what are you doing", "where are you going"
- Negative feedback: "not feasible", "cannot", "should not", "avoid", "collided", "touching walls"
- Corrections with explanations: "you cannot turn to wall", "path should not collide", "you didn't even touch"
- ANY indication that the previous action was not correct or needs adjustment

**CRITICAL**: If the user prompt contains ANY of the above patterns or suggests the previous action was wrong, you MUST provide the "grounding_update" field. When provided, it should:
- Analyze why the previous action was wrong
- Explain what should have been done instead
- Provide knowledge/guidance to avoid this mistake in the future
- Keep it brief (2-3 sentences max)
- Be specific about the mistake and the correct approach

**Only omit this field** if the user prompt is clearly a normal instruction or continuation without any negative feedback or correction.

**Important**: 
- You MUST provide exactly {action_count} actions in the "trajectory" array as a sequential sequence
- The trajectory represents consecutive actions: action[0] is executed first, then action[1], then action[2], etc.
- Each action in the trajectory should consider the state after the previous action
- Only the first action will be executed in this step, but the full trajectory will be logged for analysis
- Think of this as planning the next {action_count} steps ahead

**environment_info**: Describe environment relative to robot's heading:
- Robot's heading and relative location to objects
- Obstacles and open paths relative to heading
- Traversability (blocked vs open paths)
- Spatial relationships affecting navigation

## Notes
- Valid JSON format required
- Actions must be from the list above
- Complete mission from user prompt
- Use relative movements based on heading, not coordinates
- Provide exactly {action_count} actions as a sequential trajectory
- Consider how each action affects the robot's position and heading for the next action
"""


def create_scenario2_environment():
    """
    ì‹œë‚˜ë¦¬ì˜¤ 2 ì‹¤í—˜ í™˜ê²½ ìƒì„±
    
    Returns:
        CustomRoomWrapper: ì‹œë‚˜ë¦¬ì˜¤ 2 í™˜ê²½ Wrapper ì¸ìŠ¤í„´ìŠ¤
    """
    size = 10
    
    # ì™¸ë²½ ìƒì„± (ê²€ì€ìƒ‰ ë²½)
    walls = []
    for i in range(size):
        walls.append((i, 0))      # ìƒë‹¨ ë²½
        walls.append((i, size-1))  # í•˜ë‹¨ ë²½
        walls.append((0, i))      # ì¢Œì¸¡ ë²½
        walls.append((size-1, i))  # ìš°ì¸¡ ë²½
    
    # íŒŒë€ ê¸°ë‘¥: 2x2 Grid (í†µê³¼ë¶ˆê°€)
    # ìœ„ì¹˜: (3, 4), (4, 4), (3, 5), (4, 5)
    # MiniGrid ì¢Œí‘œê³„: (x, y) = (ì—´, í–‰)
    blue_pillar_positions = [
        (3, 4),  # ì™¼ìª½ ìœ„
        (4, 4),  # ì˜¤ë¥¸ìª½ ìœ„
        (3, 5),  # ì™¼ìª½ ì•„ë˜
        (4, 5),  # ì˜¤ë¥¸ìª½ ì•„ë˜
    ]
    
    # í…Œì´ë¸”: ë³´ë¼ìƒ‰ 1x3 Grid (í†µê³¼ë¶ˆê°€)
    # ìœ„ì¹˜: (5, 1), (6, 1), (7, 1)
    table_positions = [
        (5, 1),  # ì™¼ìª½
        (6, 1),  # ì¤‘ì•™
        (7, 1),  # ì˜¤ë¥¸ìª½
    ]
    
    # ê°ì²´ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    objects = []
    
    # íŒŒë€ ê¸°ë‘¥ ë°°ì¹˜ (íŒŒë€ìƒ‰ Boxë¡œ êµ¬í˜„)
    # ì°¸ê³ : BoxëŠ” í†µê³¼ ê°€ëŠ¥í•˜ì§€ë§Œ, ì‹œê°ì ìœ¼ë¡œëŠ” ìƒ‰ìƒì´ ìˆëŠ” ê°ì²´ë¡œ í‘œì‹œë©ë‹ˆë‹¤.
    # í†µê³¼ ë¶ˆê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ë ¤ë©´ ë‚˜ì¤‘ì— CustomRoomEnvë¥¼ í™•ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
    for pos in blue_pillar_positions:
        objects.append({
            'type': 'box',
            'pos': pos,
            'color': 'blue'
        })
    
    # í…Œì´ë¸” ë°°ì¹˜ (ë³´ë¼ìƒ‰ Boxë¡œ êµ¬í˜„)
    for pos in table_positions:
        objects.append({
            'type': 'box',
            'pos': pos,
            'color': 'purple'
        })
    
    # ì‹œì‘ì : ë¹¨ê°• 1x1 (ì—ì´ì „íŠ¸ ì‹œì‘ ìœ„ì¹˜)
    # ìœ„ì¹˜: (1, 8) - ë ˆì´ì•„ì›ƒì—ì„œ ğŸŸ¥ ìœ„ì¹˜
    start_pos = (1, 8)
    
    # ì¢…ë£Œì : ì´ˆë¡ 1x1 (Goal)
    # ìœ„ì¹˜: (8, 1) - ë ˆì´ì•„ì›ƒì—ì„œ ğŸŸ© ìœ„ì¹˜
    goal_pos = (8, 1)
    
    # room_config êµ¬ì„±
    room_config = {
        'start_pos': start_pos,
        'goal_pos': goal_pos,
        'walls': walls,
        'objects': objects
    }
    
    # Wrapper ìƒì„± ë° ë°˜í™˜
    return CustomRoomWrapper(size=size, room_config=room_config)


def calculate_predicted_path(
    start_pos: tuple,
    start_dir: int,
    predicted_actions: list,
    wrapper: CustomRoomWrapper
) -> list:
    """
    ì˜ˆì¸¡ëœ ì•¡ì…˜ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê²½ë¡œë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        start_pos: ì‹œì‘ ìœ„ì¹˜ (x, y)
        start_dir: ì‹œì‘ ë°©í–¥ (0=East, 1=South, 2=West, 3=North)
        predicted_actions: ì˜ˆì¸¡ëœ ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ {'action': str, ...})
        wrapper: CustomRoomWrapper ì¸ìŠ¤í„´ìŠ¤ (ì•¡ì…˜ íŒŒì‹±ìš©)
    
    Returns:
        ê²½ë¡œ ë¦¬ìŠ¤íŠ¸: [(x, y, direction), ...] - ê° ìŠ¤í…ì˜ ìœ„ì¹˜ì™€ ë°©í–¥
    """
    path = [(start_pos[0], start_pos[1], start_dir)]  # ì‹œì‘ ìœ„ì¹˜ì™€ ë°©í–¥
    
    current_x, current_y = start_pos[0], start_pos[1]
    current_dir = start_dir
    
    # ë°©í–¥ ë²¡í„°: [dx, dy] for each direction
    # 0=East(â†’), 1=South(â†“), 2=West(â†), 3=North(â†‘)
    direction_vectors = {
        0: (1, 0),   # East: x+1
        1: (0, 1),   # South: y+1
        2: (-1, 0),  # West: x-1
        3: (0, -1)   # North: y-1
    }
    
    for action_item in predicted_actions:
        # action_itemì´ ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° 'action' í‚¤ì—ì„œ ì¶”ì¶œ, ì•„ë‹ˆë©´ ì§ì ‘ ì‚¬ìš©
        if isinstance(action_item, dict):
            action_str = str(action_item.get('action', '2'))
        else:
            action_str = str(action_item)
        
        try:
            action_idx = wrapper.parse_action(action_str)
        except (ValueError, AttributeError):
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ move forwardë¡œ ê°„ì£¼
            action_idx = 2
        
        # ì•¡ì…˜ì— ë”°ë¼ ìœ„ì¹˜/ë°©í–¥ ì—…ë°ì´íŠ¸
        if action_idx == 0:  # turn left (ë°˜ì‹œê³„ ë°©í–¥)
            current_dir = (current_dir - 1) % 4
        elif action_idx == 1:  # turn right (ì‹œê³„ ë°©í–¥)
            current_dir = (current_dir + 1) % 4
        elif action_idx == 2:  # move forward
            dx, dy = direction_vectors[current_dir]
            current_x += dx
            current_y += dy
        # else: pickup, drop, toggle, doneì€ ìœ„ì¹˜ ë³€ê²½ ì—†ìŒ
        
        # ê²½ë¡œì— ì¶”ê°€
        path.append((current_x, current_y, current_dir))
    
    return path


def visualize_grid_cli(wrapper: CustomRoomWrapper, state: dict, predicted_path: list = None):
    """
    CLIì—ì„œ ê·¸ë¦¬ë“œë¥¼ í…ìŠ¤íŠ¸ë¡œ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        wrapper: CustomRoomWrapper ì¸ìŠ¤í„´ìŠ¤
        state: í˜„ì¬ í™˜ê²½ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
        predicted_path: ì˜ˆì¸¡ëœ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ [(x, y, dir), ...] (ì„ íƒì )
    """
    env = wrapper.env
    size = wrapper.size
    
    # ì—ì´ì „íŠ¸ ìœ„ì¹˜ ë° ë°©í–¥
    agent_pos = state['agent_pos']
    if isinstance(agent_pos, np.ndarray):
        agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
    else:
        agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
    
    agent_dir = state['agent_dir']
    # ê°™ì€ í­ì˜ ë¬¸ì ì‚¬ìš© (ì •ë ¬ ë¬¸ì œ í•´ê²°)
    direction_symbols = {0: '>', 1: 'v', 2: '<', 3: '^'}
    direction_names = {0: 'East', 1: 'South', 2: 'West', 3: 'North'}
    agent_symbol = direction_symbols.get(agent_dir, 'A')
    agent_dir_name = direction_names.get(agent_dir, 'Unknown')
    
    # ì˜ˆì¸¡ ê²½ë¡œ ìœ„ì¹˜ ì§‘í•© ìƒì„± (ì‹œê°í™”ìš©)
    predicted_path_set = set()
    if predicted_path:
        for x, y, _ in predicted_path:
            # ê·¸ë¦¬ë“œ ë²”ìœ„ ë‚´ì— ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
            if 0 <= x < size and 0 <= y < size:
                predicted_path_set.add((x, y))
    
    # ê·¸ë¦¬ë“œ ìƒì„±
    grid_chars = []
    for y in range(size):
        row = []
        for x in range(size):
            # ê·¸ë¦¬ë“œ ì…€ í™•ì¸
            cell = env.grid.get(x, y)
            
            # ì—ì´ì „íŠ¸ ìœ„ì¹˜ì¸ ê²½ìš°
            if x == agent_x and y == agent_y:
                row.append(agent_symbol)
            # ì˜ˆì¸¡ ê²½ë¡œ ìœ„ì¹˜ì¸ ê²½ìš° (ì—ì´ì „íŠ¸ ìœ„ì¹˜ê°€ ì•„ë‹Œ ê²½ìš°ë§Œ)
            elif (x, y) in predicted_path_set:
                row.append('Â·')  # ì˜ˆì¸¡ ê²½ë¡œ í‘œì‹œ
            # ë²½ì¸ ê²½ìš°
            elif cell is not None and cell.type == 'wall':
                row.append('â¬›')
            # Goalì¸ ê²½ìš°
            elif cell is not None and cell.type == 'goal':
                row.append('ğŸŸ©')
            # ê°ì²´ì¸ ê²½ìš° (ìƒ‰ìƒì— ë”°ë¼)
            elif cell is not None:
                if hasattr(cell, 'color'):
                    if cell.color == 'blue':
                        row.append('ğŸŸ¦')
                    elif cell.color == 'purple':
                        row.append('ğŸŸª')
                    elif cell.color == 'red':
                        row.append('ğŸŸ¥')
                    elif cell.color == 'green':
                        row.append('ğŸŸ©')
                    else:
                        row.append('ğŸŸ¨')  # ê¸°íƒ€ ê°ì²´
                else:
                    row.append('ğŸŸ¨')
            # ë¹ˆ ê³µê°„
            else:
                row.append('â¬œï¸')
        grid_chars.append(row)
    
    # ê·¸ë¦¬ë“œ ì¶œë ¥
    print("\n" + "=" * 60)
    print("Current Grid State:")
    print("=" * 60)
    legend = "Legend: â¬›=Wall, â¬œï¸=Empty, ğŸŸ¦=Blue Pillar, ğŸŸª=Purple Table, ğŸŸ©=Goal, >v<^=Agent Direction (R/D/L/U)"
    if predicted_path:
        legend += ", Â·=Predicted Path"
    print(legend)
    print("=" * 60)
    for y in range(size):
        row_str = ''.join(grid_chars[y])
        print(row_str)
    print("=" * 60)
    print(f"Agent Position: ({agent_x}, {agent_y}), Direction: {agent_dir} ({agent_symbol} = {agent_dir_name})")
    print("=" * 60 + "\n")


def get_user_prompt(task_hint: str = None) -> str:
    """
    ì‚¬ìš©ìë¡œë¶€í„° í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥ë°›ëŠ” í•¨ìˆ˜ (CLI)
    
    Args:
        task_hint: ì‘ì—… íŒíŠ¸ (ìë™ ì‹¤í–‰ë˜ì§€ ì•ŠìŒ, ë‹¨ì§€ íŒíŠ¸ë¡œë§Œ í‘œì‹œ)
    
    Returns:
        ì‚¬ìš©ìê°€ ì…ë ¥í•œ í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
    """
    print("\n" + "=" * 60)
    if task_hint:
        print(f"Task Hint: {task_hint}")
        print("=" * 60)
    print("Enter your instruction for the agent (or press Enter to use default):")
    user_input = input("> ").strip()
    
    # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (ì‚¬ìš©ìê°€ ì•„ë¬´ê²ƒë„ ì…ë ¥í•˜ì§€ ì•Šì€ ê²½ìš°)
    if not user_input:
        # Task ì •ë³´ë¥¼ í¬í•¨í•œ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸
        if task_hint:
            default_prompt = f"Task: {task_hint}\n\nBased on the current image, choose the next action to complete this task."
        else:
            default_prompt = "Based on the current image, choose the next action to complete the task."
        print(f"Using default prompt: {default_prompt}")
        return default_prompt
    
    return user_input


def save_experiment_data(
    step: int,
    image: np.ndarray,
    state: dict,
    action: int,
    action_name: str,
    user_prompt: str,
    vlm_response: dict,
    reward: float,
    done: bool,
    log_dir: Path,
    all_predicted_actions: list = None,
    vlm_input: dict = None,
    vlm_output: dict = None,
    memory_summary: str = None,
    grounding_section: str = None
):
    """
    ì‹¤í—˜ ë°ì´í„°ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        step: í˜„ì¬ ìŠ¤í… ë²ˆí˜¸
        image: í™˜ê²½ ì´ë¯¸ì§€ (numpy ë°°ì—´)
        state: í™˜ê²½ ìƒíƒœ ì •ë³´
        action: ì‹¤í–‰ëœ ì•¡ì…˜ (ì •ìˆ˜)
        action_name: ì‹¤í–‰ëœ ì•¡ì…˜ ì´ë¦„ (ë¬¸ìì—´)
        user_prompt: ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
        vlm_response: VLM ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ (actions, environment_info)
        reward: ë³´ìƒ
        done: ì—í”¼ì†Œë“œ ì¢…ë£Œ ì—¬ë¶€
        log_dir: ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        all_predicted_actions: VLMì´ ì˜ˆì¸¡í•œ ëª¨ë“  ì•¡ì…˜ ë¦¬ìŠ¤íŠ¸ (ë¡œê¹…ìš©)
        vlm_input: VLM ì…ë ¥ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (image_info, system_prompt, user_prompt)
        vlm_output: VLM ì¶œë ¥ ì •ë³´ ë”•ì…”ë„ˆë¦¬ (raw_response, parsed_response, tokens_used ë“±)
        memory_summary: í˜„ì¬ ì˜êµ¬ ë©”ëª¨ë¦¬ ìš”ì•½ (ì„ íƒì )
        grounding_section: í˜„ì¬ Grounding ì§€ì‹ ì„¹ì…˜ (ì„ íƒì )
    """
    # 1. ì´ë¯¸ì§€ ì €ì¥ (PNG)
    image_path = log_dir / f"step_{step:04d}.png"
    img_pil = Image.fromarray(image)
    img_pil.save(image_path)
    
    # 2. JSON ë¡œê·¸ ì €ì¥
    # numpy íƒ€ì…ì„ Python ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
    agent_pos = state['agent_pos']
    if isinstance(agent_pos, np.ndarray):
        agent_pos_list = [int(agent_pos[0]), int(agent_pos[1])]
    elif isinstance(agent_pos, (tuple, list)):
        agent_pos_list = [int(agent_pos[0]), int(agent_pos[1])]
    else:
        # numpy scalarë‚˜ ë‹¤ë¥¸ íƒ€ì…ì¸ ê²½ìš°
        try:
            if hasattr(agent_pos, '__len__') and len(agent_pos) >= 2:
                agent_pos_list = [int(agent_pos[0]), int(agent_pos[1])]
            else:
                agent_pos_list = [0, 0]
        except (TypeError, IndexError):
            agent_pos_list = [0, 0]
    
    json_data = {
        "step": int(step),
        "timestamp": datetime.now().isoformat(),
        "state": {
            "agent_pos": agent_pos_list,
            "agent_dir": int(state['agent_dir']),
            "mission": str(state['mission']) if state['mission'] else ""
        },
        "action": {
            "index": int(action),
            "name": str(action_name)
        },
        "user_prompt": str(user_prompt),
        "vlm_response": {k: str(v) for k, v in vlm_response.items()},
        "all_predicted_actions": all_predicted_actions if all_predicted_actions else [],
        "memory_summary": str(memory_summary) if memory_summary else "",
        "grounding_section": str(grounding_section) if grounding_section else "",
        "reward": float(reward),
        "done": bool(done),
        "image_path": str(image_path.name),
        "vlm_input": vlm_input if vlm_input else {},
        "vlm_output": vlm_output if vlm_output else {}
    }
    
    # 2. JSON ë¡œê·¸ ì €ì¥ (í•˜ë‚˜ì˜ íŒŒì¼ì— ë°°ì—´ë¡œ ëˆ„ì )
    json_path = log_dir / "experiment_log.json"
    
    # ê¸°ì¡´ ë°ì´í„° ì½ê¸° (íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°)
    if json_path.exists():
        with open(json_path, 'r', encoding='utf-8') as f:
            try:
                all_data = json.load(f)
                if not isinstance(all_data, list):
                    all_data = [all_data]  # ê¸°ì¡´ ë°ì´í„°ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹ˆë©´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            except json.JSONDecodeError:
                all_data = []
    else:
        all_data = []
    
    # ìƒˆ ë°ì´í„° ì¶”ê°€
    all_data.append(json_data)
    
    # íŒŒì¼ì— ì €ì¥
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(all_data, f, indent=2, ensure_ascii=False)
    
    # 2-1. VLM I/O ë³„ë„ ë¡œê·¸ íŒŒì¼ ì €ì¥ (í…ìŠ¤íŠ¸ í˜•ì‹, í•˜ë‚˜ì˜ íŒŒì¼ì— ëˆ„ì )
    if vlm_input or vlm_output:
        vlm_io_path = log_dir / "vlm_io_log.txt"
        
        # ì¶”ê°€ ëª¨ë“œë¡œ ì—´ê¸° (íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒì„±)
        with open(vlm_io_path, 'a', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write(f"VLM I/O Log - Step {step}\n")
            f.write("=" * 80 + "\n\n")
            
            # VLM ì…ë ¥ ì •ë³´
            f.write("VLM INPUT:\n")
            f.write("-" * 80 + "\n")
            if vlm_input:
                f.write(f"Image Shape: {vlm_input.get('image_shape', 'N/A')}\n")
                f.write(f"Image Dtype: {vlm_input.get('image_dtype', 'N/A')}\n")
                f.write(f"Image Value Range: [{vlm_input.get('image_min', 'N/A')}, {vlm_input.get('image_max', 'N/A')}]\n")
                f.write(f"System Prompt: Used (Length: {vlm_input.get('system_prompt_length', 0)} characters, see {vlm_input.get('system_prompt_file', 'system_prompt.txt')})\n")
                f.write(f"User Prompt Length: {vlm_input.get('user_prompt_length', 0)} characters\n")
                f.write(f"\nUser Prompt:\n{vlm_input.get('user_prompt', 'N/A')}\n")
                # System PromptëŠ” VLM API í˜¸ì¶œì— í¬í•¨ë˜ì§€ë§Œ, ì „ì²´ ë‚´ìš©ì€ system_prompt.txt íŒŒì¼ ì°¸ì¡°
            else:
                f.write("No input data\n")
            
            f.write("\n" + "=" * 80 + "\n\n")
            
            # VLM ì¶œë ¥ ì •ë³´
            f.write("VLM OUTPUT:\n")
            f.write("-" * 80 + "\n")
            if vlm_output:
                f.write(f"Raw Response Length: {vlm_output.get('raw_response_length', 0)} characters\n")
                f.write(f"Inference Time: {vlm_output.get('inference_time_seconds', 0):.2f} seconds\n")
                f.write(f"Tokens Used: {vlm_output.get('tokens_used', 0)}\n")
                f.write(f"Parsing Success: {vlm_output.get('parsing_success', 'N/A')}\n")
                if vlm_output.get('parsing_error'):
                    f.write(f"Parsing Error: {vlm_output.get('parsing_error')}\n")
                f.write(f"\nRaw Response:\n{vlm_output.get('raw_response', 'N/A')}\n")
                if vlm_output.get('parsed_response'):
                    f.write(f"\nParsed Response:\n")
                    for k, v in vlm_output.get('parsed_response', {}).items():
                        f.write(f"  {k}: {v}\n")
            else:
                f.write("No output data\n")
            
            # ì˜êµ¬ ë©”ëª¨ë¦¬ ì •ë³´
            if memory_summary:
                f.write("\n" + "-" * 80 + "\n")
                f.write("PERMANENT MEMORY (Updated):\n")
                f.write("-" * 80 + "\n")
                f.write(f"{memory_summary}\n")
            
            # Grounding ì§€ì‹ ì •ë³´
            if grounding_section:
                f.write("\n" + "-" * 80 + "\n")
                f.write("GROUNDING KNOWLEDGE:\n")
                f.write("-" * 80 + "\n")
                f.write(f"{grounding_section}\n")
            
            f.write("\n" + "=" * 80 + "\n\n")
    
    # 3. CSV ë¡œê·¸ ì €ì¥ (ì¶”ê°€ ëª¨ë“œ)
    csv_path = log_dir / "experiment_log.csv"
    file_exists = csv_path.exists()
    
    with open(csv_path, 'a', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        
        # í—¤ë” ì‘ì„± (ì²« ë²ˆì§¸ ì‹¤í–‰ ì‹œì—ë§Œ)
        if not file_exists:
            writer.writerow([
                "step", "timestamp", "agent_x", "agent_y", "agent_dir",
                "action_index", "action_name", "user_prompt",
                "vlm_action_executed", "vlm_environment_info", "vlm_all_predicted_actions",
                "memory_summary",
                "reward", "done", "image_path",
                "vlm_image_shape", "vlm_image_dtype", "vlm_system_prompt_len",
                "vlm_user_prompt_len", "vlm_raw_response_len", "vlm_inference_time_seconds", "vlm_tokens_used"
            ])
        
        # ë°ì´í„° ì‘ì„±
        agent_pos = state['agent_pos']
        if isinstance(agent_pos, np.ndarray):
            agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
        elif isinstance(agent_pos, (tuple, list)):
            agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
        else:
            # numpy scalarì¸ ê²½ìš°
            try:
                if hasattr(agent_pos, '__len__') and len(agent_pos) >= 2:
                    agent_x, agent_y = int(agent_pos[0]), int(agent_pos[1])
                else:
                    agent_x, agent_y = 0, 0
            except (TypeError, IndexError):
                agent_x, agent_y = 0, 0
        
        # VLM I/O ì •ë³´ ì¶”ì¶œ
        vlm_img_shape = vlm_input.get('image_shape', '') if vlm_input else ''
        vlm_img_dtype = vlm_input.get('image_dtype', '') if vlm_input else ''
        vlm_sys_prompt_len = vlm_input.get('system_prompt_length', 0) if vlm_input else 0
        vlm_usr_prompt_len = vlm_input.get('user_prompt_length', 0) if vlm_input else 0
        vlm_raw_resp_len = vlm_output.get('raw_response_length', 0) if vlm_output else 0
        vlm_inference_time = vlm_output.get('inference_time_seconds', 0.0) if vlm_output else 0.0
        vlm_tokens = vlm_output.get('tokens_used', 0) if vlm_output else 0
        
        # ëª¨ë“  ì˜ˆì¸¡ëœ ì•¡ì…˜ì„ JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (CSVìš©)
        all_actions_json = json.dumps(all_predicted_actions, ensure_ascii=False) if all_predicted_actions else "[]"
        # ì²« ë²ˆì§¸ ì•¡ì…˜ ì •ë³´ (ì‹¤í–‰ëœ ì•¡ì…˜)
        first_action_str = all_predicted_actions[0].get('action', '') if all_predicted_actions else vlm_response.get('action', '')
        
        writer.writerow([
            step,
            datetime.now().isoformat(),
            agent_x,
            agent_y,
            int(state['agent_dir']),
            int(action),
            action_name,
            user_prompt,
            first_action_str,  # ì‹¤í–‰ëœ ì•¡ì…˜ (ì²« ë²ˆì§¸ ì˜ˆì¸¡)
            vlm_response.get('environment_info', ''),
            all_actions_json,  # ëª¨ë“  ì˜ˆì¸¡ëœ ì•¡ì…˜ (JSON ë¬¸ìì—´)
            str(memory_summary) if memory_summary else '',  # ì˜êµ¬ ë©”ëª¨ë¦¬ ìš”ì•½
            str(grounding_section) if grounding_section else '',  # Grounding ì§€ì‹
            float(reward),
            bool(done),
            image_path.name,
            str(vlm_img_shape),
            str(vlm_img_dtype),
            int(vlm_sys_prompt_len),
            int(vlm_usr_prompt_len),
            int(vlm_raw_resp_len),
            float(vlm_inference_time),
            int(vlm_tokens)
        ])


def run_vlm_controlled_experiment():
    """
    VLMì„ í†µí•œ í™˜ê²½ ì œì–´ ì‹¤í—˜ ë©”ì¸ í•¨ìˆ˜
    """
    print("=" * 60)
    print("ì‹œë‚˜ë¦¬ì˜¤ 2: VLM ì œì–´ ì‹¤í—˜")
    print("=" * 60)
    print("\ní™˜ê²½ êµ¬ì„±:")
    print("  - íŒŒë€ ê¸°ë‘¥: 2x2 Grid (í†µê³¼ë¶ˆê°€)")
    print("  - í…Œì´ë¸”: ë³´ë¼ìƒ‰ 1x3 Grid (í†µê³¼ë¶ˆê°€)")
    print("  - ì‹œì‘ì : ë¹¨ê°• (1, 8)")
    print("  - ì¢…ë£Œì : ì´ˆë¡ (8, 1)")
    print("\nMission: íŒŒë€ ê¸°ë‘¥ìœ¼ë¡œ ê°€ì„œ ì˜¤ë¥¸ìª½ìœ¼ë¡œ ëŒê³ , í…Œì´ë¸” ì˜†ì— ë©ˆì¶”ì‹œì˜¤")
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("logs") / f"scenario2_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    log_dir.mkdir(parents=True, exist_ok=True)
    print(f"\në¡œê·¸ ë””ë ‰í† ë¦¬: {log_dir}")
    
    # ì˜êµ¬ ë©”ëª¨ë¦¬ íŒŒì¼ ê²½ë¡œ
    memory_file = log_dir / "permanent_memory.txt"
    
    # ì˜êµ¬ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)
    memory_summary = ""
    grounding_section = ""
    if memory_file.exists():
        with open(memory_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            # ë©”ëª¨ë¦¬ íŒŒì¼ êµ¬ì¡°: === MEMORY SUMMARY === ì™€ === GROUNDING === êµ¬ë¶„
            if "=== MEMORY SUMMARY ===" in content:
                parts = content.split("=== GROUNDING ===")
                memory_summary = parts[0].replace("=== MEMORY SUMMARY ===", "").strip()
                if len(parts) > 1:
                    grounding_section = parts[1].strip()
            else:
                # êµ¬ë²„ì „ í˜•ì‹ (ì „ì²´ë¥¼ memory_summaryë¡œ ì²˜ë¦¬)
                memory_summary = content
        print(f"ì˜êµ¬ ë©”ëª¨ë¦¬ ë¡œë“œ: memory_summary={len(memory_summary)} ë¬¸ì, grounding={len(grounding_section)} ë¬¸ì")
    else:
        print("ì˜êµ¬ ë©”ëª¨ë¦¬ ì´ˆê¸°í™”: ë¹ˆ ë©”ëª¨ë¦¬")
    
    # System Prompt ìƒì„± (ë™ì  ê°’ í¬í•¨, ë©”ëª¨ë¦¬ ë° grounding í¬í•¨)
    SYSTEM_PROMPT = get_system_prompt(ACTION_PREDICTION_COUNT, memory_summary, grounding_section)
    
    # System Promptë¥¼ ì²˜ìŒì—ë§Œ ë³„ë„ íŒŒì¼ë¡œ ì €ì¥
    system_prompt_path = log_dir / "system_prompt.txt"
    with open(system_prompt_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("System Prompt\n")
        f.write("=" * 80 + "\n\n")
        f.write(SYSTEM_PROMPT)
        f.write("\n\n" + "=" * 80 + "\n")
    print(f"System Prompt ì €ì¥: {system_prompt_path}")
    
    # 1. í™˜ê²½ ìƒì„±
    print("\n[1] í™˜ê²½ ìƒì„± ì¤‘...")
    wrapper = create_scenario2_environment()
    
    # 2. í™˜ê²½ ì´ˆê¸°í™”
    print("[2] í™˜ê²½ ì´ˆê¸°í™” ì¤‘...")
    wrapper.reset()
    
    # í™˜ê²½ ìƒíƒœ ì •ë³´ ì¶œë ¥
    state = wrapper.get_state()
    print(f"ì—ì´ì „íŠ¸ ì‹œì‘ ìœ„ì¹˜: {state['agent_pos']}")
    print(f"ì—ì´ì „íŠ¸ ë°©í–¥: {state['agent_dir']} (0=ì˜¤ë¥¸ìª½, 1=ì•„ë˜, 2=ì™¼ìª½, 3=ìœ„)")
    print(f"ë¯¸ì…˜: {state['mission']}")
    
    # 3. VLM Wrapper ì´ˆê¸°í™”
    print("\n[3] VLM Wrapper ì´ˆê¸°í™” ì¤‘...")
    try:
        # ì½”ë“œ ìƒë‹¨ì˜ VLM ì„¤ì • ë³€ìˆ˜ ì‚¬ìš©
        vlm = ChatGPT4oVLMWrapper(
            model=VLM_MODEL,
            temperature=VLM_TEMPERATURE,
            max_tokens=VLM_MAX_TOKENS
        )
        
        print(f"VLM Wrapper ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"  - ëª¨ë¸: {VLM_MODEL}")
        print(f"  - Temperature: {VLM_TEMPERATURE}")
        print(f"  - Max Tokens: {VLM_MAX_TOKENS}")
    except Exception as e:
        print(f"VLM Wrapper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("\nVLM ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”:")
        print("  - ì½”ë“œ ìƒë‹¨ì˜ VLM_MODEL, VLM_TEMPERATURE, VLM_MAX_TOKENS ë³€ìˆ˜ í™•ì¸")
        print("  - OpenAI API í‚¤ í™•ì¸: export OPENAI_API_KEY=your-key")
        return
    
    # 4. VLM PostProcessor ì´ˆê¸°í™”
    print("[4] VLM PostProcessor ì´ˆê¸°í™” ì¤‘...")
    postprocessor = VLMResponsePostProcessor(
        required_fields=["trajectory", "environment_info", "memory_update"]  # memory_updateëŠ” í•„ìˆ˜ í•„ë“œ
    )
    print(f"PostProcessor ì´ˆê¸°í™” ì™„ë£Œ (ê¶¤ì  ê¸¸ì´: {ACTION_PREDICTION_COUNT})")
    
    # 5. ì•¡ì…˜ ê³µê°„ ì •ë³´ ì¶œë ¥
    action_space = wrapper.get_action_space()
    print(f"\nì•¡ì…˜ ê³µê°„: {action_space['actions']}")
    
    # ë©”ì¸ ë£¨í”„
    step = 0
    done = False
    task_hint = "Mission: Go to the blue pillar, turn right, then stop next to the table."
    
    # ì´ì „ ìŠ¤í…ì˜ ì˜ˆì¸¡ ê²½ë¡œ ì €ì¥ (ì‹œê°í™” ìœ ì§€ìš©)
    previous_predicted_path = None
    
    print("\n" + "=" * 60)
    print("ì‹¤í—˜ ì‹œì‘")
    print("=" * 60)
    print("OpenCV ì°½ì´ ì—´ë¦½ë‹ˆë‹¤. í™˜ê²½ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("=" * 60)
    
    # OpenCV ì°½ ì´ë¦„ ê³ ì • (í•˜ë‚˜ì˜ ì°½ë§Œ ì‚¬ìš©)
    WINDOW_NAME = "Scenario 2: VLM Control"
    
    def display_image(img, window_name=None, predicted_path=None, cell_size=32):
        """
        OpenCVë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ í‘œì‹œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
        í•˜ë‚˜ì˜ ì°½ë§Œ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ë§Œ ìˆ˜í–‰
        
        Args:
            img: í‘œì‹œí•  ì´ë¯¸ì§€ (RGB)
            window_name: ì°½ ì´ë¦„
            predicted_path: ì˜ˆì¸¡ëœ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸ [(x, y, dir), ...] (ì„ íƒì )
            cell_size: ê·¸ë¦¬ë“œ ì…€ í¬ê¸° (í”½ì…€)
        """
        if window_name is None:
            window_name = WINDOW_NAME
        
        if img is not None:
            try:
                # RGBë¥¼ BGRë¡œ ë³€í™˜ (OpenCVëŠ” BGR í˜•ì‹ì„ ì‚¬ìš©)
                img_bgr = cv2.cvtColor(img.copy(), cv2.COLOR_RGB2BGR)
                
                # ì˜ˆì¸¡ ê²½ë¡œ ê·¸ë¦¬ê¸°
                if predicted_path and len(predicted_path) > 1:
                    # ê²½ë¡œë¥¼ ê·¸ë¦¬ë“œ ì¢Œí‘œì—ì„œ í”½ì…€ ì¢Œí‘œë¡œ ë³€í™˜
                    for i in range(len(predicted_path) - 1):
                        x1, y1, _ = predicted_path[i]
                        x2, y2, _ = predicted_path[i + 1]
                        
                        # ê·¸ë¦¬ë“œ ì…€ ì¤‘ì‹¬ ì¢Œí‘œ ê³„ì‚°
                        px1 = int((x1 + 0.5) * cell_size)
                        py1 = int((y1 + 0.5) * cell_size)
                        px2 = int((x2 + 0.5) * cell_size)
                        py2 = int((y2 + 0.5) * cell_size)
                        
                        # ê²½ë¡œ ì„  ê·¸ë¦¬ê¸° (ë…¸ë€ìƒ‰, ë‘ê»˜ 2)
                        cv2.line(img_bgr, (px1, py1), (px2, py2), (0, 255, 255), 2)
                    
                    # ê²½ë¡œ ì  í‘œì‹œ (ì‘ì€ ì›)
                    for x, y, _ in predicted_path[1:]:  # ì‹œì‘ì  ì œì™¸
                        px = int((x + 0.5) * cell_size)
                        py = int((y + 0.5) * cell_size)
                        cv2.circle(img_bgr, (px, py), 3, (0, 255, 255), -1)
                
                # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ë” í¬ê²Œ í‘œì‹œ)
                height, width = img_bgr.shape[:2]
                max_size = 1200
                scale = 1
                if height < max_size and width < max_size:
                    scale = min(max_size // height, max_size // width, 6)
                
                if scale > 1:
                    new_width = width * scale
                    new_height = height * scale
                    img_bgr = cv2.resize(img_bgr, (new_width, new_height), interpolation=cv2.INTER_NEAREST)
                
                # ì´ë¯¸ì§€ ì°½ì— í‘œì‹œ (ê°™ì€ ì°½ ì´ë¦„ ì‚¬ìš©í•˜ì—¬ ì—…ë°ì´íŠ¸ë§Œ ìˆ˜í–‰)
                cv2.imshow(window_name, img_bgr)
                cv2.waitKey(1)  # ì°½ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ ì§§ì€ ëŒ€ê¸°
            except Exception as e:
                print(f"ì´ë¯¸ì§€ í‘œì‹œ ì˜¤ë¥˜: {e}")
    
    while not done:
        step += 1
        print("\n" + "=" * 80)
        print(f"STEP {step} START")
        print("=" * 80)
        
        # í˜„ì¬ í™˜ê²½ ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
        image = wrapper.get_image()
        state = wrapper.get_state()
        
        # í™˜ê²½ ì •ë³´ ì¶œë ¥
        print(f"í˜„ì¬ ìœ„ì¹˜: {state['agent_pos']}, ë°©í–¥: {state['agent_dir']}")
        
        # ì˜êµ¬ ë©”ëª¨ë¦¬ ì½ê¸° (VLM í˜¸ì¶œ ì „)
        if memory_file.exists():
            with open(memory_file, 'r', encoding='utf-8') as f:
                memory_summary = f.read().strip()
        else:
            memory_summary = ""
        
        # System Prompt ì—…ë°ì´íŠ¸ (ë©”ëª¨ë¦¬ í¬í•¨)
        SYSTEM_PROMPT = get_system_prompt(ACTION_PREDICTION_COUNT, memory_summary)
        
        # ì´ˆê¸° ì‹œê°í™” (ì´ì „ ìŠ¤í…ì˜ ì˜ˆì¸¡ ê²½ë¡œ í‘œì‹œ)
        all_predicted_actions = []
        
        # CLI í…ìŠ¤íŠ¸ ì‹œê°í™” (ì´ì „ ì˜ˆì¸¡ ê²½ë¡œ í¬í•¨)
        visualize_grid_cli(wrapper, state, previous_predicted_path)
        
        # GUIì— í˜„ì¬ ìƒíƒœ í‘œì‹œ (ì´ì „ ì˜ˆì¸¡ ê²½ë¡œ í¬í•¨)
        display_image(image, WINDOW_NAME, previous_predicted_path)
        
        # 3. User prompt ì…ë ¥ ë°›ê¸° (task hint í¬í•¨)
        user_prompt = get_user_prompt(task_hint=task_hint)
        
        # 5. VLMì— ìš”ì²­ ì „ì†¡
        print(f"\n[5] VLMì— ìš”ì²­ ì „ì†¡ ì¤‘...")
        # VLM í˜¸ì¶œ ì§ì „ì— ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ê°€ì ¸ì™€ì„œ ìµœì‹  ìƒíƒœ ë³´ì¥
        image = wrapper.get_image()
        
        # ì´ë¯¸ì§€ í•´ì‹œë¥¼ ê³„ì‚°í•˜ì—¬ ë³€ê²½ ì—¬ë¶€ í™•ì¸ (ë””ë²„ê¹…ìš©)
        image_hash = hashlib.md5(image.tobytes()).hexdigest()[:8] if image is not None else None
        print(f"  - ì´ë¯¸ì§€ í•´ì‹œ (ë³€ê²½ í™•ì¸ìš©): {image_hash}")
        
        # VLM ì…ë ¥ ì •ë³´ ìˆ˜ì§‘
        # System PromptëŠ” VLM API í˜¸ì¶œì— í¬í•¨ë˜ì§€ë§Œ, ë¡œê¹…ì—ì„œëŠ” ì°¸ì¡°ë§Œ ì €ì¥ (ì „ì²´ ë‚´ìš©ì€ system_prompt.txtì— ì €ì¥ë¨)
        vlm_input_info = {
            'image_shape': str(image.shape) if image is not None else None,
            'image_dtype': str(image.dtype) if image is not None else None,
            'image_min': float(image.min()) if image is not None else None,
            'image_max': float(image.max()) if image is not None else None,
            'image_hash': image_hash,
            'system_prompt_length': len(SYSTEM_PROMPT),
            'system_prompt_file': 'system_prompt.txt',  # System PromptëŠ” ë³„ë„ íŒŒì¼ ì°¸ì¡°
            'user_prompt_length': len(user_prompt),
            'user_prompt': user_prompt
        }
        
        # ì´ë¯¸ì§€ ì •ë³´ í™•ì¸ ë° ì¶œë ¥
        if image is not None:
            print(f"  - ì´ë¯¸ì§€ í¬ê¸°: {image.shape}, íƒ€ì…: {image.dtype}")
            print(f"  - ì´ë¯¸ì§€ ê°’ ë²”ìœ„: [{image.min()}, {image.max()}]")
            print(f"  - Agent ìœ„ì¹˜ (ì´ë¯¸ì§€ ì „ì†¡ ì‹œì ): {state['agent_pos']}")
        else:
            print("  - ê²½ê³ : ì´ë¯¸ì§€ê°€ Noneì…ë‹ˆë‹¤!")
        
        try:
            # VLM ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì‹œì‘
            vlm_start_time = time.time()
            vlm_response_raw = vlm.generate(
                image=image,
                system_prompt=SYSTEM_PROMPT,
                user_prompt=user_prompt
            )
            # VLM ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
            vlm_end_time = time.time()
            vlm_inference_time = vlm_end_time - vlm_start_time
            
            print("VLM ì‘ë‹µ ìˆ˜ì‹  ì™„ë£Œ")
            print(f"  - ì‘ë‹µ ê¸¸ì´: {len(vlm_response_raw)} ë¬¸ì")
            print(f"  - ì¶”ë¡  ì‹œê°„: {vlm_inference_time:.2f}ì´ˆ")
            print(f"  - ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {vlm_response_raw[:150]}...")
        except Exception as e:
            print(f"VLM API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            vlm_response_raw = ""
            vlm_inference_time = 0.0
            vlm_input_info['error'] = str(e)
            break
        
        # 6. VLM ì‘ë‹µ í›„ì²˜ë¦¬
        print(f"[6] VLM ì‘ë‹µ í›„ì²˜ë¦¬ ì¤‘...")
        # VLM ì¶œë ¥ ì •ë³´ ìˆ˜ì§‘
        vlm_output_info = {
            'raw_response': vlm_response_raw,
            'raw_response_length': len(vlm_response_raw),
            'inference_time_seconds': vlm_inference_time,  # VLM ì¶”ë¡  ì‹œê°„ ì¶”ê°€
            'tokens_used': 0  # vlm_wrapperì—ì„œ í† í° ì •ë³´ë¥¼ ë°˜í™˜í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ 0ìœ¼ë¡œ ì„¤ì •
        }
        
        # actions ë°°ì—´ íŒŒì‹± ë° ì²« ë²ˆì§¸ ì•¡ì…˜ ì¶”ì¶œ
        all_predicted_actions = []  # ëª¨ë“  ì˜ˆì¸¡ëœ ì•¡ì…˜ ì €ì¥ (ë¡œê¹…ìš©)
        first_action_str = '2'  # ê¸°ë³¸ê°’: move forward
        first_action_index = 2
        first_action_name = "move forward"
        
        try:
            vlm_response = postprocessor.process(vlm_response_raw, strict=True)
            vlm_output_info['parsed_response'] = vlm_response
            vlm_output_info['parsing_success'] = True
            
            # trajectory ë°°ì—´ì—ì„œ ëª¨ë“  ì•¡ì…˜ ì¶”ì¶œ
            trajectory_list = vlm_response.get('trajectory', [])
            
            # trajectoryê°€ ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # (vlm_postprocessorê°€ ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì„ ìœ ì§€í•˜ë„ë¡ ìˆ˜ì •ë¨)
            if isinstance(trajectory_list, str):
                # ë¬¸ìì—´ì¸ ê²½ìš° JSON íŒŒì‹± ì‹œë„ (í•˜ìœ„ í˜¸í™˜ì„±)
                try:
                    trajectory_list = json.loads(trajectory_list)
                except (json.JSONDecodeError, ValueError):
                    trajectory_list = []
            
            if not isinstance(trajectory_list, list):
                trajectory_list = []
            
            print(f"íŒŒì‹±ëœ ì‘ë‹µ:")
            print(f"  - ì˜ˆì¸¡ëœ ê¶¤ì  ê¸¸ì´: {len(trajectory_list)}")
            trajectory_reasoning = vlm_response.get('trajectory_reasoning', 'N/A')
            if trajectory_reasoning and trajectory_reasoning != 'N/A':
                print(f"  - ê¶¤ì  ì „ëµ: {trajectory_reasoning[:150]}...")
            print(f"  - Environment Info: {vlm_response.get('environment_info', 'N/A')[:100]}...")
            
            # ëª¨ë“  ì•¡ì…˜ ì •ë³´ ì €ì¥ ë° ì¶œë ¥ (ìˆœì°¨ì  ê¶¤ì ìœ¼ë¡œ ì²˜ë¦¬)
            for idx, action_item in enumerate(trajectory_list):
                # action_itemì´ ë¬¸ìì—´ì´ê±°ë‚˜ ìˆ«ìì¸ ê²½ìš°
                if isinstance(action_item, (str, int)):
                    action_str = str(action_item)
                elif isinstance(action_item, dict):
                    # ë”•ì…”ë„ˆë¦¬ì¸ ê²½ìš° 'action' í‚¤ì—ì„œ ì¶”ì¶œ
                    action_str = str(action_item.get('action', '2'))
                else:
                    action_str = str(action_item)
                
                all_predicted_actions.append({
                    'step': idx + 1,
                    'action': action_str
                })
                
                print(f"  - Step {idx + 1}: {action_str}")
                
                # ì²« ë²ˆì§¸ ì•¡ì…˜ë§Œ ì¶”ì¶œ (ì‹¤í–‰ìš©)
                if idx == 0:
                    first_action_str = action_str
            
            # ì²« ë²ˆì§¸ ì•¡ì…˜ íŒŒì‹± ë° ì˜ˆì¸¡ ê²½ë¡œ ê³„ì‚°
            predicted_path = None
            if all_predicted_actions:
                try:
                    first_action_index = wrapper.parse_action(first_action_str)
                    first_action_name = wrapper.ACTION_NAMES.get(first_action_index, f"action_{first_action_index}")
                except ValueError as e:
                    print(f"ì²« ë²ˆì§¸ ì•¡ì…˜ íŒŒì‹± ì‹¤íŒ¨: {e}, ê¸°ë³¸ ì•¡ì…˜ ì‚¬ìš©")
                    first_action_index = 2
                    first_action_name = "move forward"
                
                # ì˜ˆì¸¡ ê²½ë¡œ ê³„ì‚°
                agent_pos = state['agent_pos']
                if isinstance(agent_pos, np.ndarray):
                    start_pos = (int(agent_pos[0]), int(agent_pos[1]))
                else:
                    start_pos = (int(agent_pos[0]), int(agent_pos[1]))
                start_dir = int(state['agent_dir'])
                
                predicted_path = calculate_predicted_path(
                    start_pos=start_pos,
                    start_dir=start_dir,
                    predicted_actions=all_predicted_actions,
                    wrapper=wrapper
                )
                print(f"  - ì˜ˆì¸¡ ê²½ë¡œ ê³„ì‚° ì™„ë£Œ: {len(predicted_path)}ê°œ ìœ„ì¹˜")
            else:
                print("ê²½ê³ : trajectory ë°°ì—´ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì•¡ì…˜ ì‚¬ìš©")
                predicted_path = None
                
        except ValueError as e:
            print(f"ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì›ë³¸ ì‘ë‹µ: {vlm_response_raw[:200]}...")
            vlm_output_info['parsing_success'] = False
            vlm_output_info['parsing_error'] = str(e)
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì•¡ì…˜ ì‚¬ìš©
            vlm_response = {
                'trajectory': [],
                'trajectory_reasoning': 'Parsing failed',
                'environment_info': 'Parsing failed'
            }
        
        # 7. MiniGrid ì•¡ì…˜ ì‹¤í–‰ (ì²« ë²ˆì§¸ ì•¡ì…˜ë§Œ)
        print(f"\n[7] ì•¡ì…˜ ì‹¤í–‰ ì¤‘...")
        print(f"ì‹¤í–‰í•  ì•¡ì…˜ (ì²« ë²ˆì§¸ ì˜ˆì¸¡): {first_action_name} (ì¸ë±ìŠ¤: {first_action_index})")
        
        try:
            # ì²« ë²ˆì§¸ ì•¡ì…˜ ì‹¤í–‰
            _, reward, terminated, truncated, _ = wrapper.step(first_action_index)
            done = terminated or truncated
            
            print(f"ë³´ìƒ: {reward}, ì¢…ë£Œ: {done}")
            
        except Exception as e:
            print(f"ì•¡ì…˜ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            print("ê¸°ë³¸ ì•¡ì…˜(move forward)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            first_action_index = 2
            first_action_name = "move forward"
            _, reward, terminated, truncated, _ = wrapper.step(first_action_index)
            done = terminated or truncated
        
        # 8. í™˜ê²½ ì •ë³´ ì¶œë ¥
        print(f"[8] í™˜ê²½ ì •ë³´:")
        new_state = wrapper.get_state()
        print(f"  - ìœ„ì¹˜: {new_state['agent_pos']}")
        print(f"  - ë°©í–¥: {new_state['agent_dir']}")
        print(f"  - ë³´ìƒ: {reward}")
        print(f"  - ì¢…ë£Œ: {done}")
        
        # ì˜êµ¬ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ (VLM ì‘ë‹µ í›„)
        memory_update = vlm_response.get('memory_update', '')
        grounding_update = vlm_response.get('grounding_update', '')
        
        # Memory Summary ì—…ë°ì´íŠ¸
        if memory_update and memory_update.strip():
            new_memory_summary = memory_update.strip()
            memory_summary = new_memory_summary
            print(f"[8-1] ì˜êµ¬ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(memory_summary)} ë¬¸ì")
            print(f"  - ë©”ëª¨ë¦¬ ë‚´ìš©: {memory_summary[:100]}...")
        else:
            # memory_updateê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš° ê²½ê³ 
            print(f"[8-1] ê²½ê³ : ì˜êµ¬ ë©”ëª¨ë¦¬ ì—…ë°ì´íŠ¸ ì—†ìŒ")
            if 'memory_update' not in vlm_response:
                print(f"  - memory_update í•„ë“œê°€ VLM ì‘ë‹µì— ì—†ìŠµë‹ˆë‹¤.")
            elif not memory_update.strip():
                print(f"  - memory_update í•„ë“œê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        # Grounding ì—…ë°ì´íŠ¸ (feedbackì´ ìˆëŠ” ê²½ìš°)
        if grounding_update and grounding_update.strip():
            print(f"\n[8-2] âš ï¸  Feedback ì¸ì‹ë¨: Grounding ì§€ì‹ ì—…ë°ì´íŠ¸")
            print("=" * 80)
            
            # ì—…ë°ì´íŠ¸ëœ grounding (ìƒˆë¡œ ì¶”ê°€ëœ ë¶€ë¶„)
            new_grounding_text = grounding_update.strip()
            print(f"\n[ìƒˆë¡œ ì¶”ê°€ëœ Grounding ì§€ì‹]:")
            print("-" * 80)
            print(new_grounding_text)
            print("-" * 80)
            
            # Grounding ì„¹ì…˜ì— ìƒˆë¡œìš´ ì§€ì‹ ì¶”ê°€ (ëˆ„ì )
            if grounding_section:
                new_grounding = f"{grounding_section}\n\n{new_grounding_text}"
            else:
                new_grounding = new_grounding_text
            grounding_section = new_grounding
            
            # ì „ì²´ Grounding ì¶œë ¥
            print(f"\n[ì „ì²´ Grounding ì§€ì‹ (ëˆ„ì )]:")
            print("=" * 80)
            print(grounding_section)
            print("=" * 80)
            print(f"\nGrounding ì§€ì‹ ì—…ë°ì´íŠ¸ ì™„ë£Œ: ì´ {len(grounding_section)} ë¬¸ì")
        
        # ë©”ëª¨ë¦¬ íŒŒì¼ì— ì €ì¥ (êµ¬ì¡°í™”ëœ í˜•ì‹)
        with open(memory_file, 'w', encoding='utf-8') as f:
            f.write("=== MEMORY SUMMARY ===\n")
            f.write(memory_summary)
            f.write("\n\n=== GROUNDING ===\n")
            f.write(grounding_section)
        
        # ì•¡ì…˜ ì‹¤í–‰ í›„ CLI í…ìŠ¤íŠ¸ ì‹œê°í™” (ì˜ˆì¸¡ ê²½ë¡œ í¬í•¨)
        visualize_grid_cli(wrapper, new_state, predicted_path)
        
        # ì•¡ì…˜ ì‹¤í–‰ í›„ ì—…ë°ì´íŠ¸ëœ ì´ë¯¸ì§€ í‘œì‹œ (ì˜ˆì¸¡ ê²½ë¡œ í¬í•¨, ê°™ì€ ì°½ì— ì—…ë°ì´íŠ¸)
        updated_image = wrapper.get_image()
        display_image(updated_image, WINDOW_NAME, predicted_path)
        
        # ë‹¤ìŒ ìŠ¤í…ì„ ìœ„í•´ ì˜ˆì¸¡ ê²½ë¡œ ì €ì¥ (ì‹œê°í™” ìœ ì§€ìš©)
        previous_predicted_path = predicted_path
        
        # 9. ë¡œê¹…
        print(f"[9] ì‹¤í—˜ ë°ì´í„° ë¡œê¹… ì¤‘...")
        try:
            save_experiment_data(
                step=step,
                image=image,
                state=state,
                action=first_action_index,
                action_name=first_action_name,
                user_prompt=user_prompt,
                vlm_response=vlm_response,
                reward=reward,
                done=done,
                log_dir=log_dir,
                all_predicted_actions=all_predicted_actions,
                vlm_input=vlm_input_info,
                vlm_output=vlm_output_info,
                memory_summary=memory_summary,
                grounding_section=grounding_section
            )
            print(f"  - ì´ë¯¸ì§€: {log_dir / f'step_{step:04d}.png'}")
            print(f"  - JSON: {log_dir / 'experiment_log.json'} (ëˆ„ì )")
            print(f"  - VLM I/O: {log_dir / 'vlm_io_log.txt'} (ëˆ„ì )")
            print(f"  - CSV: {log_dir / 'experiment_log.csv'} (ëˆ„ì )")
        except Exception as e:
            print(f"ë¡œê¹… ì˜¤ë¥˜: {e}")
        
        # Step ì¢…ë£Œ í‘œì‹œ
        print("\n" + "=" * 80)
        print(f"STEP {step} END")
        print("=" * 80)
        
        # Goal ë„ì°© í™•ì¸
        if done:
            print("\n" + "=" * 80)
            print("Goal ë„ì°©! ì‹¤í—˜ ì¢…ë£Œ")
            print("=" * 80)
            break
        
        # ìµœëŒ€ ìŠ¤í… ì œí•œ (ë¬´í•œ ë£¨í”„ ë°©ì§€)
        if step >= 100:
            print("\n" + "=" * 80)
            print("ìµœëŒ€ ìŠ¤í… ìˆ˜(100)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            print("=" * 80)
            break
    
    # ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    cv2.destroyAllWindows()
    wrapper.close()
    print(f"\nì‹¤í—˜ ì™„ë£Œ. ë¡œê·¸ëŠ” {log_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    
    VLM ì„¤ì •ì€ ì½”ë“œ ìƒë‹¨ì˜ ë³€ìˆ˜ì—ì„œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
    - VLM_TYPE: VLM íƒ€ì… ("gpt4o", "chatgpt4o", "openai")
    - VLM_MODEL: ì‚¬ìš©í•  ëª¨ë¸ëª… (ì˜ˆ: "gpt-4o", "gpt-4o-mini")
    - VLM_TEMPERATURE: ìƒì„± ì˜¨ë„ (0.0 ~ 2.0)
    - VLM_MAX_TOKENS: ìµœëŒ€ í† í° ìˆ˜
    """
    try:
        run_vlm_controlled_experiment()
    except KeyboardInterrupt:
        print("\n\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
